id: 'opencspm-darkbit-community-controls'
title: 'Darkbit Community Controls'
description: 'This checks the majority of the AWS 1.3.0 and GCP CIS 1.1 Benchmark controls'
controls:
  - id: darkbit-aws-9
    title: Users Should Not Have Access Keys Unused for Over 90 Days
    description:
      AWS Access Keys that have not been used for over 90 days indicate that
      they are no longer in regular use and represent an unnecessary exposure of 
      valid credentials.
    remediation: |
      Review the access keys that have not been used and consider revoking them.
    validation: |-
      Run `aws iam get-credential-report --output text | base64 -d | awk -F, 
      '{print $1 " " $11 " " $16}'` to get a list of `user`, `access_key_1_last_used_date`,
      and `access_key_2_last_used_date`. Ensure all dates are within 90 days.
    impact: 5
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
    refs:
      - text: AWS Access Keys
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_access-keys.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.12
      - aws-cfg:
          - aws-cfg-iam-user-unused-credentials-check
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-8.1.4
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-4
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(1)
          - nist-800-53-rev4-AC-2(3)
          - nist-800-53-rev4-AC-2(f)
          - nist-800-53-rev4-AC-3
          - nist-800-53-rev4-AC-6
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.2
          - nist-800-171-3.1.4
          - nist-800-171-3.1.5
          - nist-800-171-3.5.6
      - hipaa:
          - hipaa-164.308(a)(3)(ii)(B)
          - hipaa-164.308(a)(4)(ii)(C)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(1)
          - fedramp-moderate-AC-2(3)
          - fedramp-moderate-AC-2(f)
          - fedramp-moderate-AC-3
          - fedramp-moderate-AC-6
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-IA.3.086
      - cmmc-level4:
          - cmmc-level4-IA.3.086
      - cmmc-level3:
          - cmmc-level3-IA.3.086
      - aws-wa-security:
          - aws-wa-security-SEC-2
      - aws
  - id: darkbit-aws-10
    title: Users With a Console Password Should Have MFA Enabled
    description:
      All AWS Console and API user accounts should have a multi-factor authentication
      (MFA) device configured to enforce two-factor authentication for access.  This
      is especially important for user accounts with administrative level access to
      the account.
    remediation:
      'Notify the affected users and have them configure their 2FA device
      on their own accounts. As new users are configured, ensure they configure 2FA
      devices and routinely audit for their configuration.  Note: Users will need specific
      permissions to manage their own MFA device(s).'
    validation: |-
      Run `aws iam get-credential-report --output text | base64 -d | awk -F, 
      '{print $1 " "$4" "$8}' | grep " true "` to get list of `user`,
      `password_enabled`, `mfa_enabled`.  Ensure all are `true`.
    impact: 9
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
    refs:
      - text: Self-Manage AWS MFA Permissions
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/reference_policies_examples_aws_my-sec-creds-self-manage-mfa-only.html
      - text: AWS MFA
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa.html
      - text: AWS MFA Status
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_checking-status.html
      - text: AWS MFA Enablement
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_virtual.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.10
      - aws-cfg:
          - aws-cfg-iam-user-mfa-enabled
          - aws-cfg-mfa-enabled-for-iam-console-access
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-8.3
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-7
      - nist-800-53-rev4:
          - nist-800-53-rev4-IA-2(1)(2)(11)
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.2
          - nist-800-171-3.1.14
          - nist-800-171-3.5.2
          - nist-800-171-3.5.3
      - hipaa:
          - hipaa-164.308(a)(3)(ii)(A)
          - hipaa-164.312(d)
      - fedramp-moderate:
          - fedramp-moderate-IA-2(1)(2)
      - fedramp-low:
          - fedramp-low-IA-2
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.002
          - cmmc-level5-IA.1.077
          - cmmc-level5-MA.2.113
          - cmmc-level5-IA.3.083
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.002
          - cmmc-level4-IA.1.077
          - cmmc-level4-MA.2.113
          - cmmc-level4-IA.3.083
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.002
          - cmmc-level3-IA.1.077
          - cmmc-level3-MA.2.113
          - cmmc-level3-IA.3.083
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.002
          - cmmc-level2-IA.1.077
          - cmmc-level2-MA.2.113
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.002
          - cmmc-level1-IA.1.077
      - aws-wa-security:
          - aws-wa-security-SEC-2
      - aws
  - id: darkbit-aws-11
    title:
      IAM Passwords Should Require Adquate Length, Complexity, Expiration, and
      Reuse by Policy
    description:
      A password policy for AWS passwords should be in place that enforces
      an adequate length, complexity, expiration, and reuse policy to prevent users
      from using weak or guessable credentials.
    remediation:
      As the <root> AWS user, configure the minimum password length to 14
      characters, require one or more of each character type for complexity, allow users
      to change their own passwords, expire passwords after 180 days, prevent password
      reuse for the past 5 or more, and require users to contact their administrator
      if their passwords expire.
    validation:
      Run `aws iam get-account-password-policy` and verify that `MinimumPasswordLength`
      is `>= 14`, `RequireSymbols` is `true`, `RequireNumbers` is `true`, `RequireUppercaseCharacters`
      is `true`, `RequireLowercaseCharacters` is `true`, `ExpirePasswords` is `true`,
      `MaxPasswordAge` is `<= 90`, and `PasswordReusePrevention` is `>= 5`.
    impact: 5
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_PASSWORD_POLICY
    refs:
      - text: AWS IAM Account Password Policy
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_passwords_account-policy.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.8
          - aws-cis1.3-1.9
      - aws-cfg:
          - aws-cfg-iam-password-policy
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-8.2.3
          - pci-dss-3.2.1-8.2.4
          - pci-dss-3.2.1-8.2.5
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(1)
          - nist-800-53-rev4-AC-2(f)
          - nist-800-53-rev4-AC-2(j)
          - nist-800-53-rev4-IA-2
          - nist-800-53-rev4-IA-5(1)(a)(d)(e)
          - nist-800-53-rev4-IA-5(4)
      - nist-800-171:
          - nist-800-171-3.5.5
          - nist-800-171-3.5.6
          - nist-800-171-3.5.7
          - nist-800-171-3.5.8
      - hipaa:
          - hipaa-164.308(a)(4)(ii)(C)
          - hipaa-164.312(d)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(1)
          - fedramp-moderate-AC-2(f)
          - fedramp-moderate-AC-2(j)
          - fedramp-moderate-IA-2
          - fedramp-moderate-IA-5(1)(a)(d)(e)
          - fedramp-moderate-IA-5(4)
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.002
          - cmmc-level5-IA.1.077
          - cmmc-level5-IA.2.078
          - cmmc-level5-IA.2.079
          - cmmc-level5-IA.3.085
          - cmmc-level5-IA.3.086
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.002
          - cmmc-level4-IA.1.077
          - cmmc-level4-IA.2.078
          - cmmc-level4-IA.2.079
          - cmmc-level4-IA.3.085
          - cmmc-level4-IA.3.086
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.002
          - cmmc-level3-IA.1.077
          - cmmc-level3-IA.2.078
          - cmmc-level3-IA.2.079
          - cmmc-level3-IA.3.085
          - cmmc-level3-IA.3.086
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.002
          - cmmc-level2-IA.1.077
          - cmmc-level2-IA.2.078
          - cmmc-level2-IA.2.079
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.002
          - cmmc-level1-IA.1.077
      - aws-wa-security:
          - aws-wa-security-SEC-2
      - aws
  - id: darkbit-aws-12
    title: The <Root> AWS Account Should Not Have Access Keys Generated
    description:
      The <root> AWS access account should not be used to access the AWS
      APIs via access keys.  Instead, the <root> account should only be used to create
      other named accounts with the appropriate permissions and each account should
      generate their own access keys as necessary.
    remediation: Revoke the <root> account's AWS access keys and monitor for their recreation.
    validation: |-
      Run `aws iam get-credential-report --output text | base64 -d | grep
      "^<root_account>" | awk -F, '{print $1 " "$9 " "$14}'` and ensure both columns
      show `false` for `access_key_1_active` and `access_key_2_active`, respectively.
    impact: 5
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
    refs:
      - text: AWS Access Key Management
        url: https://docs.aws.amazon.com/general/latest/gr/managing-aws-access-keys.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.4
      - aws-cfg:
          - aws-cfg-iam-root-access-key-check
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-7.2
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-4
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(f)
          - nist-800-53-rev4-AC-2(j)
          - nist-800-53-rev4-AC-3
          - nist-800-53-rev4-AC-6
          - nist-800-53-rev4-AC-6(10)
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.2
          - nist-800-171-3.1.4
          - nist-800-171-3.1.5
          - nist-800-171-3.1.6
          - nist-800-171-3.1.7
          - nist-800-171-3.4.6
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.308(a)(3)(i)
          - hipaa-164.308(a)(3)(ii)(B)
          - hipaa-164.308(a)(4)(ii)(C)
          - hipaa-164.312(a)(2)(i)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(f)
          - fedramp-moderate-AC-2(j)
          - fedramp-moderate-AC-3
          - fedramp-moderate-AC-6
          - fedramp-moderate-AC-6(10)
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.002
          - cmmc-level5-AC.2.007
          - cmmc-level5-AC.2.008
          - cmmc-level5-AC.3.017
          - cmmc-level5-AC.3.018
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.002
          - cmmc-level4-AC.2.007
          - cmmc-level4-AC.2.008
          - cmmc-level4-AC.3.017
          - cmmc-level4-AC.3.018
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.002
          - cmmc-level3-AC.2.007
          - cmmc-level3-AC.2.008
          - cmmc-level3-AC.3.017
          - cmmc-level3-AC.3.018
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.002
          - cmmc-level2-AC.2.007
          - cmmc-level2-AC.2.008
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.002
      - aws-wa-security:
          - aws-wa-security-SEC-1
          - aws-wa-security-SEC-2
      - aws
  - id: darkbit-aws-13
    title: The <Root> AWS Account Should Be Protected by a Hardware MFA Device
    description:
      As the <root> AWS user has full control over the AWS account, it represents
      a single set of credentials that protect the integrity of the entire environment.  Protecting
      these credentials with with a hardware token increases the attack cost to needed
      physical access to a hardware token.
    remediation:
      Purchase and configure a hardware MFA token on the <root> account and
      all "administrator" accounts.
    validation: |-
      Run `aws iam get-credential-report --output text | base64 -d | awk -F,
      '{print $1 " "$8}' | grep "^<root_account>"` and verify it shows `true`.  Next,
      run `aws iam list-virtual-mfa-devices --assignment-status Assigned --query 'VirtualMFADevices[*].SerialNumber'
      --output json | grep root-account-mfa-device` and verify no results are returned.
    impact: 5
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
      - AWS_IAM_MFA_DEVICE
    refs:
      - text: AWS Hardware MFA
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_mfa_enable_physical.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-level2
          - aws-cis1.3-1
          - aws-cis1.3-1.5
          - aws-cis1.3-1.6
      - aws-cfg:
          - aws-cfg-root-account-hardware-mfa-enabled
          - aws-cfg-root-account-mfa-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-8.3
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-7
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(j)
          - nist-800-53-rev4-IA-2(1)(11)
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.2
          - nist-800-171-3.5.3
      - hipaa:
          - hipaa-164.308(a)(3)(ii)(A)
          - hipaa-164.312(d)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(j)
          - fedramp-moderate-IA-2(1)
      - fedramp-low:
          - fedramp-low-AC-2
          - fedramp-low-IA-2
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.002
          - cmmc-level2-IA.1.077
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.002
          - cmmc-level1-IA.1.077
      - aws-wa-security:
          - aws-wa-security-SEC-1
          - aws-wa-security-SEC-2
      - aws
  - id: darkbit-aws-17
    title: The IAM Support Policy Should Be Attached to Manage Incidents With AWS Support
    description:
      Non-administrative users that need the ability to open AWS support
      cases need to be granted explicit access, and the list of those users should be
      reviewed.
    remediation:
      If any non-administrative users require support access, grant them
      the `AWSSupportAccess` IAM role.  Review the list of these users on a routine
      basis.
    validation: |-
      Run `aws iam list-entities-for-policy --policy-arn arn:aws:iam::aws:policy/AWSSupportAccess` 
      and ensure at least one user, group, or role entry is present.
    impact: 2
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_MANAGED_POLICY
    refs:
      - text: AWS Support Access
        url: https://docs.aws.amazon.com/awssupport/latest/user/getting-started.html#accessing-support
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.17
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-4
          - nist-csf-pr.at
          - nist-csf-pr.at-2
      - aws
  - id: darkbit-aws-18
    title: Ensure IAM Policies That Allow Full "*:*" Administrative Privileges Are Not Used
    description:
      To reduce management complexity, custom IAM roles that are identical
      to the built-in IAM roles should not be used.  Instead, use the `AdministratorAccess`
      IAM Role.
    remediation:
      Modify the IAM Role binding to leverage the built-in `AdministratorAccess`
      IAM Role.
    validation: N/A
    impact: 2
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - PLACEHOLDER
    refs:
      - text: IAM Access Policies
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/access_policies_job-functions.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-1
          - aws-cis1.3-1.16
      - aws-cfg:
          - aws-cfg-iam-policy-no-statements-with-admin-access
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-7.2
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-PR.AC-1
          - nist-csf-PR.AC-4
          - nist-csf-PR.PT
          - nist-csf-PR.PT-3
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(j)
          - nist-800-53-rev4-AC-3
          - nist-800-53-rev4-AC-5c
          - nist-800-53-rev4-AC-6
          - nist-800-53-rev4-SC-2
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.2
          - nist-800-171-3.1.4
          - nist-800-171-3.1.5
          - nist-800-171-3.1.7
          - nist-800-171-3.4.6
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.308(a)(3)(i)
          - hipaa-164.308(a)(3)(ii)(B)
          - hipaa-164.308(a)(4)(i)
          - hipaa-164.308(a)(4)(ii)(B)
          - hipaa-164.308(a)(4)(ii)(C)
          - hipaa-164.312(a)(1)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(j)
          - fedramp-moderate-AC-3
          - fedramp-moderate-AC-5c
          - fedramp-moderate-AC-6
          - fedramp-moderate-SC-2
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.002
          - cmmc-level5-AC.2.007
          - cmmc-level5-AC.2.008
          - cmmc-level5-AC.3.017
          - cmmc-level5-AC.3.018
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.002
          - cmmc-level4-AC.2.007
          - cmmc-level4-AC.2.008
          - cmmc-level4-AC.3.017
          - cmmc-level4-AC.3.018
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.002
          - cmmc-level3-AC.2.007
          - cmmc-level3-AC.2.008
          - cmmc-level3-AC.3.017
          - cmmc-level3-AC.3.018
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.002
          - cmmc-level2-AC.2.007
          - cmmc-level2-AC.2.008
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.002
      - aws-wa-security:
          - aws-wa-security-SEC-3
      - aws
  - id: darkbit-aws-20
    title: IAM Users Should Not Have Attached or Inline Policies
    description:
      To reduce management complexity, IAM permissions should only be assigned
      to roles and groups. Users can then be added to those groups. Policies should
      not be applied directly to a user.
    remediation:
      Create groups with the required policies, move the IAM users to the
      applicable groups, and then remove the inline and directly attached policies from
      the IAM user.
    validation:
      Run the following command and ensure each user has no entries listed.
      
      ```
      for i in $(aws iam list-users --query 'Users[*].UserName' --output text)
        do POLS="$(aws iam list-user-policies --user-name $i --query 'PolicyNames[*]' --output text)"
        ATT="$(aws iam list-attached-user-policies --user-name $i --query 'AttachedPolicies[*].PolicyArn' --output text)"
        echo "$i $POLS $ATT"
      done
      ```
    impact: 2
    effort: 5
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
      - AWS_IAM_POLICY
    refs:
      - text: IAM Best Practicies
        url: http://docs.aws.amazon.com/IAM/latest/UserGuide/best-practices.html#use-groups-for-permissions
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.15
      - aws-cfg:
          - aws-cfg-iam-no-inline-policy-check
          - aws-cfg-iam-user-no-policies-check
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-4
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(j)
          - nist-800-53-rev4-AC-3
          - nist-800-53-rev4-AC-5c
          - nist-800-53-rev4-AC-6
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.2
          - nist-800-171-3.1.4
          - nist-800-171-3.1.5
          - nist-800-171-3.4.6
      - hipaa:
          - hipaa-164.308(a)(3)(i)
          - hipaa-164.308(a)(3)(ii)(B)
          - hipaa-164.308(a)(4)(i)
          - hipaa-164.308(a)(4)(ii)(B)
          - hipaa-164.308(a)(4)(ii)(C)
          - hipaa-164.312(a)(1)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(j)
          - fedramp-moderate-AC-3
          - fedramp-moderate-AC-5c
          - fedramp-moderate-AC-6
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.002
          - cmmc-level5-AC.2.007
          - cmmc-level5-AC.2.008
          - cmmc-level5-AC.3.017
          - cmmc-level5-AC.3.018
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.002
          - cmmc-level4-AC.2.007
          - cmmc-level4-AC.2.008
          - cmmc-level4-AC.3.017
          - cmmc-level4-AC.3.018
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.002
          - cmmc-level3-AC.2.007
          - cmmc-level3-AC.2.008
          - cmmc-level3-AC.3.017
          - cmmc-level3-AC.3.018
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.002
          - cmmc-level2-AC.2.007
          - cmmc-level2-AC.2.008
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.002
      - aws-wa-security:
          - aws-wa-security-SEC-3
      - aws
  - id: darkbit-aws-23
    title: Security Groups Should Not Allow Access to Administrative Ports From All Hosts
    description:
      AWS Security Groups that permit inbound/ingress access from any IP
      address (0.0.0.0/0) to administrative ports via TCP/22 and TCP/3389 should be reviewed
      for necessity to prevent unintended exposure of services and systems protected
      by that security group.  The primary exclusion to this is a dedicated, hardened
      bastion host.
    remediation:
      For each security group, assess whether the attached system requires
      SSH access from any IP address.  If it doesn't, consider reducing the source IP
      ranges to a specific set of subnets or to the security group attached to the bastion
      host(s) in the environment.
    validation: |-
      Run the following commands to show security groups that allow administrative ports from any host.

      ```
      aws ec2 describe-security-groups \
        --region $AWS_REGION \
        --query 'SecurityGroups[*]' \
        --output json | jq -r --arg PROTO tcp --arg PORT 22 '.[] | . as $group | .IpPermissions[] |
        select(.IpRanges[].CidrIp=="0.0.0.0/0"
        or .Ipv6Ranges[].CidrIpv6=="::/0"
        or .IpRanges==[]
        or .Ipv6Ranges==[]) |
        select((.IpProtocol==$PROTO or .IpProtocol=="-1")
        and ((.FromPort==null or .FromPort<=($PORT|tonumber))
        and (.ToPort==null or .ToPort>=($PORT|tonumber)))
        and (.UserIdGroupPairs==[])) | "\($group.GroupId) \($group.GroupName)"'
      ```

      and

      ```
      aws ec2 describe-security-groups \
        --region $AWS_REGION \
        --query 'SecurityGroups[*]' \
        --output json | jq -r --arg PROTO tcp --arg PORT 3389 '.[] | . as $group | .IpPermissions[] | 
        select(.IpRanges[].CidrIp=="0.0.0.0/0"
        or .Ipv6Ranges[].CidrIpv6=="::/0"
        or .IpRanges==[]
        or .Ipv6Ranges==[]) |
        select((.IpProtocol==$PROTO or .IpProtocol=="-1")
        and ((.FromPort==null or .FromPort<=($PORT|tonumber))
        and (.ToPort==null or .ToPort>=($PORT|tonumber)))
        and (.UserIdGroupPairs==[])) | "\($group.GroupId) \($group.GroupName)"'
      ```
    impact: 5
    effort: 2
    platform: AWS
    category: Network Access Control
    resource: Security Groups
    nodes:
      - PLACEHOLDER
    refs:
      - text: VPC Security Groups
        url: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_SecurityGroups.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-5
          - aws-cis1.3-5.2
      - aws-cfg:
          - aws-cfg-restricted-ssh
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-1.2.1
          - pci-dss-3.2.1-1.3
          - pci-dss-3.2.1-2.2.2
      - nist-csf:
          - nist-csf-PR.AC-3
          - nist-csf-PR.AC-5
          - nist-csf-PR.PT-4
          - nist-csf-DE.AE-1
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-4
          - nist-800-53-rev4-SC-7
          - nist-800-53-rev4-SC-7(3)
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.14
          - nist-800-171-3.1.2
          - nist-800-171-3.1.20
          - nist-800-171-3.1.3
          - nist-800-171-3.13.1
          - nist-800-171-3.13.2
          - nist-800-171-3.13.6
          - nist-800-171-3.4.7
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.312(e)(1)
      - fedramp-moderate:
          - fedramp-moderate-AC-4
          - fedramp-moderate-SC-7
          - fedramp-moderate-SC-7(3)
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.003
          - cmmc-level5-SC.1.175
          - cmmc-level5-AC.2.016
          - cmmc-level5-CM.3.068
          - cmmc-level5-SC.3.180
          - cmmc-level5-AC.4.023
          - cmmc-level5-RM.4.151
          - cmmc-level5-SC.5.230
          - cmmc-level5-SC.5.208
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.003
          - cmmc-level4-SC.1.175
          - cmmc-level4-AC.2.016
          - cmmc-level4-CM.3.068
          - cmmc-level4-SC.3.180
          - cmmc-level4-AC.4.023
          - cmmc-level4-RM.4.151
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.003
          - cmmc-level3-SC.1.175
          - cmmc-level3-AC.2.016
          - cmmc-level3-CM.3.068
          - cmmc-level3-SC.3.180
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.003
          - cmmc-level2-SC.1.175
          - cmmc-level2-AC.2.016
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.003
          - cmmc-level1-SC.1.175
      - aws-wa-security:
          - aws-wa-security-SEC-5
      - aws
  - id: darkbit-aws-29
    title: VPC Flow Logging Should Be Enabled
    description:
      VPC flow logs record metadata about all traffic flowing in to and out
      of a VPC. These logs are vital for auditing and review after security incidents
      to be able to create an accurate timeline of network events to go with application
      and AWS API logs.
    remediation:
      For each active VPC, modify it via Actions > Create flow log and direct
      the output to S3 or a CloudWatch Log.  When directing to an S3 bucket, enable
      bucket versioning and optionally configure an object lifecycle policy to retain
      the data for the desired period only.
    validation: |-
      Run `for i in $(aws ec2 describe-vpcs --region <region> --query 'Vpcs[?State=="available"].VpcId'
      --output text); do f=$(aws ec2 describe-flow-logs --region <region> --filter Name="resource-id",Values="${i}"
      --query 'FlowLogs[?FlowLogStatus=="ACTIVE"].FlowLogId' --output text); echo
      "$i: $f"; done` for each region and ensure each VPC has an associated flow log id.
    impact: 2
    effort: 2
    platform: AWS
    category: Networking and Content Delivery
    resource: VPC
    nodes:
      - AWS_VPC
      - AWS_FLOW_LOG
    refs:
      - text: VPC Flow Logs
        url: http://docs.aws.amazon.com/AmazonVPC/latest/UserGuide/flow-logs.html
      - text: Working with VPC Flow Logs
        url: https://docs.aws.amazon.com/vpc/latest/userguide/working-with-flow-logs.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level2
          - aws-cis1.3-3
          - aws-cis1.3-3.9
      - aws-cfg:
          - aws-cfg-vpc-flow-logs-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-10.3.3
          - pci-dss-3.2.1-10.3.4
          - pci-dss-3.2.1-10.3.5
          - pci-dss-3.2.1-10.3.6
      - nist-csf:
          - nist-csf-ID.AM-3
          - nist-csf-PR.DS-5
          - nist-csf-PR.PT-1
          - nist-csf-DE.AE-1
          - nist-csf-DE.AE-3
          - nist-csf-DE.CM-1
          - nist-csf-DE.CM-7
      - nist-800-53-rev4:
          - nist-800-53-rev4-AU-2(a)(d)
          - nist-800-53-rev4-AU-3
          - nist-800-53-rev4-AU-12(a)(c)
      - nist-800-171:
          - nist-800-171-3.1.12
          - nist-800-171-3.13.1
          - nist-800-171-3.14.6
          - nist-800-171-3.14.7
          - nist-800-171-3.3.1
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(D)
          - hipaa-164.308(a)(3)(ii)(A)
          - hipaa-164.308(a)(6)(ii)
          - hipaa-164.312(b)
      - fedramp-moderate:
          - fedramp-moderate-AU-2(a)(d)
          - fedramp-moderate-AU-3
          - fedramp-moderate-AU-12(a)(c)
      - fedramp-low:
          - fedramp-low-AU-2
      - cmmc-level5:
          - cmmc-level5-AU.2.042
          - cmmc-level5-SI.2.217
          - cmmc-level5-AU.5.055
          - cmmc-level5-SI.5.223
      - cmmc-level4:
          - cmmc-level4-AU.2.042
          - cmmc-level4-SI.2.217
      - cmmc-level3:
          - cmmc-level3-AU.2.042
          - cmmc-level3-SI.2.217
      - cmmc-level2:
          - cmmc-level2-AU.2.042
          - cmmc-level2-SI.2.217
      - aws-wa-security:
          - aws-wa-security-SEC-4
      - aws
  - id: darkbit-aws-31
    title: Amazon EKS Cluster API Should Not Be Public
    description: |-
      Amazon EKS creates an endpoint for the managed Kubernetes API server that you use to communicate with your cluster using kubectl, for example. By default, this API server endpoint is public to the internet, and access to the API server is secured using a combination of AWS Identity and Access Management (IAM), native Kubernetes Role Based Access Control (RBAC), and an access control list that allows any IP address (0.0.0.0/0) to connect.

      While this makes administration convenient, the scope of potential attackers is not limited should a newly discovered vulnerability or denial-of-service become available.  Also, should valid credentials from a phished administrator/developer be stolen or leaked, they can be directly used without having to originate from a known set of IP ranges.
    remediation:
      During cluster creation, specify `endpointPrivateAccess=true` in the
      `resourcesVpcConfig` block.  Or, specify `publicAccessCidrs` to be a set of CIDR
      ranges that do not include `0.0.0.0/0`.  For existing clusters, `publicAccessCidrs`
      can be updated using `aws eks update-cluster-config` under the `--resources-vpc-config`
      flag.
    validation:
      Run `aws eks describe-cluster` and review the contents of the `resourcesVpcConfig`
      block.  `endpointPrivateAccess` should be `true`.  Or, if `endpointPublicAccess`
      is `true`, `publicAccessCidrs` should be set to something other than `0.0.0.0/0`.
    impact: 8
    effort: 5
    platform: AWS
    category: Network Access Control
    resource: EKS
    nodes:
      - AWS_EKS_CLUSTER
    refs:
      - text: CVE-2019-11253 Billion Laughs Writeup
        url: https://www.stackrox.com/post/2019/09/protecting-kubernetes-api-against-cve-2019-11253-billion-laughs-attack/
      - text: CVE-2019-11253 Billion Laughs Exploit Code
        url: https://gist.github.com/bgeesaman/0e0349e94cd22c48bf14d8a9b7d6b8f2
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.4
          - eks-cis1.0.1-5.4.1
          - eks-cis1.0.1-5.4.2
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ip
          - nist-csf-pr.ip-1
          - nist-csf-pr.ma
          - nist-csf-pr.ma-2
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
          - nist-csf-pr.pt-4
      - aws
  - id: darkbit-aws-32
    title: Instance Metadata Access From Pods Should Be Protected
    description:
      Pods should not have the ability to reach the EC2 Metadata API of the underlying node
      without restriction.  In EKS, the IAM permissions are that of the worker IAM Role
      for the Nodegroup.  If the default IAM Role was customized to add permissions to
      other services, those permissions may be used to enumerate other resources or create
      new resources.  Also by extension, this means that every container image ever run
      in this cluster has had the ability to reach and export these credentials.
    remediation:
      Deploy a solution such as Amazon's EKS Pod Identity Webhook to run
      a daemonset that blocks and redirects pods attempting to access the Instance's
      Metadata API and will optionally serve up a specific IAM Role's credentials instead.  Alternatively,
      deploy an egress NetworkPolicy blocking egress to 169.254.169.254 for all non-kube-system
      namespaces.
    validation: |-
      Verify that the EKS Pod Identity Webhook daemonset is installed on all
      clusters, and attempt to access the instance credentials via `curl` from inside
      a running `pod`.  For example: 

      ```
      kubectl exec -it -n <namespace> <podname> \
       curl -s https://169.254.169.254/latest/meta-data/iam/security-credentials
      ```
    impact: 9
    effort: 5
    platform: AWS
    category: Network Access Control
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: IAM Roles to Kubernetes Service Accounts
        url: https://aws.amazon.com/blogs/opensource/introducing-fine-grained-iam-roles-service-accounts/
      - text: EKS Pod Identity
        url: https://github.com/aws/amazon-eks-pod-identity-webhook
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.2
          - eks-cis1.0.1-5.2.1
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-4
          - nist-csf-pr.ac-5
      - aws
  - id: darkbit-aws-34
    title: Amazon ECR Image Scan-on-Push Should Be Enabled
    description:
      AWS ECR provides support for performing vulnerability scans on container
      images either manually or on upload/push. The vulnerability details can be reviewed
      via manual or automated means to provide a risk assessment of the contents of
      the container.
    remediation:
      Enable ECR scan on push for all ECR repositories, and then review the vulnerability
      details for each container for any critical or high severity risks.  Consider
      updating images by rebuilding them and redeploying.
    validation: |
      In each region, run `aws ecr describe-repositories --region <region>
      --query "repositories[]" --output json | jq -r '.[] | "(.repositoryArn) (.imageScanningConfiguration.scanOnPush)"'
      | grep "false$"` and ensure no entries are present.
    impact: 5
    effort: 2
    platform: AWS
    category: Containers
    resource: ECR
    nodes:
      - AWS_ECR_REPOSITORY
    refs:
      - text: ECR Image Scanning
        url: https://docs.aws.amazon.com/AmazonECR/latest/userguide/image-scanning.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.1
          - eks-cis1.0.1-5.1.1
      - nist-csf:
          - nist-csf-id
          - nist-csf-id.gv
          - nist-csf-id.gv-1
          - nist-csf-id.ra
          - nist-csf-id.ra-2
          - nist-csf-id.ra-5
          - nist-csf-id.sc
          - nist-csf-id.sc-2
      - aws
  - id: darkbit-aws-35
    title: S3 Server Access Control Logging Should Be Enabled
    description:
      S3 bucket logging helps maintain an audit trail of access that can
      be used in the event of a security incident.  S3 logging is disabled by default,
      so you will be blind to any unauthorized access unless this is explicitly enabled.
    remediation:
      In each region, create a dedicated bucket for receiving logs.  Configure
      each S3 bucket to enable Server Access Logging to that region's logging bucket,
      and configure an object lifecycle policy to move older logs to "colder" storage
      and to retain those logs for a desired period of time.
    validation: |-
      Run the command below and ensure each bucket has a valid logging configuration present.

      ```
      for i in $(aws s3api list-buckets --query 'Buckets[*].Name' --output text); do
        echo $i: $(aws s3api get-bucket-logging --bucket $i | grep TargetBucket)
      done
      ```
    impact: 8
    effort: 2
    platform: AWS
    category: Storage
    resource: S3
    nodes:
      - AWS_S3_BUCKET
    refs:
      - text: S3 Access Logging
        url: https://docs.aws.amazon.com/AmazonS3/latest/dev/ServerLogs.html
      - text: S3 Object Lifecycle Management
        url: https://docs.aws.amazon.com/AmazonS3/latest/dev/object-lifecycle-mgmt.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-3
          - aws-cis1.3-3.6
      - aws-cfg:
          - aws-cfg-s3-bucket-logging-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-10.1
          - pci-dss-3.2.1-10.2.1
          - pci-dss-3.2.1-10.2.2
          - pci-dss-3.2.1-10.2.3
          - pci-dss-3.2.1-10.2.4
          - pci-dss-3.2.1-10.2.7
          - pci-dss-3.2.1-10.3.1
          - pci-dss-3.2.1-10.3.2
          - pci-dss-3.2.1-10.3.3
          - pci-dss-3.2.1-10.3.4
          - pci-dss-3.2.1-10.3.5
          - pci-dss-3.2.1-10.3.6
      - nist-csf:
          - nist-csf-ID.AM-3
          - nist-csf-PR.AC-6
          - nist-csf-PR.DS-5
          - nist-csf-PR.PT-1
          - nist-csf-DE.AE-1
          - nist-csf-DE.AE-3
          - nist-csf-DE.AE-4
          - nist-csf-DE.CM-1
          - nist-csf-DE.CM-3
          - nist-csf-DE.CM-6
          - nist-csf-DE.CM-7
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(g)
          - nist-800-53-rev4-AU-2(a)(d)
          - nist-800-53-rev4-AU-3
          - nist-800-53-rev4-AU-12(a)(c)
      - nist-800-171:
          - nist-800-171-3.1.12
          - nist-800-171-3.13.1
          - nist-800-171-3.14.6
          - nist-800-171-3.14.7
          - nist-800-171-3.3.1
          - nist-800-171-3.3.2
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(D)
          - hipaa-164.308(a)(3)(ii)(A)
          - hipaa-164.308(a)(6)(ii)
          - hipaa-164.312(b)
          - hipaa-164.312(e)(2)(i)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(g)
          - fedramp-moderate-AU-2(a)(d)
          - fedramp-moderate-AU-3
          - fedramp-moderate-AU-12(a)(c)
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-IA.1.076
          - cmmc-level5-AU.2.041
          - cmmc-level5-AU.2.042
          - cmmc-level5-CM.2.065
          - cmmc-level5-SI.2.217
          - cmmc-level5-AC.3.018
          - cmmc-level5-AU.5.055
          - cmmc-level5-SI.5.223
      - cmmc-level4:
          - cmmc-level4-IA.1.076
          - cmmc-level4-AU.2.041
          - cmmc-level4-AU.2.042
          - cmmc-level4-CM.2.065
          - cmmc-level4-SI.2.217
          - cmmc-level4-AC.3.018
      - cmmc-level3:
          - cmmc-level3-IA.1.076
          - cmmc-level3-AU.2.041
          - cmmc-level3-AU.2.042
          - cmmc-level3-CM.2.065
          - cmmc-level3-SI.2.217
          - cmmc-level3-AC.3.018
      - cmmc-level2:
          - cmmc-level2-IA.1.076
          - cmmc-level2-AU.2.041
          - cmmc-level2-AU.2.042
          - cmmc-level2-CM.2.065
          - cmmc-level2-SI.2.217
      - cmmc-level1:
          - cmmc-level1-IA.1.076
      - aws-wa-security:
          - aws-wa-security-SEC-4
      - aws
  - id: darkbit-aws-42
    title: S3 Bucket Should Have a Secure Transport Policy Enabled
    description:
      S3 buckets can be configured with a bucket policy that further restricts
      the conditions in which access can be granted, and all S3 buckets should have
      a bucket policy enforcing the use of SSL/TLS connections to prevent eavesdropping
      of sensitive data when accessed via HTTP.
    remediation: |
      Configure S3 bucket policies for each bucket such that the `aws:SecureTransport`
      setting is `false` in a `deny all` condition block.  Refer to: https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html
      for a working example.
    validation: |-
      Run the following command and verify that each bucket has a policy defined that includes `"aws:SecureTransport":"false"`
      on the `"Deny"` Effect.
      
      ```
      for i in $(aws s3api list-buckets --query 'Buckets[*].Name' --output text)
        do OUT="$(aws s3api get-bucket-policy --query Policy --bucket $i --output text 2> /dev/null | grep 'aws:SecureTransport')"
        echo "$i: $OUT"
      done
      ```
    impact: 2
    effort: 2
    platform: AWS
    category: Storage
    resource: S3
    nodes:
      - PLACEHOLDER
    refs:
      - text: S3 SSL Enforcement
        url: https://docs.aws.amazon.com/config/latest/developerguide/s3-bucket-ssl-requests-only.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level2
          - aws-cis1.3-2
          - aws-cis1.3-2.1
          - aws-cis1.3-2.1.2
      - aws-cfg:
          - aws-cfg-s3-bucket-ssl-requests-only
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-4.1
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ds
          - nist-csf-pr.ds-2
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-17(2)
          - nist-800-53-rev4-SC-7
          - nist-800-53-rev4-SC-8
          - nist-800-53-rev4-SC-8(1)
          - nist-800-53-rev4-SC-13
      - nist-800-171:
          - nist-800-171-3.1.13
          - nist-800-171-3.13.1
          - nist-800-171-3.13.8
          - nist-800-171-3.5.10
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.312(a)(2)(iv)
          - hipaa-164.312(e)(1)
          - hipaa-164.312(e)(2)(i)
          - hipaa-164.312(e)(2)(ii)
      - fedramp-moderate:
          - fedramp-moderate-AC-17(2)
          - fedramp-moderate-SC-7
          - fedramp-moderate-SC-8
          - fedramp-moderate-SC-8(1)
      - fedramp-low:
          - fedramp-low-AC-17
      - cmmc-level5:
          - cmmc-level5-SC.1.175
          - cmmc-level5-IA.2.081
          - cmmc-level5-AC.3.014
          - cmmc-level5-SC.3.185
      - cmmc-level4:
          - cmmc-level4-SC.1.175
          - cmmc-level4-IA.2.081
          - cmmc-level4-AC.3.014
          - cmmc-level4-SC.3.185
      - cmmc-level3:
          - cmmc-level3-SC.1.175
          - cmmc-level3-IA.2.081
          - cmmc-level3-AC.3.014
          - cmmc-level3-SC.3.185
      - cmmc-level2:
          - cmmc-level2-SC.1.175
          - cmmc-level2-IA.2.081
      - cmmc-level1:
          - cmmc-level1-SC.1.175
      - aws-wa-security:
          - aws-wa-security-SEC-9
      - aws
  - id: darkbit-aws-43
    title: AMI/EBS Volumes/EBS Snapshots Should Be Encrypted
    description:
      Certain compliance mandates like HIPAA indicate that AMIs, EBS Volumes,
      and EBS snapshots should have at-rest encryption enabled through AWS using KMS
      to require an attacker to also have access to the KMS keys to be able to access
      data on volumes in the account.
    remediation:
      Note that EBS Snapshots taken from EBS Volumes without at-rest encryption
      will also not be encrypted.
    validation: |-
      Run `aws ec2 describe-images --region <region> --owners self --query 'Images[*]' --output json | jq -r '.[] | select(.BlockDeviceMappings[].Ebs.Encrypted!=true) | "(.ImageId) (.Name) (.Description)"'` for each region and ensure no entries are present.

      Run `aws ec2 describe-snapshots --region <region> --owner-ids 597908126950 --query 'Snapshots[*]' --output json | jq -r '.[] | select(.State=="completed" and .Encrypted!=true) | "(.SnapshotId) (.Description)"'` for each region and ensure no entries are present.

      Run `aws ec2 describe-volumes --region <region> --query 'Volumes[*]' --output json | jq -r '.[] | select(.Encrypted!=true) | "(.VolumeId) (.SnapshotId)"'` for reach region and ensure no entries are present.
    impact: 2
    effort: 9
    platform: AWS
    category: Storage
    resource: EC2
    nodes:
      - PLACEHOLDER
    refs:
      - text: EBS Encryption
        url: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html
      - text: AMI Encryption
        url: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/AMIEncryption.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-level2
          - aws-cis1.3-2
          - aws-cis1.3-2.2
          - aws-cis1.3-2.2.1
      - aws-cfg:
          - aws-cfg-encrypted-volumes
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-3.4
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ds
          - nist-csf-pr.ds-1
      - nist-800-53-rev4:
          - nist-800-53-rev4-SC-13
          - nist-800-53-rev4-SC-28
      - nist-800-171:
          - nist-800-171-3.13.16
          - nist-800-171-3.5.10
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.312(a)(2)(iv)
          - hipaa-164.312(e)(2)(ii)
      - fedramp-moderate:
          - fedramp-moderate-SC-28
      - fedramp-low:
          - fedramp-low-SC-13
      - cmmc-level5:
          - cmmc-level5-IA.2.081
          - cmmc-level5-SC.3.191
      - cmmc-level4:
          - cmmc-level4-IA.2.081
          - cmmc-level4-SC.3.191
      - cmmc-level3:
          - cmmc-level3-IA.2.081
          - cmmc-level3-SC.3.191
      - cmmc-level2:
          - cmmc-level2-IA.2.081
      - aws-wa-security:
          - aws-wa-security-SEC-8
      - aws
  - id: darkbit-aws-47
    title: Amazon EKS Clusters Should Have Application Secrets Encryption Enabled
    description:
      Envelope encryption for secrets is available for new Amazon EKS clusters
      running Kubernetes version 1.13 and above. You can setup your own Customer Master
      Key (CMK) in KMS and link this key by providing the CMK ARN when you create an
      EKS cluster. When secrets are stored using the Kubernetes secrets API, they are
      encrypted with a Kubernetes-generated data encryption key, which is then further
      encrypted using the linked AWS KMS key.
    remediation:
      'Configure a Customer Master Key (CMK) in KMS, and during cluster creation,
      configure the `--encryption-config` option to specify the ARN of the KMS key.  Note:
      This cannot be added to an existing EKS cluster.'
    validation:
      Run `aws eks describe-cluster` and look for the presence of the `encryptionConfig`
      block.
    impact: 3
    effort: 8
    platform: AWS
    category: Containers
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: Kubernetes KMS Provider Docs
        url: https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/
      - text: EKS Application Secrets Encryption
        url: https://aws.amazon.com/about-aws/whats-new/2020/03/amazon-eks-adds-envelope-encryption-for-secrets-with-aws-kms/
      - text: EKS Create Cluster
        url: https://docs.aws.amazon.com/eks/latest/userguide/create-cluster.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.3
          - eks-cis1.0.1-5.3.1
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ds
          - nist-csf-pr.ds-1
      - aws
  - id: darkbit-aws-51
    title: CloudTrail Should Be Enabled in All Regions
    description:
      CloudTrail should be enabled for all regions in order to maintain a
      record of all changes in the environment, detect suspicious activity and resource
      usage, and support incident response analysis efforts should an issue occur
    remediation:
      Enable CloudTrail for all regions and ensure that one region is also
      configured to monitor "global" service events.  Also, enable log file validation
      to have log digests delivered to your S3 bucket to verify that your log files
      did not change after CloudTrail delivered them.
    validation: |-
      Run the following command in each region and ensure **at least one** is `true`.
      
      ```
      aws cloudtrail describe-trails \
        --region <region> \
        --query 'trailList[*]' \
        --output json \
        --no-include-shadow-trails | jq -r '.[] | "\(.TrailARN) \(.IncludeGlobalServiceEvents)"'
      ```

      Run the following command in each region and ensure **each one** is `true`. 
      
      ```
      aws cloudtrail describe-trails \
        --region <region> \
        --query 'trailList[*]' \
        --output json \
        --no-include-shadow-trails | jq -r '.[] | "\(.TrailARN) \(.IsMultiRegionTrail)"'
      ```
    impact: 8
    effort: 2
    platform: AWS
    category: Management and Governance
    resource: CloudTrail
    nodes:
      - PLACEHOLDER
    refs:
      - text: AWS CloudTrail
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-create-and-update-a-trail.html
      - text: AWS CloudTrail Log File Validation
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-3
          - aws-cis1.3-3.1
      - aws-cfg:
          - aws-cfg-cloudtrail-enabled
          - aws-cfg-multi-region-cloudtrail-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-10.2.1
          - pci-dss-3.2.1-10.2.2
          - pci-dss-3.2.1-10.2.3
          - pci-dss-3.2.1-10.2.4
          - pci-dss-3.2.1-10.2.5
          - pci-dss-3.2.1-10.2.6
          - pci-dss-3.2.1-10.2.7
          - pci-dss-3.2.1-10.3.1
          - pci-dss-3.2.1-10.3.2
          - pci-dss-3.2.1-10.3.3
          - pci-dss-3.2.1-10.3.4
          - pci-dss-3.2.1-10.3.5
          - pci-dss-3.2.1-10.3.6
      - nist-csf:
          - nist-csf-ID.AM-3
          - nist-csf-PR.AC-6
          - nist-csf-PR.DS-5
          - nist-csf-PR.MA-2
          - nist-csf-PR.PT-1
          - nist-csf-DE.AE-1
          - nist-csf-DE.AE-3
          - nist-csf-DE.AE-4
          - nist-csf-DE.CM-1
          - nist-csf-DE.CM-3
          - nist-csf-DE.CM-6
          - nist-csf-DE.CM-7
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(4)
          - nist-800-53-rev4-AC-2(g)
          - nist-800-53-rev4-AU-2(a)(d)
          - nist-800-53-rev4-AU-3
          - nist-800-53-rev4-AU-12(a)(c)
      - nist-800-171:
          - nist-800-171-3.1.12
          - nist-800-171-3.13.1
          - nist-800-171-3.14.6
          - nist-800-171-3.14.7
          - nist-800-171-3.3.1
          - nist-800-171-3.3.2
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(D)
          - hipaa-164.308(a)(3)(ii)(A)
          - hipaa-164.308(a)(6)(ii)
          - hipaa-164.312(b)
          - hipaa-164.312(e)(2)(i)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(4)
          - fedramp-moderate-AC-2(g)
          - fedramp-moderate-AU-2(a)(d)
          - fedramp-moderate-AU-3
          - fedramp-moderate-AU-12(a)(c)
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-IA.1.076
          - cmmc-level5-AU.2.041
          - cmmc-level5-AU.2.042
          - cmmc-level5-CM.2.065
          - cmmc-level5-SI.2.217
          - cmmc-level5-AC.3.018
          - cmmc-level5-AU.5.055
          - cmmc-level5-SI.5.223
      - cmmc-level4:
          - cmmc-level4-IA.1.076
          - cmmc-level4-AU.2.041
          - cmmc-level4-AU.2.042
          - cmmc-level4-CM.2.065
          - cmmc-level4-SI.2.217
          - cmmc-level4-AC.3.018
      - cmmc-level3:
          - cmmc-level3-IA.1.076
          - cmmc-level3-AU.2.041
          - cmmc-level3-AU.2.042
          - cmmc-level3-CM.2.065
          - cmmc-level3-SI.2.217
          - cmmc-level3-AC.3.018
      - cmmc-level2:
          - cmmc-level2-IA.1.076
          - cmmc-level2-AU.2.041
          - cmmc-level2-AU.2.042
          - cmmc-level2-CM.2.065
          - cmmc-level2-SI.2.217
      - cmmc-level1:
          - cmmc-level1-IA.1.076
      - aws-wa-security:
          - aws-wa-security-SEC-4
      - aws
  - id: darkbit-aws-58
    title: AWS Config Service Should Be Configured and All Rules Active
    description:
      AWS Config records the change history of all supported resources. It
      can assist in auditing efforts by correlating configuration changes (what exactly
      was changed) with AWS CloudTrail events (who made the change and when).  Finally,
      it can generate reports to aid in assessing compliance over a period of time.
    remediation:
      Enable the AWS Config Service for all regions and resources in the
      account and ensure that it is delivering logs.
    validation:
      In each region, run `aws configservice get-status --region <region>`
      and verify the `recorder` is `ON` and the `last status` is `SUCCESS` 
      and `include_global_resource_types is `true`.
    impact: 5
    effort: 2
    platform: AWS
    category: Management and Governance
    resource: Config
    nodes:
      - AWS_CONFIG_CONFIGURATION_RECORDER
    refs:
      - text: AWS Config
        url: https://aws.amazon.com/config
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-3
          - aws-cis1.3-3.5
      - nist-csf:
          - nist-csf-id
          - nist-csf-id.am
          - nist-csf-id.am-2
          - nist-csf-id.gv
          - nist-csf-id.gv-4
          - nist-csf-id.ra
          - nist-csf-id.ra-1
          - nist-csf-id.ra-3
          - nist-csf-id.ra-4
          - nist-csf-id.ra-5
      - aws
  - id: darkbit-aws-61
    title: Amazon EKS Clusters Should Have Full Audit Logging Configured
    description:
      "Amazon EKS control plane logging provides audit and diagnostic logs
      directly from the Amazon EKS control plane to CloudWatch Logs in your account.
      \ By default, cluster control plane logs aren't sent to CloudWatch Logs. You must
      enable each log type individually to send logs for your cluster. CloudWatch Logs
      ingestion, archive storage, and data scanning rates apply to enabled control plane
      logs.  The following cluster control plane log types are available: \n\n* Kubernetes
      API server component logs (api) – Your cluster's API server is the control plane
      component that exposes the Kubernetes API.\n* Audit (audit) – Kubernetes audit
      logs provide a record of the individual users, administrators, or system components
      that have affected your cluster.\n* Authenticator (authenticator) – Authenticator
      logs are unique to Amazon EKS. These logs represent the control plane component
      that Amazon EKS uses for Kubernetes Role Based Access Control (RBAC) authentication
      using IAM credentials.\n* Controller manager (controllerManager) – The controller
      manager manages the core control loops that are shipped with Kubernetes.\n* Scheduler
      (scheduler) – The scheduler component manages when and where to run pods in your
      cluster."
    remediation:
      To update the current EKS cluster configuration to add logging to CloudWatch,
      run `aws eks --region <region> update-cluster-config --name <clustername> --logging
      '{'clusterLogging':[{'types':['api','audit','authenticator','controllerManager','scheduler'],'enabled':true}]}'`.  The
      configuration of the cluster will take several minutes to complete.
    validation:
      To determine the current control plane logging configuration, run `aws
      eks describe-cluster` and review the `clusterLogging` types that are `enabled`.  All
      should be listed as `enabled` and none should be listed as `disabled`.
    impact: 9
    effort: 2
    platform: AWS
    category: Security, Identity, and Compliance
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: EKS Control Plane Logging
        url: https://docs.aws.amazon.com/eks/latest/userguide/control-plane-logs.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-2
          - eks-cis1.0.1-2.1
          - eks-cis1.0.1-2.1.1
      - nist-csf:
          - nist-csf-de
          - nist-csf-de.ae
          - nist-csf-de.ae-1
          - nist-csf-pr
          - nist-csf-pr.pt
          - nist-csf-pr.pt-1
      - aws
  - id: darkbit-aws-67
    title: CloudTrail Log File Validation Should Be Enabled
    description:
      CloudTrail can optionally generate a SHA-256 hash for every file it
      delivers and an digest of hashes on an hourly basis.  This provides a forensic
      investigation with assurance that log files were not tampered with and can be
      relied on for accuracy and integrity.  The digests are delivered to a separate
      folder to allow for more granular permissions such as preventing deletion.
    remediation:
      Configure the `Enable Log File Validation` setting on each trail.  Using
      the CLI, run `aws cloudtrail update --name <trailname> --enable-log-file-validation`.
    validation:
      In each region, run `for i in $(aws cloudtrail list-trails --region
      <region> --query 'Trails[*].TrailARN' --output text); do aws cloudtrail describe-trails
      --trail-name $i --output json | jq -r '.trailList[] | "(.TrailARN) (.LogFileValidationEnabled)"';
      done` and ensure all cloudtrails are `true`.
    impact: 5
    effort: 2
    platform: AWS
    category: Security, Identity, and Compliance
    resource: CloudTrail
    nodes:
      - AWS_CLOUDTRAIL_TRAIL
    refs:
      - text: CloudTrail Log File Integrity
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-intro.html
      - text: CloudTrail Log File Integrity Validation
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/cloudtrail-log-file-validation-enabling.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level2
          - aws-cis1.3-3
          - aws-cis1.3-3.2
      - aws-cfg:
          - aws-cfg-cloud-trail-log-file-validation-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-10.5.2
          - pci-dss-3.2.1-10.5.5
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ds
          - nist-csf-pr.ds-6
      - nist-800-53-rev4:
          - nist-800-53-rev4-SI-7
          - nist-800-53-rev4-SI-7(1)
      - nist-800-171:
          - nist-800-171-3.13.1
          - nist-800-171-3.3.8
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.312(c)(1)
          - hipaa-164.312(c)(2)
      - fedramp-moderate:
          - fedramp-moderate-SI-7
          - fedramp-moderate-SI-7(1)
      - cmmc-level5:
          - cmmc-level5-CM.2.064
          - cmmc-level5-AU.3.049
          - cmmc-level5-CM.5.074
      - cmmc-level4:
          - cmmc-level4-CM.2.064
          - cmmc-level4-AU.3.049
      - cmmc-level3:
          - cmmc-level3-CM.2.064
          - cmmc-level3-AU.3.049
      - cmmc-level2:
          - cmmc-level2-CM.2.064
      - aws-wa-security:
          - aws-wa-security-SEC-4
      - aws
  - id: darkbit-aws-72
    title: S3 Buckets Should Not Be Exposed to the Public
    description:
      Giving all users the ability to read the contents of S3 buckets has
      led to unintentional data breaches for organizations of all sizes and maturity.  It
      often happens as the result of a misconfiguration or temporary policy applied
      for testing purposes.  In nearly all use cases, directly exposed S3 buckets to
      all users is not desired.
    remediation: |-
      For each bucket, review the permissions to ensure that `Everyone` (via the console) or `http://acs.amazonaws.com/groups/global/AllUsers` (via the API) are not granted any permissions.

      Strongly consider enabling all four of the S3 Block Public Access policies on the account to enforce non-public buckets regardless of how the resources are created.
    validation: |-
      Run the following commands in each region where buckets are present and verify that no permissions are granted to `http://acs.amazonaws.com/groups/global/AllUsers`.

      ```
      for i in $(aws s3api list-buckets --query 'Buckets[*].Name' --region $AWS_REGION --output text); do
        echo $i
        aws s3api get-bucket-acl \
          --region $AWS_REGION \
          --bucket $i
      done
      ```

      To check the S3 Block Public Access policy, run the following commands and ensure the policy shown is configured to block public access to the entire bucket.

      ```
      for i in $(aws s3api list-buckets --query 'Buckets[*].Name' --output text); do
        echo $i
        aws s3api get-public-access-block --bucket $i
      done
      ```
    impact: 9
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: S3
    nodes:
      - AWS_S3_BUCKET
    refs:
      - text: S3 Bucket Policies
        url: https://docs.aws.amazon.com/AmazonS3/latest/dev/acl-overview.html
      - text: Amazon S3 block public access
        url: https://docs.aws.amazon.com/AmazonS3/latest/dev/access-control-block-public-access.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.20
      - aws-cfg:
          - aws-cfg-s3-account-level-public-access-blocks
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-1.3
          - pci-dss-3.2.1-2.2
      - nist-csf:
          - nist-csf-PR.AC-3
          - nist-csf-PR.AC-5
          - nist-csf-PR.DS-5
          - nist-csf-PR.PT-3
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-3
          - nist-800-53-rev4-AC-4
          - nist-800-53-rev4-AC-6
          - nist-800-53-rev4-AC-21(b)
          - nist-800-53-rev4-SC-7
          - nist-800-53-rev4-SC-7(3)
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.14
          - nist-800-171-3.1.2
          - nist-800-171-3.1.20
          - nist-800-171-3.1.3
          - nist-800-171-3.13.2
          - nist-800-171-3.3.8
          - nist-800-171-3.4.6
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.308(a)(3)(i)
          - hipaa-164.312(a)(1)
      - fedramp-moderate:
          - fedramp-moderate-AC-3
          - fedramp-moderate-AC-4
          - fedramp-moderate-AC-6
          - fedramp-moderate-AC-21(b)
          - fedramp-moderate-SC-7
          - fedramp-moderate-SC-7(3)
      - fedramp-low:
          - fedramp-low-AC-3
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.003
          - cmmc-level5-SC.1.175
          - cmmc-level5-AC.2.007
          - cmmc-level5-AC.2.016
          - cmmc-level5-CM.2.062
          - cmmc-level5-AU.3.049
          - cmmc-level5-SC.3.180
          - cmmc-level5-AC.4.023
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.003
          - cmmc-level4-SC.1.175
          - cmmc-level4-AC.2.007
          - cmmc-level4-AC.2.016
          - cmmc-level4-CM.2.062
          - cmmc-level4-AU.3.049
          - cmmc-level4-SC.3.180
          - cmmc-level4-AC.4.023
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.003
          - cmmc-level3-SC.1.175
          - cmmc-level3-AC.2.007
          - cmmc-level3-AC.2.016
          - cmmc-level3-CM.2.062
          - cmmc-level3-AU.3.049
          - cmmc-level3-SC.3.180
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.003
          - cmmc-level2-SC.1.175
          - cmmc-level2-AC.2.007
          - cmmc-level2-AC.2.016
          - cmmc-level2-CM.2.062
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.003
          - cmmc-level1-SC.1.175
      - aws-wa-security:
          - aws-wa-security-SEC-5
          - aws-wa-security-SEC-8
      - aws
  - id: darkbit-aws-75
    title: KMS Keys Should Have Key Rotation Enabled
    description:
      Enabling automatic key rotation for a customer managed CMK means that
      AWS KMS generates new cryptographic material for the CMK every year. It also saves
      the CMK's older cryptographic material in perpetuity (deleted only when the CMK
      is deleted) so it can be used to decrypt data that it encrypted.  This process
      makes rotation seamless and automatic, and this typically satisfies compliance
      requirements for rotation of credentials.
    remediation:
      To enable automatic key rotation, run `aws kms enable-key-rotation
      --key-id <keyid>`.
    validation: |-
      In each region, run the following command and verify that all are `true`.

      ```
      for i in $(aws kms list-aliases --region $AWS_REGION --query 'Aliases[*].TargetKeyId' --output text); do \
        aws kms get-key-rotation-status \
        --region $AWS_REGION \
        --key-id $i | jq -r --arg ARN $i '"\($ARN) \(.KeyRotationEnabled)"'
      done
      ```
    impact: 5
    effort: 2
    platform: AWS
    category: Management and Governance
    resource: KMS
    nodes:
      - AWS_KMS_KEY
    refs:
      - text: KMS Key Rotation
        url: https://docs.aws.amazon.com/kms/latest/developerguide/rotate-keys.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level2
          - aws-cis1.3-3
          - aws-cis1.3-3.8
      - aws-cfg:
          - aws-cfg-cmk-backing-key-rotation-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-3.6.4
      - nist-800-53-rev4:
          - nist-800-53-rev4-SC-12
      - nist-800-171:
          - nist-800-171-3.13.10
      - fedramp-moderate:
          - fedramp-moderate-SC-12
      - fedramp-low:
          - fedramp-low-SC-12
      - cmmc-level5:
          - cmmc-level5-SC.3.187
      - cmmc-level4:
          - cmmc-level4-SC.3.187
      - cmmc-level3:
          - cmmc-level3-SC.3.187
      - aws-wa-security:
          - aws-wa-security-SEC-8
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ds
          - nist-csf-pr.ds-1
      - aws
  - id: darkbit-aws-108
    title: AWS <Root> Account Should Not Be Routinely Used
    description:
      With the creation of an AWS account, a `root` user is created that
      cannot be disabled or deleted. That user has unrestricted access
      to and control over all resources in the AWS account. It is highly
      recommended that the use of this account be avoided for everyday tasks.
    remediation: |
      Discontinue use of the `root` account for day-to-day actions. Consider 
      deleting the `root` access keys to prevent programatic usage of the `root` account.
    validation: |
      Generate a credential report with `aws iam generate-credential-report`. Then 
      download the report with `aws iam get-credential-report --query 'Content' 
      --output text | base64 -d | cut -d, -f1,5,11,16 | grep -B1 '<root_account>'`, 
      and confirm the root account password and access keys have not been recently 
      used.
    impact: 8
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
    refs:
      - text: AWS Account Root User
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_root-user.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.7
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-110
    title: IAM Users Should Have Only One Valid AWS Access Key Available
    description: |
      Access keys are long-term credentials. Access keys can be used to
      sign programmatic requests to the AWS CLI or AWS API. Minimize the risk 
      of credential leakage by not allowing users to have multiple access keys.
    remediation: |-
      If a user has multiple `Active` access keys, remove one after notifying them.

      ```
      aws iam update-access-key --access-key-id <access-key-id> \
        --status Inactive --user-name <user-name>
      ```
    validation: |-
      List all the IAM users in the account.

      ```
      aws iam list-users --query "Users[*].UserName"
      ```

      For each user, list the access keys and their status.

      ```
      aws iam list-access-keys --user-name <user-name>
      ```

      Verify that each user has only one `Active` access key.
    impact: 3
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
    refs:
      - text: AWS Access Keys Best Practices
        url: https://docs.aws.amazon.com/general/latest/gr/aws-access-keys-best-practices.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.13
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-111
    title: AWS Access Keys Should Be Rotated Every 90 Days
    description: |
      AWS Access Keys are used to make programmatic requests to AWS APIs, and they are 
      commonly stored on developer and administrator workstation disks, CI/CD systems, 
      and baked into applications.  Periodic rotation of all keys can reduce the time 
      window available for leaked or stolen credentials to be valid, and it helps 
      identify places where keys are "hardcoded" into applications and system configuration.
    remediation: |-
      Create a second access key.

      ```
      aws iam create-access-key
      ```

      After updating all applications to use the new key, wait several days.  Verify that the keys have not been used since the time of rotation.

      ```
      aws iam get-access-key-last-used
      ```

      Delete the previous key.

      ```
      aws iam delete-access-key
      ```
    validation: |-
      Generate a new AWS Credentials Report and review the `access_key_#_last_rotated` fields.

      ```
      aws iam generate-credential-report
      aws iam get-credential-report --query 'Content' --output text | base64 -d
      ```
    impact: 5
    effort: 7
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_IAM_USER
    refs:
      - text: Finding Unused AWS Credentials
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_finding-unused.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.14
      - aws-cfg:
          - aws-cfg-access-keys-rotated
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-8.2.4
      - nist-csf:
          - nist-csf-PR.AC-1
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(1)
          - nist-800-53-rev4-AC-2(j)
      - hipaa:
          - hipaa-164.308(a)(3)(ii)(C)
          - hipaa-164.308(a)(4)(ii)(C)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(1)
          - fedramp-moderate-AC-2(j)
      - fedramp-low:
          - fedramp-low-AC-2
      - aws-wa-security:
          - aws-wa-security-SEC-2
      - aws
  - id: darkbit-aws-113
    title: AWS IAM SSL/TLS Certificates Should Not Be Expired
    description: |
      SSL/TLS Certificates can be handled by AWS Certificate Manager (ACM) or imported from an 
      external provider and stored in IAM for use in regions where ACM is not supported.  
      Certificates stored in IAM should not be expired to avoid the risk of deploying it in 
      an expired state.
    remediation: |-
      Identify expired certificates by running the following command and looking at the `Expiration` field.

      ```
      aws iam list-server-certificates
      ```

      For certificates that have expired, delete them.

      ```
      aws iam delete-server-certificate --server-certificate-name <CERTIFICATE_NAME>
      ```
    validation: |-
      Review the `Expiration` field for any certificates that are listed with the following command.

      ```
      aws iam list-server-certificates
      ```
    impact: 5
    effort: 2
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - PLACEHOLDER
    refs:
      - text: Managing Server Certificates in IAM
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/id_credentials_server-certs.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-1
          - aws-cis1.3-1.19
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-114
    title: AWS IAM Access Analyzer Should Be Enabled
    description: |
      AWS IAM Access Analyzer helps organizations identify the resources in the 
      organization and accounts that are shared with an external entity.  For 
      each instance, Access Analyzer generates a finding with information about 
      the access and the external principal to allow administrators to determine 
      if the access is unintended and a security risk.
    remediation: |-
      Enable Access Analyzer for the intended region(s).

      ```
      aws accessanalyzer create-analyzer \
        --analyzer-name <name> \
        --type <ACCOUNT|ORGANIZATION>
      ```
    validation: |-
      Run the following command against each region and check for an `ACTIVE` status.

      ```
      aws accessanalyzer list-analyzers --region <region>
      ```
    impact: 6
    effort: 4
    platform: AWS
    category: Identity and Access Management
    resource: IAM
    nodes:
      - AWS_ACCESS_ANALYZER
    refs:
      - text: Using AWS IAM Access Analyzer
        url: https://docs.aws.amazon.com/IAM/latest/UserGuide/what-is-access-analyzer.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-1
          - aws-cis1.3-1.21
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-115
    title: S3 Buckets Should Be Encrypted At Rest
    description: Encrypting data at rest stored in S3 Buckets requires the combination of IAM access and access to/possession of the decryption keys to be read, and this provides a stronger defense than protection with IAM alone for little to no additional cost.
    remediation: |-
      Run either of the following commands for each non-server access logging bucket.

      ```
       aws s3api put-bucket-encryption \
        --bucket <bucket name> \
        --server-side-encryption-configuration '{"Rules": [{"ApplyServerSideEncryptionByDefault": {"SSEAlgorithm": "AES256"}}]}'
      ```

      or

      ```
      aws s3api put-bucket-encryption \
        --bucket <bucket name> \
        --server-side-encryption-configuration '{"Rules": [{"ApplyServerSideEncryptionByDefault":{"SSEAlgorithm": "aws:kms","KMSMasterKeyID": "aws/s3"}}]}'
      ```
    validation: |-
      For each bucket, run the following command and verify it displays `AES256` or `aws:kms` for the `SSEAlgorithm` field.

      ```
      aws s3api get-bucket-encryption --bucket <bucket name>
      ```
    impact: 5
    effort: 5
    platform: AWS
    category: Storage
    resource: S3
    nodes:
      - PLACEHOLDER
    refs:
      - text: S3 Server Side Encryption
        url: https://docs.aws.amazon.com/AmazonS3/latest/dev/bucket-encryption.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-level2
          - aws-cis1.3-2
          - aws-cis1.3-2.1
          - aws-cis1.3-2.1.1
      - aws-cfg:
          - aws-cfg-s3-bucket-server-side-encryption-enabled
          - aws-cfg-s3-default-encryption-kms
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-3.4
      - nist-csf:
          - nist-csf-PR.DS-1
      - nist-800-53-rev4:
          - nist-800-53-rev4-SC-13
          - nist-800-53-rev4-SC-28
      - nist-800-171:
          - nist-800-171-3.13.16
          - nist-800-171-3.3.8
          - nist-800-171-3.5.10
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.312(a)(2)(iv)
          - hipaa-164.312(e)(2)(ii)
      - fedramp-moderate:
          - fedramp-moderate-SC-28
      - fedramp-low:
          - fedramp-low-SC-13
      - cmmc-level5:
          - cmmc-level5-IA.2.081
          - cmmc-level5-AU.3.049
          - cmmc-level5-SC.3.191
      - cmmc-level4:
          - cmmc-level4-IA.2.081
          - cmmc-level4-AU.3.049
          - cmmc-level4-SC.3.191
      - cmmc-level3:
          - cmmc-level3-IA.2.081
          - cmmc-level3-AU.3.049
          - cmmc-level3-SC.3.191
      - cmmc-level2:
          - cmmc-level2-IA.2.081
      - aws-wa-security:
          - aws-wa-security-SEC-8
      - aws
  - id: darkbit-aws-116
    title: AWS EBS Volume Default Encryption Should Be Enabled
    description: |
      Although disabled by default, Elastic Block Store (EBS) Volumes can be set to automatically enabled on 
      new volumes as they are created using the default key provided by AWS or a user-supplied KMS key in 
      each desired region. Encrypting data at rest stored in EBS volumes requires the combination of IAM 
      access and access to/possession of the decryption keys to be read, and this provides a stronger 
      defense than protection with IAM alone for little to no additional cost.
    remediation: |-
      In each region, run the following command.

      ```
      aws --region <region> ec2 enable-ebs-encryption-by-default
      ```
    validation: |-
      In each region, run the following command and ensure it returns `true`.

      ```
      aws --region <region> ec2 get-ebs-encryption-by-default
      ```
    impact: 4
    effort: 5
    platform: AWS
    category: Storage
    resource: EC2
    nodes:
      - PLACEHOLDER
    refs:
      - text: EBS Volume Encryption By Default
        url: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/EBSEncryption.html#encryption-by-default
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-level2
          - aws-cis1.3-2
          - aws-cis1.3-2.2
          - aws-cis1.3-2.2.1
      - aws-cfg:
          - aws-cfg-ec2-ebs-encryption-by-default
      - nist-800-53-rev4:
          - nist-800-53-rev4-SC-28
      - nist-800-171:
          - nist-800-171-3.13.16
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.312(a)(2)(iv)
          - hipaa-164.312(e)(2)(ii)
      - fedramp-moderate:
          - fedramp-moderate-SC-28
      - fedramp-low:
          - fedramp-low-SC-13
      - cmmc-level5:
          - cmmc-level5-IA.2.081
          - cmmc-level5-SC.3.191
      - cmmc-level4:
          - cmmc-level4-IA.2.081
          - cmmc-level4-SC.3.191
      - cmmc-level3:
          - cmmc-level3-IA.2.081
          - cmmc-level3-SC.3.191
      - cmmc-level2:
          - cmmc-level2-IA.2.081
      - aws-wa-security:
          - aws-wa-security-SEC-8
      - aws
  - id: darkbit-aws-117
    title: CloudTrail Logging Bucket Should Not Be Public
    description: CloudTrail logs a record of every API call made in an AWS Account to a specified S3 bucket, and this bucket should not have a bucket policy or ACL that permits public access of any kind.
    remediation: |-
      Obtain the S3 Bucket names used for CloudTrail logging.

      ```
      aws cloudtrail describe-trails --query 'trailList[*].S3BucketName' --output text
      ```

      In the AWS Console, navigate to the S3 dashboard, click on the bucket name.  Under the `Permissions` tab, remove ACL grants to `https://acs.amazonaws.com/groups/global/AllUsers` or `https://acs.amazonaws.com/groups/global/AuthenticatedUsers` if they are present.  Modify the Bucket Policy to remove access granted to `"*"` or `{"AWS": "*"}`.
    validation: |-
      Obtain the S3 Bucket names used for CloudTrail logging.

      ```
      aws cloudtrail describe-trails --query 'trailList[*].S3BucketName' --output text
      ```

      Ensure no privileges are granted to `AllUsers` or `AuthenticatedUsers`.

      ```
      aws s3api get-bucket-acl --bucket <bucket_name> --query 'Grants[?Grantee.URI==`https://acs.amazonaws.com/groups/global/AllUsers`]'
      ```

      and

      ```
      aws s3api get-bucket-acl --bucket <bucket_name> --query 'Grants[?Grantee.URI==`https://acs.amazonaws.com/groups/global/AuthenticatedUsers`]'
      ```

      Finally, ensure the bucket policy does not `Allow` a `Principal` of `"*"` or `{"AWS": "*"}`

      ```
      aws s3api get-bucket-policy --bucket <bucket_name>  --output json | jq -r '.Policy' | sed -e 's/\\"/"/g' | jq
      ```
    impact: 5
    effort: 3
    platform: AWS
    category: Identity and Access Management
    resource: CloudTrail
    nodes:
      - AWS_CLOUDTRAIL_TRAIL
    refs:
      - text: AWS Bucket Policies
        url: https://docs.aws.amazon.com/AmazonS3/latest/dev/using-iam-policies.html
      - text: AWS Bucket ACLs
        url: https://docs.aws.amazon.com/AmazonS3/latest/dev/S3_ACLs_UsingACLs.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-3
          - aws-cis1.3-3.3
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-118
    title: CloudTrail Logs Should Be Integrated With CloudWatch Logs
    description: AWS CloudTrail records the identity, source IP, and other key details of requests made to AWS APIs to an S3 bucket for historical storage purposes.  To be able to implement alerts, notifications, and other actions in response to sensitive or anomalous activities, the logs from CloudTrail should be delivered to a CloudWatch log group.  In addition, the retention policy of logs stored in CloudTrail S3 buckets and CloudWatch log groups should be set to at least 365 days to support incident response needs.
    remediation: |-
      Configure the CloudTrail trail with a CloudWatch log group and role ARN

      ```
      aws cloudtrail update-trail --name <trail_name> --cloudwatch-logs-log-group-arn <cloudwatch-log-group-arn> --cloudwatch-logs-role-arn <role_arn>
      ```
    validation: |-
      For each CloudTrail, ensure the timestamp for the last delivery to the CloudWatch log group is less than 24 hours old and not "null"

      ```
      for i in $(aws cloudtrail describe-trails --output json | jq -r '.trailList[] | .Name'); do echo -n "$i: "; aws cloudtrail get-trail-status --name $i --output json | jq -r '"\(.LatestcloudwatchLogdDeliveryTime)"'; done
      ```
    impact: 5
    effort: 3
    platform: AWS
    category: Security, Identity, and Compliance
    resource: CloudWatch
    nodes:
      - AWS_CLOUDTRAIL_TRAIL
    refs:
      - text: Sending CloudTrail Logs to CloudWatch
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/send-cloudtrail-events-to-cloudwatch-logs.html
      - text: Monitoring CloudTrail Log Files
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/monitor-cloudtrail-log-files-with-cloudwatch-logs.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-3
          - aws-cis1.3-3.4
      - aws-cfg:
          - aws-cfg-cloud-trail-cloud-watch-logs-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-10.2
      - nist-csf:
          - nist-csf-PR.PT-1
          - nist-csf-DE.AE-1
          - nist-csf-DE.AE-3
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(4)
          - nist-800-53-rev4-AC-2(g)
          - nist-800-53-rev4-AU-2(a)(d)
          - nist-800-53-rev4-AU-3
          - nist-800-53-rev4-AU-6(1)(3)
          - nist-800-53-rev4-AU-7(1)
          - nist-800-53-rev4-AU-12(a)(c)
          - nist-800-53-rev4-CA-7(a)(b)
          - nist-800-53-rev4-SI-4(2)
          - nist-800-53-rev4-SI-4(4)
          - nist-800-53-rev4-SI-4(5)
          - nist-800-53-rev4-SI-4(a)(b)(c)
      - nist-800-171:
          - nist-800-171-3.3.1
          - nist-800-171-3.3.2
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(D)
          - hipaa-164.308(a)(6)(ii)
          - hipaa-164.312(b)
          - hipaa-164.312(e)(2)(i)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(4)
          - fedramp-moderate-AC-2(g)
          - fedramp-moderate-AU-2(a)(d)
          - fedramp-moderate-AU-3
          - fedramp-moderate-AU-6(1)(3)
          - fedramp-moderate-AU-7(1)
          - fedramp-moderate-AU-12(a)(c)
          - fedramp-moderate-CA-7(a)(b)
          - fedramp-moderate-SI-4(2)
          - fedramp-moderate-SI-4(4)
          - fedramp-moderate-SI-4(5)
          - fedramp-moderate-SI-4(a)(b)(c)
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-AC.2.013
          - cmmc-level5-AU.2.041
          - cmmc-level5-AU.2.042
          - cmmc-level5-SI.2.214
          - cmmc-level5-SI.2.216
          - cmmc-level5-SI.2.217
          - cmmc-level5-AU.3.046
          - cmmc-level5-AU.4.053
          - cmmc-level5-AU.5.055
          - cmmc-level5-SI.5.223
      - cmmc-level4:
          - cmmc-level4-AC.2.013
          - cmmc-level4-AU.2.041
          - cmmc-level4-AU.2.042
          - cmmc-level4-SI.2.214
          - cmmc-level4-SI.2.216
          - cmmc-level4-SI.2.217
          - cmmc-level4-AU.3.046
          - cmmc-level4-AU.4.053
      - cmmc-level3:
          - cmmc-level3-AC.2.013
          - cmmc-level3-AU.2.041
          - cmmc-level3-AU.2.042
          - cmmc-level3-SI.2.214
          - cmmc-level3-SI.2.216
          - cmmc-level3-SI.2.217
          - cmmc-level3-AU.3.046
      - cmmc-level2:
          - cmmc-level2-AC.2.013
          - cmmc-level2-AU.2.041
          - cmmc-level2-AU.2.042
          - cmmc-level2-SI.2.214
          - cmmc-level2-SI.2.216
          - cmmc-level2-SI.2.217
      - aws-wa-security:
          - aws-wa-security-SEC-4
      - aws
  - id: darkbit-aws-119
    title: CloudTrail Logs Should Be Encrypted At Rest With a KMS Key
    description: To further protect the potentially sensitive logs generated by AWS CloudTrail as they are stored in an S3 bucket, it's recommended to enable integration with KMS to encrypt them with a customer-managed key before they are written.  This helps ensure that identities with the ability to read those files in S3 must also have access to decrypt/read the KMS key that was used to originally encrypt them, and this greatly increases an attacker's cost of obtaining these logs.
    remediation: |-
      Update the CloudTrail trail with a KMS key id.

      ```
      aws cloudtrail update-trail --name <trail_name> --kms-id <kms-key-id>
      ```

      Modify the KMS key policy to grant CloudTrail access to that KMS key

      ```
      aws kms put-key-policy --key-id <kms-key-id> --policy <kms-key-policy>
      ``` 
    validation: |-
      For each region, ensure each trail has a valid `KmsKeyId` assigned

      ```
      aws cloudtrail describe-trails \
        --region <aws-region> \
        --output json | jq -r '.trailList[] | "\(.Name) \(.KmsKeyId)"'
      ```
    impact: 3
    effort: 8
    platform: AWS
    category: Security, Identity, and Compliance
    resource: CloudTrail
    nodes:
      - AWS_CLOUDTRAIL_TRAIL
    refs:
      - text: Encrypting CloudTrail Logs with SSE-KMS
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/encrypting-cloudtrail-log-files-with-aws-kms.html
      - text: Configuring KMS Key Policies for CloudTrail
        url: https://docs.aws.amazon.com/awscloudtrail/latest/userguide/create-kms-key-policy-for-cloudtrail.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level2
          - aws-cis1.3-3
          - aws-cis1.3-3.7
      - aws-cfg:
          - aws-cfg-cloud-trail-encryption-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-10.5.1
      - nist-csf:
          - nist-csf-PR.DS-1
      - nist-800-53-rev4:
          - nist-800-53-rev4-AU-9
          - nist-800-53-rev4-SC-13
          - nist-800-53-rev4-SC-28
      - nist-800-171:
          - nist-800-171-3.13.16
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(B)
          - hipaa-164.312(a)(2)(iv)
          - hipaa-164.312(e)(2)(ii)
      - fedramp-moderate:
          - fedramp-moderate-AU-9
          - fedramp-moderate-SC-28
      - fedramp-low:
          - fedramp-low-AU-9
          - fedramp-low-SC-13
      - cmmc-level5:
          - cmmc-level5-AU.3.049
          - cmmc-level5-SC.3.191
      - cmmc-level4:
          - cmmc-level4-AU.3.049
          - cmmc-level4-SC.3.191
      - cmmc-level3:
          - cmmc-level3-AU.3.049
          - cmmc-level3-SC.3.191
      - aws-wa-security:
          - aws-wa-security-SEC-4
          - aws-wa-security-SEC-8
      - aws
  - id: darkbit-aws-120
    title: S3 Object Level Logging to CloudTrail For Read & Write Events Should Be Enabled
    description: |
      By default, CloudTrail trails don't log data events for S3 object-level API 
      operations such as `GetObject`, `DeleteObject`, and `PutObject`. Enabling 
      object-level logging will help you meet certain compliance requirements, 
      perform comprehensive security analysis, monitor specific patterns of user 
      behavior, or take immediate actions on any object-level S3 API activity using 
      Amazon CloudWatch Events.
    remediation: |-
      In the AWS Console, visit [https://console.aws.amazon.com/cloudtrail/#/trails](https://console.aws.amazon.com/cloudtrail/#/trails), select the desired trail, click `Edit` on the `Data events : S3` section, and ensure the `Data Events` checkbox is selected.  Then save changes.
    validation: |-
      List the event selectors associated with each CloudTrail and ensure they are not empty and return a`ReadWriteType` of `All`, a `Type` of `AWS::S3::Object` and `Values` of an array of `arn:aws:s3` or specific S3 bucket ARNs

      ```
      for region in $(aws ec2 describe-regions --query Regions[].RegionName --output text); do 
        for trail in $(aws cloudtrail list-trails --region $region --query Trails[*].Name --output text); do 
          aws cloudtrail get-event-selectors --region $region --trail-name $trail --query EventSelectors[*].DataResources[]; 
        done;
      done
      ```
    impact: 3
    effort: 6
    platform: AWS
    category: Security, Identity, and Compliance
    resource: S3
    nodes:
      - PLACEHOLDER
    refs:
      - text: CloudTrail Data Event Logging
        url: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level2
          - aws-cis1.3-3
          - aws-cis1.3-3.10
      - aws-cfg:
          - aws-cfg-cloudtrail-s3-dataevents-enabled
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-10.3.1
          - pci-dss-3.2.1-10.3.2
          - pci-dss-3.2.1-10.3.3
          - pci-dss-3.2.1-10.3.4
          - pci-dss-3.2.1-10.3.5
          - pci-dss-3.2.1-10.3.6
      - nist-csf:
          - nist-csf-PR.DS-5
          - nist-csf-DE.AE-3
          - nist-csf-DE.AE-4
          - nist-csf-DE.CM-1
          - nist-csf-DE.CM-3
          - nist-csf-DE.CM-6
          - nist-csf-DE.CM-7
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-2(g)
          - nist-800-53-rev4-AU-2(a)(d)
          - nist-800-53-rev4-AU-3
          - nist-800-53-rev4-AU-12(a)(c)
      - nist-800-171:
          - nist-800-171-3.1.12
          - nist-800-171-3.13.1
          - nist-800-171-3.14.6
          - nist-800-171-3.14.7
          - nist-800-171-3.3.1
          - nist-800-171-3.3.2
      - hipaa:
          - hipaa-164.308(a)(1)(ii)(D)
          - hipaa-164.308(a)(3)(ii)(A)
          - hipaa-164.308(a)(6)(ii)
          - hipaa-164.312(b)
          - hipaa-164.312(e)(2)(i)
      - fedramp-moderate:
          - fedramp-moderate-AC-2(g)
          - fedramp-moderate-AU-2(a)(d)
          - fedramp-moderate-AU-3
          - fedramp-moderate-AU-12(a)(c)
      - fedramp-low:
          - fedramp-low-AC-2
      - cmmc-level5:
          - cmmc-level5-AU.2.041
          - cmmc-level5-AU.2.042
          - cmmc-level5-SI.2.217
          - cmmc-level5-AC.3.018
          - cmmc-level5-AU.5.055
          - cmmc-level5-SI.5.223
      - cmmc-level4:
          - cmmc-level4-AU.2.041
          - cmmc-level4-AU.2.042
          - cmmc-level4-SI.2.217
          - cmmc-level4-AC.3.018
      - cmmc-level3:
          - cmmc-level3-AU.2.041
          - cmmc-level3-AU.2.042
          - cmmc-level3-SI.2.217
          - cmmc-level3-AC.3.018
      - cmmc-level2:
          - cmmc-level2-AU.2.041
          - cmmc-level2-AU.2.042
          - cmmc-level2-SI.2.217
      - aws-wa-security:
          - aws-wa-security-SEC-4
      - aws
  # DUPE - redundant with aws-120
  # - id: darkbit-aws-121
  #   title: S3 Object Level Logging to CloudTrail For Read Events Should Be Enabled
  #   description: By default, CloudTrail trails don't log data events for S3 object-level API operations such as `GetObject`, `DeleteObject`, and `PutObject`. Enabling object-level logging will help you meet certain compliance requirements, perform comprehensive security analysis, monitor specific patterns of user behavior, or take immediate actions on any object-level S3 API activity using Amazon CloudWatch Events.
  #   remediation: |-
  #     In the AWS Console, visit [https://console.aws.amazon.com/cloudtrail/#/trails](https://console.aws.amazon.com/cloudtrail/#/trails), select the desired trail, click `Edit` on the `Data events : S3` section, and ensure the `Data Events` checkbox is selected.  Then save changes.
  #   validation: |-
  #     List the event selectors associated with each CloudTrail and ensure they are not empty and return a `Type` of `AWS::S3::Object` and `Values` of an array of `arn:aws:s3` or specific S3 bucket ARNs

  #     ```
  #     for region in $(aws ec2 describe-regions --query Regions[].RegionName --output text); do 
  #       for trail in $(aws cloudtrail list-trails --region $region --query Trails[*].Name --output text); do 
  #         aws cloudtrail get-event-selectors --region $region --trail-name $trail --query EventSelectors[*].DataResources[]; 
  #       done;
  #     done
  #     ```
  #   impact: 3
  #   effort: 6
  #   platform: AWS
  #   category: Security, Identity, and Compliance
  #   resource: S3
  #   nodes:
  #     - PLACEHOLDER
  #   refs:
  #     - text: CloudTrail Data Event Logging
  #       url: https://docs.aws.amazon.com/AmazonS3/latest/user-guide/enable-cloudtrail-events.html
  #   public: true
  #   tags:
  #     - cis:
  #         - aws-cis1.3
  #         - aws-cis1.3-level2
  #         - aws-cis1.3-3
  #         - aws-cis1.3-3.11
  #     - aws-cfg:
  #         - aws-cfg-cloudtrail-s3-dataevents-enabled
  #     - pci-dss-3.2.1:
  #         - pci-dss-3.2.1-10.3.1
  #         - pci-dss-3.2.1-10.3.2
  #         - pci-dss-3.2.1-10.3.3
  #         - pci-dss-3.2.1-10.3.4
  #         - pci-dss-3.2.1-10.3.5
  #         - pci-dss-3.2.1-10.3.6
  #     - nist-csf:
  #         - nist-csf-PR.DS-5
  #         - nist-csf-DE.AE-3
  #         - nist-csf-DE.AE-4
  #         - nist-csf-DE.CM-1
  #         - nist-csf-DE.CM-3
  #         - nist-csf-DE.CM-6
  #         - nist-csf-DE.CM-7
  #     - nist-800-53-rev4:
  #         - nist-800-53-rev4-AC-2(g)
  #         - nist-800-53-rev4-AU-2(a)(d)
  #         - nist-800-53-rev4-AU-3
  #         - nist-800-53-rev4-AU-12(a)(c)
  #     - nist-800-171:
  #         - nist-800-171-3.1.12
  #         - nist-800-171-3.13.1
  #         - nist-800-171-3.14.6
  #         - nist-800-171-3.14.7
  #         - nist-800-171-3.3.1
  #         - nist-800-171-3.3.2
  #     - hipaa:
  #         - hipaa-164.308(a)(1)(ii)(D)
  #         - hipaa-164.308(a)(3)(ii)(A)
  #         - hipaa-164.308(a)(6)(ii)
  #         - hipaa-164.312(b)
  #         - hipaa-164.312(e)(2)(i)
  #     - fedramp-moderate:
  #         - fedramp-moderate-AC-2(g)
  #         - fedramp-moderate-AU-2(a)(d)
  #         - fedramp-moderate-AU-3
  #         - fedramp-moderate-AU-12(a)(c)
  #     - fedramp-low:
  #         - fedramp-low-AC-2
  #     - cmmc-level5:
  #         - cmmc-level5-AU.2.041
  #         - cmmc-level5-AU.2.042
  #         - cmmc-level5-SI.2.217
  #         - cmmc-level5-AC.3.018
  #         - cmmc-level5-AU.5.055
  #         - cmmc-level5-SI.5.223
  #     - cmmc-level4:
  #         - cmmc-level4-AU.2.041
  #         - cmmc-level4-AU.2.042
  #         - cmmc-level4-SI.2.217
  #         - cmmc-level4-AC.3.018
  #     - cmmc-level3:
  #         - cmmc-level3-AU.2.041
  #         - cmmc-level3-AU.2.042
  #         - cmmc-level3-SI.2.217
  #         - cmmc-level3-AC.3.018
  #     - cmmc-level2:
  #         - cmmc-level2-AU.2.041
  #         - cmmc-level2-AU.2.042
  #         - cmmc-level2-SI.2.217
  #     - aws-wa-security:
  #         - aws-wa-security-SEC-4
  #     - aws
  - id: darkbit-aws-122
    title: Network ACLs Should Not Allow Access to Administrative Ports From All Hosts
    description: Network Access Control Lists (NACLs) provide stateless network traffic filtering of both ingress and egress packets as they traverse a subnet in a VPC, and they can complement the filtering of Security Group rules by filtering certain types of traffic "early".  The recommendation is to allow ingress access to `tcp/22` and `tcp/3389` from specific IPs/Ranges instead of `0.0.0.0/0` using NACLs so that a misconfiguration of a Security Group does not expose a system accidentally.
    remediation: |-
      Visit [https://console.aws.amazon.com/vpc/#acls](https://console.aws.amazon.com/vpc/#acls), select each NACL, and modify the source subnet of any `Inbound` rules that `Allow` to `0.0.0.0/0` via `All` protocols and port ranges that include `tcp/22` and `tcp/3389`..
    validation: |-
      Run the following command in each region and ensure that no NACLs provide `Ingress: true`, `Protocol: "-1"` or `Protocol: "tcp"`,`RuleAction: "allow"` where one or more `Associations` are present.

      ```
      aws ec2 describe-network-acls --region <aws-region> --query NetworkAcls[][] --output json | jq -r '.[]'
      ```
    impact: 2
    effort: 7
    platform: AWS
    category: Network Access Control
    resource: Network ACL
    nodes:
      - PLACEHOLDER
    refs:
      - text: VPC Network Access Control Lists
        url: https://docs.aws.amazon.com/vpc/latest/userguide/vpc-network-acls.html
      - text: VPC Security Groups versus NACLs
        url: https://docs.aws.amazon.com/vpc/latest/userguide/VPC_Security.html
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level1
          - aws-cis1.3-5
          - aws-cis1.3-5.1
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-123
    title: VPC Default Security Groups Should Restrict All Traffic
    description: AWS VPCs are provisioned with a default security group whose initial settings deny all inbound traffic, allow all outbound traffic, and allow all traffic between instances assigned to the security group. EC2 instances that are launched without specifying a security group have the default VPC security group automatically assigned.  To avoid any implicitly granted network access using a shared resource, it's recommended that the default security group for each VPC have its rules configured to explicitly deny all traffic in both directions.  Instances should be assigned their own security group with the necessary access for its intended purpose to avoid any unintended consequences resulting from future changes.
    remediation: |-
      For VPCs where the default security group is not assigned to any active instances, visit [https://console.aws.amazon.com/vpc/#securityGroups:](https://console.aws.amazon.com/vpc/#securityGroups:), find the `default` security group, select it, and remove all `ingress` and `egress` `allow` rules.

      For VPCs where the default security group is in use, VPC flow logging can be helpful in profiling actual network traffic used by live systems to identify the requirements for each new security group.
    validation: |-
      Run the following command in each region and verify that both numbers representing the count of `ingress` and `egress` rules are `0`.

      ```
      aws ec2 describe-security-groups --region <aws_region> --output json | jq -r '.SecurityGroups[] | select(.GroupName=="default") | "\(.Description) \(.IpPermissions | length) \(.IpPermissionsEgress | length)"'
      ```
    impact: 3
    effort: 9
    platform: AWS
    category: Network Access Control
    resource: Security Groups
    nodes:
      - PLACEHOLDER
    refs:
      - text: EC2 Security Groups
        url: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/ec2-security-groups.html
      - text: Default EC2 Security Groups
        url: https://docs.aws.amazon.com/AWSEC2/latest/UserGuide/default-custom-security-groups.html#default-security-group
    public: true
    tags:
      - cis:
          - aws-cis1.3
          - aws-cis1.3-level2
          - aws-cis1.3-5
          - aws-cis1.3-5.3
      - aws-cfg:
          - aws-cfg-vpc-default-security-group-closed
      - pci-dss-3.2.1:
          - pci-dss-3.2.1-1.2
          - pci-dss-3.2.1-1.3
          - pci-dss-3.2.1-2.1
      - nist-csf:
          - nist-csf-PR.AC-3
          - nist-csf-PR.AC-5
          - nist-csf-DE.AE-1
      - nist-800-53-rev4:
          - nist-800-53-rev4-AC-4
          - nist-800-53-rev4-SC-7
          - nist-800-53-rev4-SC-7(3)
      - nist-800-171:
          - nist-800-171-3.1.1
          - nist-800-171-3.1.14
          - nist-800-171-3.1.2
          - nist-800-171-3.1.20
          - nist-800-171-3.1.3
          - nist-800-171-3.13.2
          - nist-800-171-3.4.7
      - fedramp-moderate:
          - fedramp-moderate-AC-4
          - fedramp-moderate-SC-7
          - fedramp-moderate-SC-7(3)
      - cmmc-level5:
          - cmmc-level5-AC.1.001
          - cmmc-level5-AC.1.003
          - cmmc-level5-SC.1.175
          - cmmc-level5-AC.2.016
          - cmmc-level5-CM.3.068
          - cmmc-level5-SC.3.180
          - cmmc-level5-AC.4.023
          - cmmc-level5-RM.4.151
          - cmmc-level5-SC.5.230
          - cmmc-level5-SC.5.208
      - cmmc-level4:
          - cmmc-level4-AC.1.001
          - cmmc-level4-AC.1.003
          - cmmc-level4-SC.1.175
          - cmmc-level4-AC.2.016
          - cmmc-level4-CM.3.068
          - cmmc-level4-SC.3.180
          - cmmc-level4-AC.4.023
          - cmmc-level4-RM.4.151
      - cmmc-level3:
          - cmmc-level3-AC.1.001
          - cmmc-level3-AC.1.003
          - cmmc-level3-SC.1.175
          - cmmc-level3-AC.2.016
          - cmmc-level3-CM.3.068
          - cmmc-level3-SC.3.180
      - cmmc-level2:
          - cmmc-level2-AC.1.001
          - cmmc-level2-AC.1.003
          - cmmc-level2-SC.1.175
          - cmmc-level2-AC.2.016
      - cmmc-level1:
          - cmmc-level1-AC.1.001
          - cmmc-level1-AC.1.003
          - cmmc-level1-SC.1.175
      - aws-wa-security:
          - aws-wa-security-SEC-5
      - aws
  - id: darkbit-aws-125
    title: Amazon EKS Nodegroups Should Run EKS Optimized AMIs
    description: |
      Unless a specific technical restriction requires a custom AMI, the recommendation is to leverage the operational expertise, configuration hardening, stability fixes, and security updates of EKS optimized AMIs maintained by AWS.  Many Kubernetes-specific settings for worker node security posture have already been implemented and tested, and this can reduce the amount of work necessary to validate your EKS cluster posture against industry-accepted security benchmarks like the CIS EKS Benchmark.
    remediation: |
      Create new EKS managed NodeGroups and select `Amazon Linux 2 (AL2_x86_64)` for non-GPU instances, `Amazon Linux 2 GPU Enabled (AL2_x86_64_GPU)` for GPU instances, or `Amazon Linux 2 (AL2_ARM_64)` for Arm instances for the `AMI type` setting.
    validation: |-
      Run the following command to list node groups for each EKS cluster.

      ```
      aws eks list-nodegroups --cluster-name <cluster-name>
      ```

      And run the following command for each node group name to validate the `AMI type` setting..

      ```
      aws eks describe-nodegroup --cluster-name <cluster-name> --nodegroup-name <nodegroup-name>
      ```
    impact: 6
    effort: 9
    platform: AWS
    category: Management and Governance
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: EKS Optimized AMIs
        url: https://docs.aws.amazon.com/eks/latest/userguide/eks-optimized-ami.html
      - text: EKS Optimized AMI Build Scripts
        url: https://github.com/awslabs/amazon-eks-ami
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-3
          - eks-cis1.0.1-3.1
          - eks-cis1.0.1-3.1.1
          - eks-cis1.0.1-3.1.2
          - eks-cis1.0.1-3.1.3
          - eks-cis1.0.1-3.1.4
          - eks-cis1.0.1-3.2
          - eks-cis1.0.1-3.2.1
          - eks-cis1.0.1-3.2.2
          - eks-cis1.0.1-3.2.3
          - eks-cis1.0.1-3.2.4
          - eks-cis1.0.1-3.2.5
          - eks-cis1.0.1-3.2.6
          - eks-cis1.0.1-3.2.7
          - eks-cis1.0.1-3.2.8
          - eks-cis1.0.1-3.2.9
          - eks-cis1.0.1-3.2.10
          - eks-cis1.0.1-3.2.11
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-126
    title: Amazon EKS Clusters Should Have The Latest CNI Plugin That Supports Network Policy
    description: |
      By default in Kubernetes, all Pods can communicate with each other
      by IP and egress to any subnet (including the Internet) unless routing or firewalls
      are added to prevent that traffic.  This presents ample opportunity for lateral
      movement from the perspective of a compromised workload. One of the best ways
      to reduce the scope of that movement is to deploy `NetworkPolicy` resources that
      define firewall rules for pod-to-pod traffic.  In EKS, a CNI that supports Network
      Policy must be installed to allow for the cluster to enforce firewall policies on Pods.
    remediation: |
      Install a CNI plugin such as `Calico` using a supported version appropriate for the EKS
      cluster as a part of the steps taken just after installation and before workloads are
      deployed.
    validation: |-
      Run the following command to validate the presence of a `DaemonSet` corresponding to a CNI plugin with `NetworkPolicy` support.  A common example might be `calico-node` in the `kube-system` namespace.

      ```
      kubectl get ds -A
      ```
    impact: 8
    effort: 8
    platform: AWS
    category: Network Access Control
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: AWS EKS Calico CNI Installation
        url: https://docs.aws.amazon.com/eks/latest/userguide/calico.html
      - text: Sample Kubernetes Network Policies
        url: https://github.com/ahmetb/kubernetes-network-policy-recipes
      - text: Sample Calico Network Policies
        url: https://docs.projectcalico.org/security/calico-network-policy
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-4
          - eks-cis1.0.1-4.3
          - eks-cis1.0.1-4.3.1
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-128
    title: Amazon EKS Clusters Should Have Read-Only Permissions to ECR Registries
    description: |
      In production clusters, container images should not be built inside the cluster, so "push" or write access to ECR is not necessary. Therefore, the worker instance's IAM Role should have read-only access to ECR sufficient for reading or "pulling" container images to follow the principle of least privilege and prevent a malicious actor with access to the instance credentials from being able to insert a backdoor into container images hosted in ECR.
    remediation: |
      Review the IAM Policies attached to the worker node's Instance Role and ensure the only policies attached are the `AmazonEKSWorkerNodePolicy`, `AmazonEC2ContainerRegistryReadOnly`, and optionally `AmazonEKS_CNI_Policy`.  Note that changing the permissions attached to the instances may break applications running in Pods on those nodes if they rely on the credentials provided to the instance for AWS API access.
    validation: |-
      Review the IAM Policies attached to the worker node's Instance Role and ensure the only ECR permissions are as follows.

      ```
      {
          "Version": "2012-10-17",
          "Statement": [
              {
                  "Effect": "Allow",
                  "Action": [
                      "ecr:GetAuthorizationToken",
                      "ecr:BatchCheckLayerAvailability",
                      "ecr:GetDownloadUrlForLayer",
                      "ecr:GetRepositoryPolicy",
                      "ecr:DescribeRepositories",
                      "ecr:ListImages",
                      "ecr:DescribeImages",
                      "ecr:BatchGetImage",
                      "ecr:GetLifecyclePolicy",
                      "ecr:GetLifecyclePolicyPreview",
                      "ecr:ListTagsForResource",
                      "ecr:DescribeImageScanFindings"
                  ],
                  "Resource": "*"
              }
          ]
      }
      ```
    impact: 8
    effort: 3
    platform: AWS
    category: Identity and Access Management
    resource: ECR
    nodes:
      - PLACEHOLDER
    refs:
      - text: AWS EKS Node Role
        url: https://docs.aws.amazon.com/eks/latest/userguide/create-node-role.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.1
          - eks-cis1.0.1-5.1.3
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-129
    title: Amazon EKS Worker Nodes Should Not Have Public IP Addresses
    description: EKS worker nodes can be configured to only have private IP addresses assigned, and this forces direct network access to exposed services running on the nodes such as SSH to originate from systems within the internal network instead of any system on the Internet. Even if a security group or network access control list was modified to grant ingress access to a public system, the 1-to-1 NAT would not be in place to route that traffic inbound to the system.  Note that egress access must then be granted via Cloud NAT or your own NAT gateway, and the cluster has to be configured with a private master IP range and IP aliasing must be enabled.
    remediation: |-
      When defining the VPC and subnet topology for EKS clusters, most use cases require the "public and private" subnet approach where the worker nodes are provisioned in private subnets and load balancers/ingress resources are deployed in the public subnets.  To modify a subnet to avoid mapping a public IP to EC2 instances as they launch, run the following command on all desired subnets.

      ```
      aws ec2 modify-subnet-attribute --region <aws_region> --map-public-ip-on-launch=false --subnet-id <subnet_id>
      ```
    validation: |-
      Run the following command in all regions where EKS workers are running to validate that a public IP will not be automatically mapped upon launch in those subnets.

      ```
      aws ec2 describe-subnets --region <aws_region> --output json | jq -r '.Subnets[] | "\(.VpcId) \(.SubnetId) \(.MapPublicIpOnLaunch)"'
      ```
    impact: 6
    effort: 9
    platform: AWS
    category: Networking and Content Delivery
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: EKS VPC Considerations
        url: https://docs.aws.amazon.com/eks/latest/userguide/network_reqs.html
      - text: EKS Private Clusters
        url: https://docs.aws.amazon.com/eks/latest/userguide/private-clusters.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.4
          - eks-cis1.0.1-5.4.3
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-130
    title: Amazon EKS HTTPS Load Balancers Should Use TLS Certificates
    description: To ease the administrative overhead of managing TLS certificates and their expiration dates, using AWS Certificate Manager (ACM) in conjunction with all Network and Application Load Balancers exposed via EKS Services is strongly recommended.
    remediation: |-
      When creating a service of type `LoadBalancer`, ensure that the annotations in that service description includes `service.beta.kubernetes.io/aws-load-balancer-ssl-cert: <arn_of_acm_cert>`.  When using an ingress controller, this annotation is typically on the single load balancer shared by all ingress resources.
    validation: |-
      Run the following command in each EKS cluster to list all exposed Kubernetes services.  Review the `service.beta.kubernetes.io/aws-load-balancer-ssl-cert` annotation for a valid ACM certificate ARN.

      ```
      kubectl get services -A --output=json | jq -r '.items[] | select(.spec.type=="LoadBalancer") | "\(.metadata.namespace) \(.metadata.name) \(.metadata.annotations)"'
      ```
    impact: 5
    effort: 4
    platform: AWS
    category: Networking and Content Delivery
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: Kubernetes Docs for AWS TLS LoadBalancers
        url: https://kubernetes.io/docs/concepts/services-networking/service/#ssl-support-on-aws
      - text: HTTPS Traffic on EKS with ACM
        url: https://aws.amazon.com/premiumsupport/knowledge-center/terminate-https-traffic-eks-acm/
      - text: EKS Network Load Balancing
        url: https://docs.aws.amazon.com/eks/latest/userguide/load-balancing.html
      - text: EKS Application Load Balancing
        url: https://docs.aws.amazon.com/eks/latest/userguide/alb-ingress.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level2
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.4
          - eks-cis1.0.1-5.4.5
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-131
    title: Amazon EKS Clusters Should Manage RBAC Users With AWS IAM Authenticator
    description: Amazon EKS clusters can be integrated with native AWS IAM to map IAM users and/or Roles to RBAC groups inside the cluster.  Combined with a kubeconfig that leverages the current IAM identity's bearer token, this can allow for near seamless use of kubectl while taking advantage of all the account protections available for IAM users such as session duration and multi-factor authentication.
    remediation: |-
      By default, EKS clusters have this integration enabled in the control plane.  Administrators should leverage kubectl to modify the `aws-auth` `ConfigMap` in the `kube-system` `namespace` to include the IAM Roles and/or Users that should be mapped to a given RBAC `group`.  From there, standard RBAC `ClusterRoleBinding` and `RoleBinding` resources can be created that give permissions to those `group` names.  Note that `groups` named `system:*` should not be assigned to users as they can add to confusion when reviewing the Kubernetes audit logs shipped to CloudWatch.
    validation: |-
      To verify access granted to IAM identities, run the following command in the cluster.

      ```
      kubectl get configmap -n kube-system aws-auth -o yaml
      ```
    impact: 6
    effort: 5
    platform: AWS
    category: Identity and Access Management
    resource: EKS
    nodes:
      - PLACEHOLDER
    refs:
      - text: Managing users or IAM Roles for EKS Clusters
        url: https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level2
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.5
          - eks-cis1.0.1-5.5.1
      - nist-csf:
          - nist-csf-todo
      - aws
  - id: darkbit-aws-132
    title: Fargate Should Be Used For Running Untrusted EKS Workloads
    description: |
      In most enterprise use cases, workloads are being deployed from known specifications and leveraging container images that are granted a certain degree of implicit or explicit trust.  The assumptions are that the packages, libraries, binaries, and scripts in the container image are not malicious or expected to be malicious.  In cases where input or code may be supplied by an untrusted third party, additional protections and isolation methods are prudent to help eliminate likely classes of container escape.  AWS Fargate integrates with EKS in that its custom scheduler can place specifically designated workloads on dedicated Fargate nodes with microvm level isolation and other supporting restrictions.
      There are some limitations with EKS workloads on Fargate in that container-based, malicious activity detection workloads like Falco, Aqua, and Prisma do not currently have introspection capabilities into these workloads.
    remediation: |-
      Determine if any workloads may be running code from untrusted sources such as Jupyter Notebooks or Apache Spark.  If those workloads meet the requirements for running in Fargate, create a Fargate pod execution role to be attached to the hidden Fargate node(s), and create a Fargate profile that combines the pod execution role, the subnets, the namespace, and the label selectors.
    validation: |-
      To validate if a `pod` is running on a Fargate node, look for node names beginning with `fargate-ip-` in the output of the following command.

      ```
      kubectl get pods -o wide
      ```
    impact: 5
    effort: 9
    platform: AWS
    category: Security, Identity, and Compliance
    resource: Fargate
    nodes:
      - PLACEHOLDER
    refs:
      - text: EKS Fargate Getting Started
        url: https://docs.aws.amazon.com/eks/latest/userguide/fargate-getting-started.html
    public: true
    tags:
      - cis:
          - eks-cis1.0.1
          - eks-cis1.0.1-level1
          - eks-cis1.0.1-5
          - eks-cis1.0.1-5.6
          - eks-cis1.0.1-5.6.1
      - nist-csf:
          - nist-csf-todo
      - aws
  #GCP
  - id: darkbit-gcp-6
    title:
      IAM.ServiceAccountUser and IAM.serviceAccountTokenCreator Should Not Be Granted
      at the Project Level
    description:
      When creating a GCP resources that attaches a GCP service account,
      the calling user or service account must have the permission to "use" that 
      service account. Otherwise, the ability to create a GCE instance with any 
      service account attached would be a direct path to privilege escalation if at 
      least one other service account in the project had higher permissions. This 
      permission can be granted on a specific service account or at the project level.  
      Granting the permission directly on the service account resource permits "use" 
      of just that service account.  Granting the permission at the project level 
      permits the "use" of any service account in the project.  Even if the project 
      does not currently have a highly privileged service account today, it might in 
      the future, and that would inadvertently increase the power of this binding.
    remediation:
      Remove the permission at the project level and instead assign `iam.serviceAccountUser`
      or `iam.serviceAccountTokenCreator` on the small number of service accounts in the
      project that are necessary.
    validation: |-
      Run the following commands for each project and ensure no results are displayed for 
      `iam.serviceAccountUser` or `iam.serviceAccountTokenCreator`.
      
      ```
      gcloud beta projects get-iam-policy [PROJECT_ID] 
        --flatten='bindings[]' \
        --filter=bindings.role:iam.serviceAccountUser \
        --format="csv[no-heading](bindings.members[])"
      ```

      and

      ```
      gcloud beta projects get-iam-policy [PROJECT_ID] 
        --flatten='bindings[]' \
        --filter=bindings.role:iam.serviceAccountTokenCreator \
        --format="csv[no-heading](bindings.members[])"
      ```
    impact: 5
    effort: 2
    platform: GCP
    category: Identity and Access Management
    resource: IAM
    nodes:
      - GCP_IDENTITY
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Granting IAM Roles to Service Accounts
        url: https://cloud.google.com/iam/docs/granting-roles-to-service-accounts
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.6
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-4
          - nist-csf-pr.ac-6
      - google cloud
  - id: darkbit-gcp-14
    title: VPC Flow Logging Should Be Enabled on All VPC Subnets
    description:
      VPC flow logs record metadata about all traffic flowing in and out
      of a VPC. These logs are vital for auditing and review after security incidents
      to be able to create an accurate timeline of network events to go with application
      and Cloud API Audit logs.
    remediation:
      Enable VPC Flow logging on all VPCs with a 100% sample rate for production
      environments or those that hold sensitive data.  When directing to a GCS bucket,
      enable bucket versioning and optionally configure an object lifecycle policy to
      retain the data for the desired period only.
    validation: |-
      In each project, run the following command and and ensure no entries are present.
      
      ```
      gcloud compute networks subnets list \
        --format=json | jq -r '.[] | select(.logConfig.enable | not) | "\(.name) \(.region)"'
      ``` 
    impact: 7
    effort: 4
    platform: GCP
    category: Networking and Content Delivery
    resource: VPC
    nodes:
      - GCP_COMPUTE_SUBNETWORK
    refs:
      - text: Using VPC Flow Logs
        url: https://cloud.google.com/vpc/docs/using-flow-logs
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.8
      - nist-csf:
          - nist-csf-de
          - nist-csf-de.ae
          - nist-csf-de.ae-1
          - nist-csf-de.cm
          - nist-csf-de.cm-1
          - nist-csf-de.cm-7
      - google cloud
  - id: darkbit-gcp-15
    title: Firewall Rules Should Not Allow Access to SSH TCP/22 From All Hosts
    description:
      Firewall rules that permit inbound/ingress access from any IP address
      (0.0.0.0/0) to administrative ports via SSH (tcp/22) should be reviewed for necessity
      to prevent unintended exposure of services and systems protected by that security
      group.  The primary exclusion to this is a dedicated, hardened bastion host.
    remediation:
      For each firewall rule, assess whether the attached systems requires
      SSH access from any IP address.  If not, consider reducing the source IP
      ranges to a specific set of subnets or to the bastion host(s) in the environment.
    validation: |-
      In each project, run the following command and ensure no entries that
      permit `IPProtocol` of `tcp` and `ports` of `22`.
      
      ```
      gcloud compute firewall-rules list \
        --format=json | jq -r '.[] | 
          select(.sourceRanges) | 
          select(.allowed) | 
          select(.sourceRanges[] | 
          contains("0.0.0.0/0")) | "\(.name) \(.allowed[])"'
      ``` 
    impact: 8
    effort: 4
    platform: GCP
    category: Network Access Control
    resource: Firewall
    nodes:
      - PLACEHOLDER
    refs:
      - text: Configuring Firewall Rules
        url: https://cloud.google.com/vpn/docs/how-to/configuring-firewall-rules
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.6
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-4
          - nist-csf-pr.ac-5
          - nist-csf-pr.ds
          - nist-csf-pr.ds-5
      - google cloud
  - id: darkbit-gcp-17
    title: GCE Instances Should Not Have Public IP Addresses
    description:
      GCE Instances should not have public IP addresses assigned directly
      to them, and should be administered using their private IP and services running
      on them exposed only via load balancers.  This reduces the scope of attacks against
      vulnerable services and affords additional protection against denial-of-service.
    remediation:
      Configure all GCE Instances to not have public IP addresses assigned.  In
      GKE, enable private nodes.  For services running on these systems that should
      be exposed externally, configure a load balancer.  For remote administration,
      consider using the Identity-Aware Proxy service to create an SSH or RDP-over-TLS
      tunnel directly to the private IP of the instance.
    validation: |-
      In each project, run the following command and ensure that no entries with public IPs exist.
      
      ```
      gcloud compute instances list \
        --format=json | 
          jq -r '.[] | . as $instance | .networkInterfaces[] | 
          select(.accessConfigs) | .accessConfigs[] | 
          select(.type=="ONE_TO_ONE_NAT") | "\($instance.name): \(.natIP)"'
      ```
    impact: 5
    effort: 8
    platform: GCP
    category: Networking and Content Delivery
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
    refs:
      - text: IAP Proxy for GCE
        url: https://cloud.google.com/iap/docs/enabling-compute-howto
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.9
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-4
          - nist-csf-pr.ac-5
      - google cloud
  - id: darkbit-gcp-18
    title: Default VPC Networks Should Not Be Present
    description:
      Each project contains a default VPC/Network with default subnets in
      every region for ease of use, but the IP ranges used are likely not desired and
      may overlap between different projects.  This may cause issues in the future if
      VPC Peering is desired as the subnets will conflict and prevent routing.
    remediation:
      Delete the default VPC/Network and subnets in each project and instead
      create a new VPC and subnets according to the networking and IP address management
      needs of the organization to avoid overlap.  To ensure all projects created do
      not have the default VPC/Network and subnets created automatically, configure
      the `constraints/compute.skipDefaultNetworkCreation` organization policy at the
      organization node.
    validation: |-
      In each project, run the following command and ensure no entries are listed.

      ```
      gcloud compute networks list \
        --format=json | jq -r 'select(.[]) | .[] | select(.name=="default") | .name'
      ```
    impact: 3
    effort: 8
    platform: GCP
    category: Networking and Content Delivery
    resource: VPC
    nodes:
      - PLACEHOLDER
    refs:
      - text: Default VPC Network
        url: https://cloud.google.com/vpc/docs/vpc#default-network
      - text: Organization Policies
        url: https://cloud.google.com/resource-manager/docs/organization-policy/org-policy-constraints
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.1
      - nist-csf:
          - nist-csf-id
          - nist-csf-id.am
          - nist-csf-id.am-3
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-5
          - nist-csf-pr.ds
          - nist-csf-pr.ds-7
          - nist-csf-pr.pt
          - nist-csf-pr.pt-4
      - google cloud
  - id: darkbit-gcp-19
    title: User-Managed GCP Service Account Keys Should Not Be Older Than 90 Days
    description:
      GCP Service Account Keys that are created by an administrator are known
      as "user-managed" keys.  Unless rotated by the user, they are static for their
      lifetime.  They should be rotated on a frequent basis to limit their active lifetime
      in the event the key material is compromised.
    remediation:
      GCP Service Accounts have native, dynamic key integrations to services
      like GCE, GKE, AppEngine, Cloud Functions, and more such that exporting a static
      key is only necessary for integrations from non-GCP services like Splunk, Sumologic,
      Datadog, etc that need access to GCP resources from outside the organization.  This
      removes the need for exporting keys for all but a small number of clearly defined
      use cases for those types of integrations.  Refactor GCP applications using static,
      exported service account keys to use the native integration.  For keys used by
      external services, rotate them every 90 days.
    validation: |-
      For each project, run the following command and and review the `validAfterTime` 
      and `validBeforeTime` values for each key to ensure they are not valid for longer than 90 days.
      
      ```
      for sa in $(gcloud iam service-accounts list \
        --format="value(email)" \
        --project=[PROJECT_ID]); do
          gcloud iam service-accounts keys list \
            --iam-account $sa \
            --project=[PROJECT_ID] \
            --format=json | jq -r '.[] | 
              select(.keyType=="USER_MANAGED") | "\(.name): \(.validAfterTime) \(.validBeforeTime)"'
          done
      ```
    impact: 2
    effort: 6
    platform: GCP
    category: Management and Governance
    resource: ServiceAccount
    nodes:
      - GCP_IAM_SERVICEACCOUNTKEY
    refs:
      - text: Managing Service Account Keys
        url: https://cloud.google.com/iam/docs/creating-managing-service-account-keys
      - text: Security Health Analytics
        url: https://cloud.google.com/security-command-center/docs/how-to-manage-security-health-analytics
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.7
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-6
          - nist-csf-pr.ac-7
      - google cloud
  - id: darkbit-gcp-20
    title: GCP Projects Should Enable Data Access Logging
    description:
      GCP Audit logs automatically capture administrative activity logs for
      any creation or modification of resources via the GCP API.  e.g. "create GCE instance"
      or "delete GCS bucket".  By default, read requests to get/list resources and read/write
      requests that access user-provided data are not logged.  e.g. "get or delete an
      object in a GCS bucket".  These logs can and should be enabled on non-development
      projects to enable full capture of API activity necessary to support incident
      analysis and for meeting certain regulatory compliance requirements.
    remediation: |
      Enable `Admin READ` and `Data READ and WRITE` access logging for `allServices`
      at the organization level so that it applies to all downstream folders and projects.  Configure
      Stackdriver to export audit logs to a storage medium such as a GCS bucket and
      configure a lifecycle and retention policy to automatically move data to cheaper
      storage and to prune old data.  Caveat: Audit logs are often verbose and generate
      large volumes of log entries, and storing these in GCS buckets incurs an additional
      cost.  Use the exemption configuration to remove GCP internal service audit logs
      if necessary.
    validation: |-
      For each project, run the following command:
      
      ```
      gcloud projects get-iam-policy [PROJECT_ID] --format=json | jq -r 'select(.auditConfigs)'
      ```
      
      and ensure that it is not empty and it returns the following:
      
      ```
      [
        {
          "service": "allServices",
          "auditLogConfigs": [
            { "logType": "ADMIN_READ" },
            { "logType": "DATA_READ"  },
            { "logType": "DATA_WRITE" },
          ]
        }
      ]
      ```
    impact: 8
    effort: 4
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Project
    nodes:
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Project Audit Logs
        url: https://cloud.google.com/logging/docs/audit/understanding-audit-logs
      - text: Configuring Data Access Logs
        url: https://cloud.google.com/logging/docs/audit/configure-data-access
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.1
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-2
          - gke-cis1.1-2.2
          - gke-cis1.1-2.2.1
          - gke-cis1.1-2.2.2
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.pt
          - nist-csf-pr.pt-1
      - google cloud
  - id: darkbit-gcp-22
    title: Cloud SQL Instances Should Have a Private IP Address
    description:
      By default, Cloud SQL instances are assigned a public IPv4 address,
      and the access list allows any source IP address to attempt to connect and authenticate
      to the database.  This facilitates development and ease of connectivity for troubleshooting,
      but it also means an attacker can access the potentially sensitive data in the database
      without restriction if they obtain valid credentials.
    remediation:
      To reduce the likelihood of an attacker gaining access the database service
      directly or using stolen credentials from any source IP, the instance should
      be configured with a private IP address only. This should be configured during
      creation of the instance and a VPC peering relationship should be configured between
      the Cloud SQL VPC and the VPCs where applications accessing the database reside.
    validation: |-
      In each project, run the following command and ensure no entries are listed.
      
      ```
      gcloud sql instances list \
        --format=json | jq -r '.[] | 
          select(.settings.ipConfiguration.ipv4Enabled==true) | 
          "\(.name) Public IP: \(.settings.ipConfiguration.ipv4Enabled)"'
      ```
    impact: 5
    effort: 9
    platform: GCP
    category: Networking and Content Delivery
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Cloud SQL MySQL Private IP
        url: https://cloud.google.com/sql/docs/mysql/configure-private-services-access
      - text: Cloud SQL PostgreSQL Private IP
        url: https://cloud.google.com/sql/docs/postgres/configure-private-services-access
      - text: Cloud SQL SQL Server Private IP
        url: https://cloud.google.com/sql/docs/sqlserver/configure-private-services-access
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.6
      - nist-csf:
          - nist-csf-id
          - nist-csf-id.am
          - nist-csf-id.am-3
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-5
      - google cloud
  - id: darkbit-gcp-23
    title: Cloud SQL Instances Should Require SSL/TLS
    description:
      By default, Cloud SQL instances will accept both SSL/TLS and plaintext
      database connections.  As Cloud SQL instances are provisioned inside a separate
      VPC within GCP that the user does not control, this means that database traffic
      will traverse a VPC peering connection without in-transit encryption.  Requiring
      SSL/TLS connections ensures that all connections are encrypted over the wire.
    remediation:
      Configure the instance during creation or while running to `require-ssl`.  For
      example, `gcloud sql instances patch [INSTANCE_NAME] --require-ssl`.  To allow
      applications to connect securely without changing application code, the cloudsql-proxy
      can be used.  It can handle setting up a localhost SSL proxy/tunnel and applications
      can be configured to connect to the database via that tunnel.
    validation: |
      In each project, run the following command and ensure no entries are listed.
      
      ```
      gcloud sql instances list \
        --format=json | jq -r '.[] | 
          select(.settings.ipConfiguration.requireSsl==null) | 
          "\(.name) SSL: \(.settings.ipConfiguration.requireSsl)"'
      ```
    impact: 5
    effort: 7
    platform: GCP
    category: Networking and Content Delivery
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Cloud SQL MySQL Require SSL/TLS
        url: https://cloud.google.com/sql/docs/mysql/configure-ssl-instance
      - text: Cloud SQL PostgreSQL Require SSL/TLS
        url: https://cloud.google.com/sql/docs/postgres/configure-ssl-instance
      - text: Cloud SQL Sqlserver Require SSL/TLS
        url: https://cloud.google.com/sql/docs/sqlserver/configure-ssl-instance
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.4
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-7
          - nist-csf-pr.ds
          - nist-csf-pr.ds-2
      - google cloud
  - id: darkbit-gcp-26
    title: Firewall Rules Should Not Allow Access to RDP TCP/3389 From All Hosts
    description:
      Firewall rules that permit inbound/ingress access from any IP address
      (0.0.0.0/0) to database ports via RDP (tcp/3389) should be reviewed for necessity
      to prevent unintended exposure of services and systems protected by that security
      group.  The primary exclusion to this is a dedicated, hardened bastion host.
    remediation:
      For each firewall rule, assess whether the attached systems requires
      Remote Desktop access from any IP address.  If it doesn't, consider reducing the
      source IP ranges to a specific set of subnets or to the bastion host(s) in the
      environment.
    validation: |-
      In each project, run and ensure no entries that permit `IPProtocol` of `tcp` 
      and `ports` of `3389`.
      
      ```
      gcloud compute firewall-rules list \
        --format=json | jq -r '.[] | 
          select(.sourceRanges) | 
          select(.allowed) | 
          select(.sourceRanges[] | contains("0.0.0.0/0")) | "\(.name) \(.allowed[])"'.
      ```
    impact: 7
    effort: 4
    platform: GCP
    category: Network Access Control
    resource: Firewall
    nodes:
      - GCP_COMPUTE_FIREWALL
      - GCP_COMPUTE_FIREWALLIPRANGE
    refs:
      - text: Configuring Firewall Rules
        url: https://cloud.google.com/vpn/docs/how-to/configuring-firewall-rules
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.7
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-4
          - nist-csf-pr.ac-5
          - nist-csf-pr.ds
          - nist-csf-pr.ds-5
      - google cloud
  - id: darkbit-gcp-33
    title: Load Balancers Should Use a MODERN or RESTRICTED SSL Policy
    description:
      By default, Load Balancers use an SSL/TLS policy equivalent to the
      `COMPATIBLE` profile which supports a wide range of ciphers and TLS versions of
      varying security.  In nearly all use cases, the more stringent profiles should
      be used to ensure client connections only negotiate with strong ciphers and recent
      TLS versions.  For compliance-focused environments, the `RESTRICTED` policy should
      be used.
    remediation:
      Configure all Load Balancers to leverage the `MODERN` or `RESTRICTED SSL`
      policy or a `CUSTOM SSL` policy with equivalent enforcement of secure ciphers and
      TLS versions.
    validation: |
      Run the following commands and ensure they do not return any entries.
      
      ```
      gcloud compute target-https-proxies list \
        --format=json | jq -r '.[] | 
          select(.sslPolicy | not) | "\(.name) \(.selfLink)"'
      ```
      
      and
      
      ```
      gcloud compute target-ssl-proxies list \
        --format=json | jq -r '.[] | 
          select(.sslPolicy | not) | "\(.name) \(.selfLink)"'
      ```
    impact: 3
    effort: 3
    platform: GCP
    category: Networking and Content Delivery
    resource: Load Balancer
    nodes:
      - GCP_COMPUTE_TARGETHTTPSPROXY
      - GCP_COMPUTE_TARGETSSLPROXY
    refs:
      - text: GCP SSL Policies
        url: https://cloud.google.com/load-balancing/docs/ssl-policies-concepts
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-not-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.9
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-7
          - nist-csf-pr.ds
          - nist-csf-pr.ds-2
          - nist-csf-pr.pt
          - nist-csf-pr.pt-4
      - google cloud
  - id: darkbit-gcp-38
    title: GKE Network Policy Support Should Be Installed in All Clusters
    description:
      By default in Kubernetes, all Pods can communicate with each other
      by IP and egress to any subnet (including the Internet) unless routing or firewalls
      are added to prevent that traffic.  This presents ample opportunity for lateral
      movement from the perspective of a compromised workload. One of the best ways
      to reduce the scope of that movement is to deploy `NetworkPolicy` resources that
      define firewall rules for pod-to-pod traffic.  In GKE, the Network Policy add-on
      must be enabled to allow for the cluster to enforce firewall policies on Pods.
    remediation:
      Configure the cluster to enabled Network Policy support to allow for
      in-cluster support for NetworkPolicy (firewall rules) resources.
    validation: |-
      For each cluster, run the following command and ensure the cluster name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.networkPolicy.enabled==true 
          and .networkPolicy.provider=="CALICO") | "\(.name)"'
      ```
    impact: 9
    effort: 3
    platform: GCP
    category: Network Access Control
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Kubernetes Network Policy
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_with_network_policy
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-4
          - gke-cis1.1-4.3
          - gke-cis1.1-4.3.1
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-4
          - nist-csf-pr.ac-5
          - nist-csf-pr.ds
          - nist-csf-pr.ds-5
      - google cloud
  - id: darkbit-gcp-39
    title: GKE Control Plane Access Should Be Restricted to a Known Set of IP Ranges
    description:
      The Kubernetes API Server systems (Control Plane) expose the Kubernetes
      API Server TLS port publicly without an IP restrictions or limitations.  This
      provides convenient remote administrative access, but it affords only a single
      layer of defense in front of the Kubernetes cluster and all applications and data
      inside.  A pre-authentication or denial-of-service vulnerability could compromise
      or disrupt the cluster completely.  Several pre-authentication denial-of-service
      vulnerabilities have been discovered and fixed in recent Kubernetes releases.
    remediation:
      Configure the master authorized networks on the GKE cluster to be restricted
      to a known set of IP ranges for API Server access.  In environments with bastion
      hosts or VPNs, their internal subnet range or security groups are commonly used.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.masterAuthorizedNetworksConfig.enabled==true) | 
          select(.masterAuthorizedNetworksConfig.cidrBlocks[].cidrBlock!="0.0.0.0/0")| "\(.name)"'
      ```
    impact: 6
    effort: 5
    platform: GCP
    category: Network Access Control
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Billion Laughs Attack
        url: https://www.stackrox.com/post/2019/09/protecting-kubernetes-api-against-cve-2019-11253-billion-laughs-attack/
      - text: Billion Laughs PoC
        url: https://gist.github.com/bgeesaman/0e0349e94cd22c48bf14d8a9b7d6b8f2
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.6
          - gke-cis1.1-5.6.3
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-5
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
          - nist-csf-pr.pt-4
      - google cloud
  - id: darkbit-gcp-40
    title:
      GKE Workload Identity Should Be Enabled and Enforcing Metadata Protection
      on All Nodepools
    description:
      GKE clusters without Workload Identity allow pods to reach the Instance Metadata
      API corresponding to the underlying node.  By extension, those pods can access
      the APIs and data used to bootstrap the Kubernetes worker node.  The credentials
      used to bootstrap a Kubernetes worker node are very commonly sufficient to be
      used to privilege escalate to "cluster-admin".  Also by extension, this means
      that every container image ever run in this cluster in the non-"prod" namespace
      has had the ability to reach and export these credentials.  Therefore, it is 
      important for a cluster's security posture to prevent pods from being able to
      reach the Instance Metadata API to fetch those bootstrapping credentials.
    remediation:
      Configure Workload Identity on the cluster and every node pool in the
      cluster with the `GKE_METADATA` setting enabled.  Alternatively, deploy an egress
      `NetworkPolicy` blocking egress to `169.254.169.254` for all non-kube-system namespaces.
    validation: |-
      For each cluster, run and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.workloadIdentityConfig.workloadPool | 
        test("svc.id.goog")) | "\(.name)"'
      ``` 
    impact: 9
    effort: 5
    platform: GCP
    category: Identity and Access Management
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: GKE Workload Identity
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/workload-identity
      - text: Hardening GKE
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#workload_identity
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-level2
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.2
          - gke-cis1.1-5.2.2
          - gke-cis1.1-5.4
          - gke-cis1.1-5.4.2
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-4
          - nist-csf-pr.ac-5
          - nist-csf-pr.ac-6
          - nist-csf-pr.ac-7
          - nist-csf-pr.ds
          - nist-csf-pr.ds-2
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
      - google cloud
  - id: darkbit-gcp-41
    title: GKE Node Pools Should Use Dedicated GCP Service Accounts
    description:
      By default, GKE associates the `default` compute service account to
      GKE worker nodes, and it is automatically granted the `Project Editor` IAM Role.  To
      avoid using an inherently shared service account with over-provisioned permissions,
      create a dedicated service account for each GKE cluster in each project and grant
      it only the minimal IAM permissions needed.
    remediation: |-
      Create a dedicated GCP service account for each cluster in each project. Create a 
      custom IAM Role with the `monitoring.viewer`, `monitoring.metricWriter`, and 
      `logging.logWriter` permissions, and associate that with the dedicated GCP service 
      account.  Also, ensure the OAuth Scopes attached to the nodes are:

      ```
      https://www.googleapis.com/auth/devstorage.read_only
      https://www.googleapis.com/auth/logging.write
      https://www.googleapis.com/auth/monitoring
      https://www.googleapis.com/auth/servicecontrol
      https://www.googleapis.com/auth/service.management.readonly
      https://www.googleapis.com/auth/trace.append
      ```

      Consideration: Changing the service account or the OAuth Scopes will result in a rolling redeployment of the Node Pool.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.nodePools[].config.serviceAccount | 
          test("-compute@developer.gserviceaccount.com") | not) | "\(.name)"'
      ```
    impact: 9
    effort: 6
    platform: GCP
    category: Identity and Access Management
    resource: GKE
    nodes:
      - GCP_CONTAINER_NODEPOOL
      - GCP_IDENTITY
    refs:
      - text: GKE NodePool OAuth Scopes
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#use_least_privilege_sa
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.2
          - gke-cis1.1-5.2.1
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-6
          - nist-csf-pr.ip
          - nist-csf-pr.ip-1
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
      - google cloud
  - id: darkbit-gcp-42
    title: GKE Node Pools Should Use Shielded GKE Nodes
    description:
      Starting in GKE 1.13.6 and later, GKE Worker nodes can be provisioned
      with a Virtual Trusted Platform Module (vTPM) that can be used to cryptographically
      verify the integrity of the boot process and to securely distribute the bootstrapping
      credentials used by the Kubelet to attach the node to the cluster on first boot.  Without
      this feature, the Kubelet's bootstrapping credentials are available via the GCE
      Metadata API, and that can be accessed by any Pod unless additional protections
      are put in place.  These credentials can be leveraged to escalate to cluster-admin
      in most situations.
    remediation: |
      Modify the cluster node pool configuration to enable shielded nodes
      (`--enable-shielded-nodes`) and secure boot (`--shielded-secure-boot`).  This will
      remove the sensitive bootstrapping credentials from the GCE Metadata API and enable
      additional verification checks to ensure the worker nodes have not been compromised
      at a fundamental level.  Considerations: The nodes must be running the `COS` or
      `COS_CONTAINERD` operating system, and enabling this change will require a node
      pool rolling redeployment performed at the next maintenance window.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME]> \
        --format=json | jq -r 'select(.nodePools[].config.shieldedInstanceConfig.enableIntegrityMonitoring==true 
          and .nodePools[].config.shieldedInstanceConfig.enableSecureBoot==true) | "\(.name)"'
      ```
      
    impact: 9
    effort: 6
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: GKE Hardening
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#shielded_nodes
      - text: GKE Shielded Nodes
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/shielded-gke-nodes
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.8
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-level2
          - gke-cis1.1-scored
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.3
          - gke-cis1.1-5.3.1
          - gke-cis1.1-5.4
          - gke-cis1.1-5.4.2
          - gke-cis1.1-5.5
          - gke-cis1.1-5.5.5
          - gke-cis1.1-5.5.6
          - gke-cis1.1-5.5.7
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ds
          - nist-csf-pr.ds-6
          - nist-csf-pr.ds-8
          - nist-csf-pr.pt
          - nist-csf-pr.pt-5
      - google cloud
  - id: darkbit-gcp-44
    title: GKE Clusters Should Use a Private Cluster Endpoint/IP
    description:
      By default, GKE creates clusters with a public IP address on the control
      plane without any network restriction on source IP ranges that can access it.  Even
      though the access controls protecting the API server require authentication and
      authorization, the API server is open to denial-of-service attacks, being probed
      by bots/scanners inflating activity logs, and direct exploitation should a
      Kubernetes API Server vulnerability be discovered.
    remediation: |-
      Consider rebuilding GKE clusters with the "private master endpoint" configuration to ensure 
      the API server is not assigned a routable public IP address.  Additional considerations:

      * Converting a public to a private GKE cluster requires rebuilding it.
      * Use a dedicated `/28` subnet for the control plane IP space that does not overlap anywhere and is not part of `172.17.0.0/16`.
      * Private control planes leverage VPC peering and count toward VPC peering quota.
      * Modifications to the VPC peering or firewall rules from the control plane to the worker nodes can break the cluster.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.privateClusterConfig.enablePrivateEndpoint==true) | "\(.name)"'
      ```
      
    impact: 6
    effort: 9
    platform: GCP
    category: Networking and Content Delivery
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: GKE Private Clusters
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level2
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.6
          - gke-cis1.1-5.6.4
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-3
          - nist-csf-pr.ac-5
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
          - nist-csf-pr.pt-4
      - google cloud
  - id: darkbit-gcp-47
    title: GKE Intranode Visibility Should Be Enabled on All Clusters
    description:
      When enabling VPC Flow Logs on a VPC where GKE clusters are running,
      traffic that does not exit the node is not captured.  That is, traffic between
      two pods on the same node do not exit the host's network and therefore are omitted
      from VPC Flow Logging.  Enabling Intranode Visibility allows this traffic to be
      recorded in the VPC Flow Logs for analysis and diagnosis.
    remediation: |
      Configure Intranode Visibility on GKE Clusters running in VPCs with
      Flow logging enabled.  Considerations: VPC Flow logs may increase in size and
      contribute to additional cost.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.networkConfig.enableIntraNodeVisibility==true) | "\(.name)"'
      ```
    impact: 2
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: GKE Intranode Visibility
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/intranode-visibility
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level2
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.6
          - gke-cis1.1-5.6.1
      - nist-csf:
          - nist-csf-de
          - nist-csf-de.ae
          - nist-csf-de.ae-1
          - nist-csf-de.ae-3
          - nist-csf-de.cm
          - nist-csf-de.cm-1
          - nist-csf-de.cm-7
      - google cloud
  - id: darkbit-gcp-48
    title: GKE Logs Should Be Sent to Stackdriver/Google Cloud Monitoring
    description:
      By default, GKE enables a Stackdriver log export managed add-on capability
      that ships all Host OS, Kubernetes components, and container logs to the Stackdriver
      endpoint in the current project.  This provides a detailed record of nearly all
      activities in the cluster and nodes to support troubleshooting and auditing functions.  Even
      if a third party logging solution is implemented to capture and ship logs, it's
      recommended that this add-on is enabled to ensure all Host OS and Kubernetes component
      logs are captured off-cluster.
    remediation:
      Configure the Kubernetes Engine Monitoring for "System and workload
      logging and monitoring" via the console or by way of the `--enable-stackdriver-kubernetes`
      option to gcloud on all GKE clusters. Existing clusters can have this feature
      enabled in-place with no downtime.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.loggingService=="logging.googleapis.com/kubernetes") | "\(.name)"'
      ```
    impact: 9
    effort: 3
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Kubernetes Engine Monitoring
        url: https://cloud.google.com/monitoring/kubernetes-engine/installing
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.7
          - gke-cis1.1-5.7.1
      - nist-csf:
          - nist-csf-de
          - nist-csf-de.ae
          - nist-csf-de.ae-1
          - nist-csf-de.ae-2
      - google cloud
  - id: darkbit-gcp-50
    title: GKE Node Pools Should Use the COS or COS_CONTAINERD Operating System
    description:
      GKE Nodes can leverage either Container-Optimized OS or Ubuntu-based
      operating system images.  Unless there is a very specific use-case that a Container-Optimized
      OS image cannot support such as installed certain drivers and/or kernel modules,
      Ubuntu nodes are not recommended.  Container-Optimized OS is a fully hardened
      operating system designed specifically to run containerized workloads with a high
      degree of security, and it receives automatic updates from Google.  The track
      record for security issues that affect Ubuntu nodes in GKE that did not affect
      COS nodes is also important to consider.
    remediation: |
      Configure your GKE Node Pools to leverage either the COS or COS_CONTAINERD
      image type.  The COS image leverages Docker, and the COS_CONTAINERD image implements
      only `containerd` and does not use the commonly known Docker socket at `/var/run/docker.sock`
      which allows applications that can access that socket to effectively be "root"
      on the host.  If your workloads do not require the ability to mount the docker
      socket for activities such as image building in-cluster or certain security features,
      COS_CONTAINERD offers an even smaller attack surface than COS.  Considerations:
      changing the image type recreates the nodes in the node pool.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.nodePools[].config.imageType | test("^COS")) | "\(.name)"'
      ```
    impact: 5
    effort: 8
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_NODEPOOL
    refs:
      - text: GKE Node Images
        url: https://cloud.google.com/kubernetes-engine/docs/concepts/node-images
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-level2
          - gke-cis1.1-scored
          - gke-cis1.1-3
          - gke-cis1.1-3.1
          - gke-cis1.1-3.1.1
          - gke-cis1.1-3.1.2
          - gke-cis1.1-3.1.3
          - gke-cis1.1-3.1.4
          - gke-cis1.1-3.2.1
          - gke-cis1.1-3.2.2
          - gke-cis1.1-3.2.3
          - gke-cis1.1-3.2.5
          - gke-cis1.1-3.2.6
          - gke-cis1.1-3.2.7
          - gke-cis1.1-3.2.8
          - gke-cis1.1-3.2.10
          - gke-cis1.1-3.2.11
          - gke-cis1.1-3.2.12
          - gke-cis1.1-5
          - gke-cis1.1-5.5
          - gke-cis1.1-5.5.1
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ip
          - nist-csf-pr.ip-1
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
      - google cloud
  - id: darkbit-gcp-52
    title: GKE Node Pools Should Enable AutoRepair
    description:
      GKE Nodes have several health checks that continuously run to validate
      that the worker node is running and capable of handling workloads.  If an issue
      occurs that the system cannot auto-resolve, the automatic repair feature will
      handle evicting workloads from the node and reprovisioning the underlying GCE
      instance for you.  This is highly recommended for reducing administrator load
      and maintaining healthy clusters.
    remediation:
      Configure the node pools to enable AutoRepair during node pool creation
      or to existing clusters with the `--enable-autorepair` feature set.
    validation: |
      For each clustr, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.nodePools[].management.autoRepair==true) | "\(.name)"'
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Management and Governance
    resource: GKE
    nodes:
      - GCP_CONTAINER_NODEPOOL
    refs:
      - text: GKE Node Auto Repair
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-repair
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5.5
          - gke-cis1.1-5.5.2
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.pt
          - nist-csf-pr.pt-5
      - google cloud
  - id: darkbit-gcp-53
    title: GKE Node Pools Should Enable AutoUpgrade
    description:
      The GKE service will automatically maintain the version and patch release
      of the control plane nodes, but the Node Pools are left to the user to keep upgraded
      by default.  This can result in situations where security issues are patched on
      the control plane but not on the nodes, and performing node upgrades is an activity
      that can consume a large amount of administrative time if performed manually across
      many clusters and node pools.  If your workloads are properly configured to withstand
      a single node failure, these maintenance activities can be performed during upgrade
      windows without manual intervention or downtime.
    remediation:
      Configure the node pools to enable AutoUpgrade during node pool creation
      or to existing clusters with the `--enable-autoupgrade` feature set.  It's strongly
      recommended to validate that all workloads can handle the upgrade process smoothly
      in a development cluster before enabling this setting in production.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.nodePools[].management.autoUpgrade==true) | "\(.name)"'
      ```
    impact: 2
    effort: 7
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_NODEPOOL
    refs:
      - text: GKE Node Auto Upgrade
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/node-auto-upgrades
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5.5
          - gke-cis1.1-5.5.3
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ma
          - nist-csf-pr.ma-1
          - nist-csf-pr.pt
          - nist-csf-pr.pt-5
      - google cloud
  - id: darkbit-gcp-55
    title: GKE Subnet Alias Ranges Should Be Configured
    description:
      In order to support private GKE Clusters, Alias IP ranges on the VPC
      Subnets are required.  In addition to private clusters, Alias IP ranges simplify
      the route tables in the VPC, reduce the number of "hops" traffic takes from load
      balancers to Pods, and allows the GCE network interfaces to perform anti-spoofing
      checks as IP forwarding is not required on the instance.  Finally, the separation
      of VM IPs from Container IPs allows for native firewall rules to be configured
      for pod traffic separately from VM traffic.
    remediation:
      During cluster creation, specify a VPC Subnet that supports Alias IP
      ranges and secondary ranges for Pods and Services.  This can only be done at cluster
      creation time.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.ipAllocationPolicy.useIpAliases==true) | "\(.name)"'
      ```
    impact: 5
    effort: 9
    platform: GCP
    category: Networking and Content Delivery
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: VPC Alias IPs
        url: https://cloud.google.com/vpc/docs/alias-ip
      - text: VPC-Native GKE Clusters
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/alias-ips
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.6
          - gke-cis1.1-5.6.2
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-5
          - nist-csf-pr.pt
          - nist-csf-pr.pt-3
          - nist-csf-pr.pt-4
      - google cloud
  - id: darkbit-gcp-56
    title: GKE Legacy ABAC Authorization Should Not Be Used
    description:
      Role-Based Access Control has been the default authorization mechanism
      in Kubernetes since version 1.6.  GKE still provides a legacy capability that
      supports ABAC, but it should not be used.  It essentially provides "cluster admin"
      access for any authenticated credential to the cluster and has no method for changing
      this policy.  This is an extremely permissive setting that allows for full cluster
      compromise should an attacker gain access to a single pod with a mounted Kubernetes
      service account or a valid cluster credential.
    remediation:
      Configure clusters without the `--enable-legacy-authorization` flag
      set.  If migrating an existing cluster from ABAC to RBAC by modifying this setting
      in-place, ensure that all the required RBAC RoleBindings and ClusterRoleBindings
      are present first.  This should be performed in a development environment before
      applying this process to production clusters.
    validation: |-
      For each cluster, run the following command and ensure that the cluster's name is listed.
      
      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format=json | jq -r 'select(.legacyAbac.enabled | not) | "\(.name)"'
      ``` 
    impact: 9
    effort: 8
    platform: GCP
    category: Identity and Access Management
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: GKE Cluster Hardening
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.8
          - gke-cis1.1-5.8.4
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-4
          - nist-csf-pr.ip
          - nist-csf-pr.ip-1
      - google cloud
  - id: darkbit-gcp-59
    title: Cloud SQL Instance Automatic Backups With Point-in-Time Recovery Should Be Configured
    description:
      By default, Cloud SQL instances do not have automatic backups with point-in-time
      recovery enabled.  It's important that this setting is enabled to ensure
      that all databases can be restored to a known-good state should a security incident
      occur.  For example, a SQL injection attack that results in the deletion or modification
      of tables in the database.
    remediation:
      Ensure all Cloud SQL instances are configured with automatic backups
      during a desired window and point-in-time recovery is enabled.  Existing instances
      can be modified to have these settings take effect, but it requires the instance
      to be restarted.
    validation: |-
      In each project, run the following command and ensure no entries are listed.
      
      ```
      gcloud sql instances list --format=json | jq -r '.[] | \
        select(.settings.backupConfiguration.enabled==false) | 
        "\(.name) Backups on: \(.settings.backupConfiguration.enabled) and PITR on: \(.settings.backupConfiguration.binaryLogEnabled)"'
      ```
    impact: 9
    effort: 3
    platform: GCP
    category: Management and Governance
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Cloud SQL MySQL Backups
        url: https://cloud.google.com/sql/docs/mysql/backup-recovery/backups
      - text: Cloud SQL PostgreSQL Backups
        url: https://cloud.google.com/sql/docs/postgres/backup-recovery/backups
      - text: Cloud SQL Sqlserver Backups
        url: https://cloud.google.com/sql/docs/sqlserver/backup-recovery/backups
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.7
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ip
          - nist-csf-pr.ip-10
          - nist-csf-pr.ip-4
          - nist-csf-pr.pt
          - nist-csf-pr.pt-5
      - google cloud
  - id: darkbit-gcp-60
    title: Default Compute Service Account Should Not Be Bound To Project Editor
    description:
      By default, the Default Compute service account is enabled and bound to
      the primitive IAM role named "Project Editor" in projects where the Compute API has
      been enabled.  In addition, this service account is bound to all new GCE, GKE, and
      other compute instances by default.
    remediation:
      Remove the IAM role binding of "Project Editor" from the default compute
      service account in all projects.
    validation: |-
      For each project, run the following command and validate that the
      default compute service account ending in `-compute@developer.gserviceaccount.com`
      is not present.
      
      ```
      gcloud projects get-iam-policy [PROJECT_ID] \
        --format=json | jq -r 'select(.bindings) | .bindings[] | .role as $role | 
          select(.role=="roles/editor") | "\($role): \(.members[])"'
      ``` 
    impact: 7
    effort: 7
    platform: GCP
    category: Identity and Access Management
    resource: IAM
    nodes:
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
      - GCP_IAM_ROLE
    refs:
      - text: Default Compute Service Account
        url: https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#using_the_default_service_account
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.5
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ac
          - nist-csf-pr.ac-1
          - nist-csf-pr.ac-4
      - google cloud
  - id: darkbit-gcp-61
    title: Application-Level Encryption of Secrets in GKE Etcd Should Be Implemented
    description:
      The Kubernetes API Server has the ability to leverage an external KMS
      provider for encryption and decryption of secrets stored inside etcd.  This provides
      additional protection in the event of unauthorized access to the disk or datastore
      where etcd maintains the cluster state.  It also ensures that the most sensitive
      data in etcd backups are not directly readable if stored in cloud storage buckets.
    remediation: |-
      Create a new KMS Keyring and KMS Key for each GKE Cluster.

      ```
      gcloud kms keyrings create [RING_NAME] \
        --location [LOCATION] \
        --project [KEY_PROJECT_ID]
      ```

      ```
      gcloud kms keys create [KEY_NAME] \
                  --location [LOCATION] \
                  --keyring [RING_NAME] \
                  --purpose encryption \
                  --project [KEY_PROJECT_ID]
      ```
      Grant the Kubernetes Engine Service Agent service account the Cloud KMS CryptoKey Encrypter/Decrypter role.

      ```
      gcloud kms keys add-iam-policy-binding [KEY_NAME] \
         --location [LOCATION] \
         --keyring [RING_NAME] \
         --member serviceAccount:[SERVICE_ACCOUNT_NAME] \
         --role roles/cloudkms.cryptoKeyEncrypterDecrypter \
         --project [KEY_PROJECT_ID]
      ```

      Update the existing cluster with the following command.

      ```
      gcloud container clusters update [CLUSTER_NAME] \
        --zone [ZONE] \
        --database-encryption-key projects/[KEY_PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME] \
        --project [CLUSTER_PROJECT_ID]
      ```
    validation: |-
      Run the following command in each project with GKE Clusters and ensure the `state` is `ENCRYPTED`..

      ```
      gcloud container clusters describe $CLUSTER_NAME --format json | jq '.databaseEncryption'
      ```
    impact: 2
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Kubernetes KMS Provider
        url: https://kubernetes.io/docs/tasks/administer-cluster/kms-provider/
      - text: GKE Application Secrets Encryption
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/encrypting-secrets
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.3
          - gke-cis1.1-5.3.1
      - nist-csf:
          - nist-csf-pr
          - nist-csf-pr.ds
          - nist-csf-pr.ds-1
          - nist-csf-pr.ds-5
      - google cloud
  - id: darkbit-gcp-62
    title: GCP Service Accounts Should Not Have User-Managed Keys Attached
    description:
      Anyone who has access to the keys will be able to access resources through the service
      account. GCP-managed keys are used by Cloud Platform services such as App Engine and
      Compute Engine. These keys cannot be downloaded. Google will keep the keys and
      automatically rotate them on an approximately weekly basis. User-managed keys are
      created, downloadable, and managed by users. They expire 10 years from creation.
    remediation:
      Remove user-managed keys from service accounts wherever possible. Disable
      service account key creation through the `Disable service account key creation` Organization policy.
    validation: |-
      Run the following command to list all service accounts with user-managed keys, and 
      confirm each service account returns zero items.

      ```
      for i in $(gcloud iam service-accounts list | grep 'iam.gserviceaccount.com' | awk '{print $2}')
      do
        echo "sa: $i"
        gcloud iam service-accounts keys list --iam-account=$i --managed-by=user | grep Z
      done
      ```
    impact: 7
    effort: 7
    platform: GCP
    category: Identity and Access Management
    resource: ServiceAccount
    nodes:
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
      - GCP_IDENTITY
    refs:
      - text: Managing Service Account Keys
        url: https://cloud.google.com/iam/docs/understanding-service-accounts#managing_service_account_keys
      - text: Restricting service accounts
        url: https://cloud.google.com/resource-manager/docs/organization-policy/restricting-service-accounts
      - text: Disable service account key creation
        url: https://console.cloud.google.com/iam-admin/orgpolicies/iam-disableServiceAccountKeyCreation
    public: true
    tags:
      - google cloud
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.4
      - nist-csf:
          - nist-csf-todo
  - id: darkbit-gcp-63
    title: GCP Service Accounts Should Not Be Bound To Admin, Editor, or Owner Roles
    description: |
      In most use cases, GCP Service Accounts should be bound to predefined or custom roles 
      specific to the functions performed by that account.  Attaching `roles/owner`, `roles/editor`, 
      or `admin` roles provides an overly broad and powerful set of permissions that makes the 
      credential highly valuable to an attacker if leaked or compromised.  Notable exceptions are 
      Service Accounts used for deployment and lifecycle management of other cloud resources 
      (e.g. Terraform, Deployment Manager).
    remediation: |
      Review the IAM bindings granting `roles/owner`, `roles/editor`, or "admin" roles such as 
      `roles/iam.serviceAccountAdmin` and determine if those bindings can be replaced with a small 
      number of predefined roles and/or custom roles using the GCP IAM Recommender to understand 
      which permissions are actually used in the past 90 days.
    validation: |-
      Run the following command to list the IAM policy of the current project.

      ```
      gcloud projects get-iam-policy PROJECT_ID \
         --flatten="bindings[].members" \
         --format='table(bindings.members,bindings.role)' \
         --filter="bindings.members:serviceAccount:* AND \
            (bindings.role:roles/owner OR binding.role:roles/editor \
             OR binding.role:roles/*dmin)"
      ```

      Ensure that there are no entries listed.
    impact: 5
    effort: 7
    platform: GCP
    category: Identity and Access Management
    resource: IAM
    nodes:
      - GCP_IDENTITY
      - GCP_IAM_ROLE
    refs:
      - text: Default Compute Service Account
        url: https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances#using_the_default_service_account
      - text: Understanding Service Accounts
        url: https://cloud.google.com/iam/docs/understanding-service-accounts
      - text: GCP IAM Recommender
        url: https://cloud.google.com/iam/docs/recommender-overview
    public: true
    tags:
      - google cloud
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.5
      - nist-csf:
          - nist-csf-todo
  - id: darkbit-gcp-64
    title: Service Account Admin and Service Account User Should Not Be Attached To The Same Identity
    description:
      As a best practice to ensure separation of duties, no user should have both `Service Account Admin`
      and `Service Account User` roles assigned at the same time.
    remediation:
      Practice separation of duties by ensuring that no single user has both `Service Account Admin` and
      `Service Account User` roles. Designate separate, limited, and tightly controlled users to act as
      the `Service Account Admin` role.
    validation: |-
      Run the following command to find all users that have both `Service Account Admin` and 
      `Service Account User` role and confirm that no users are returned.

      ```
      comm -1 -2 \
        <(gcloud projects get-iam-policy [PROJECT_ID] \
          --filter=bindings.role:roles/iam.serviceAccountAdmin \
          --flatten='bindings[]' \
          --format="csv[no-heading](bindings.members[])" | sort) \
        <(gcloud projects get-iam-policy [PROJECT_ID] \
          --filter=bindings.role:roles/iam.serviceAccountUser \
          --flatten='bindings[]' \
          --format="csv[no-heading](bindings.members[])" | sort)
      ```
    impact: 5
    effort: 4
    platform: GCP
    category: Identity and Access Management
    resource: IAM
    nodes:
      - GCP_IDENTITY
      - GCP_IAM_ROLE
    refs:
      - text: Service Accounts
        url: https://cloud.google.com/iam/docs/service-accounts
      - text: Understanding Roles
        url: https://cloud.google.com/iam/docs/understanding-roles
      - text: Granting Roles to Service Accounts
        url: https://cloud.google.com/iam/docs/granting-roles-to-service-accounts
    public: true
    tags:
      - google cloud
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-not-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.8
      - nist-csf:
          - nist-csf-todo
  - id: darkbit-gcp-65
    title: Cloud KMS Keys Should Not Be Public
    description:
      Granting permissions to `allUsers` or `allAuthenticatedUsers` allows anyone to access KMS keys.
      Anonymous and/or public access to a Cloud KMS keys should never be allowed.
    remediation: |-
      Remove any IAM role bindings for KMS keys for `allUsers` and `allAuthenticatedUsers` using the commands below.

      ```
      gcloud kms keys remove-iam-policy-binding [key_name] \
        --keyring=[key_ring_name] \
        --location=global \
        --member='allAuthenticatedUsers' \
        --role='[role]'
      ```

      and

      ```
      gcloud kms keys remove-iam-policy-binding [key_name] \
        --keyring=[key_ring_name] \
        --location=global \
        --member='allUsers' \
        --role='[role]'
      ```
    validation: |-
      Run the following command to list all KMS keys and ensure no roles are bound to `allUsers` 
      or `allAuthenticatedUsers`.

      ```
      for j in $(for i in $(gcloud kms keyrings list \
        --location=global \
        --format="csv[no-heading](name)")
        do 
          gcloud kms keys list \
            --keyring=$i \
            --location=global \
            --format="csv[no-heading](name)"
        done)
      do 
        echo "key: $j"
        gcloud kms keys get-iam-policy $j \
          --keyring=$i \
          --location=global \
          --filter="bindings.members:allUsers OR bindings.members:allAuthenticatedUsers" \
          --format="csv[no-heading](bindings.members[],bindings.role[])"
      done
      ```
    impact: 10
    effort: 2
    platform: GCP
    category: Identity and Access Management
    resource: KMS
    nodes:
      - GCP_CLOUDKMS_CRYPTOKEY
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Remove IAM Policy Bindings
        url: https://cloud.google.com/sdk/gcloud/reference/kms/keys/remove-iam-policy-binding
    public: true
    tags:
      - google cloud
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.9
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-66
    title: Cloud KMS Keys Should Be Rotated Every 90 Days
    description:
      Google Cloud KMS stores cryptographic keys in a hierarchical structure designed for
      useful and elegant access control management. The default key rotation period is `90`
      days. Key rotation should not be set to a period longer than `90` days. Keys should
      be rotated frequently to minimize the risk of credential leakage or misuse.
    remediation: |-
      Set the rotation period of KMS keys to a value less than or equal to `90` days.

      ```
      gcloud kms keys update new --keyring=[KEY_RING] \
        --location=global \
        --next-rotation-time=[NEXT_ROTATION_TIME] \
        --rotation-period=[ROTATION_PERIOD]
      ```
    validation: |-
      Run the following command and verify the rotation period is less than or equal to `90` days.

      ```
      gcloud kms keys list --keyring=[KEY_RING] \
        --location=global \
        --format=json'(rotationPeriod)'
      ```
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: KMS
    nodes:
      - GCP_CLOUDKMS_CRYPTOKEY
    refs:
      - text: Frequency of Key Rotation
        url: https://cloud.google.com/kms/docs/key-rotation#frequency_of_key_rotation
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.10
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-67
    title: Cloud KMS Admin and Cloud KMS Encrypter/Decrypter Should Not Be Attached To The Same Identity
    description:
      The built-in/predefined IAM role `Cloud KMS Admin` allows the user/identity to create, delete,
      and manage service account(s). The built-in/predefined IAM role `Cloud KMS CryptoKey
      Encrypter/Decrypter` allows the user/identity (with adequate privileges on concerned resources)
      to encrypt and decrypt data at rest using an encryption key(s). As a best practice to ensure
      separation of duties, no user should have both of these roles assigned at the same time.
    remediation: |
      From the console, go to [IAM & Admin/IAM](https://console.cloud.google.com/iam-admin/iam) and for 
      any users with the `Cloud KMS Admin` role remove any of the following if present: `Cloud KMS 
      CryptoKey Encrypter/Decrypter`, `Cloud KMS CryptoKey Encrypter`, and `Cloud KMS CryptoKey Decrypter`.

    validation: |-
      Run the following command to list all users and role assignments.

      ```
      gcloud projects get-iam-policy [PROJECT_ID]
      ```

      Ensure that there are no common users found in the member section for roles `cloudkms.admin` and 
      any one of `Cloud KMS CryptoKey Encrypter/Decrypter`, `Cloud KMS CryptoKey Encrypter`, 
      `Cloud KMS CryptoKey Decrypter`.
    impact: 8
    effort: 4
    platform: GCP
    category: Identity and Access Management
    resource: IAM
    nodes:
      - GCP_IDENTITY
      - GCP_IAM_ROLE
    refs:
      - text: Separation of KMS Duties
        url: https://cloud.google.com/kms/docs/separation-of-duties
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-1
          - gcp-cis1.1-1.11
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-72
    title: Log Sinks Should Be Configured For All Log Entries
    description: |
      For the best visbility into security events, at least one log sink should be configured with no 
      filters so that it will export all log entries.
    remediation: |-
      Create a sink to export all log entries to a Google Cloud Storage bucket.

      ```
      gcloud logging sinks create [SINK_NAME] storage.googleapis.com/[DESTINATION_BUCKET_NAME]
      ```
    validation: |-
      For each project, run the following command and verify that at least one sink has an empty `filter`.

      ```
      gcloud logging sinks list --project=[PROJECT_ID]
      ```
    impact: 8
    effort: 4
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_IDENTITY
      - GCP_IAM_ROLE
    refs:
      - text: Google Cloud Logging
        url: https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
      - text: Using Exported Logs
        url: https://cloud.google.com/logging/docs/export/using_exported_logs
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-73
    title: Logging Buckets Should Have Retention Policies and Bucket Lock Enabled
    description: |
      Enabling retention policies on log buckets will protect logs stored in cloud 
      storage buckets from being overwritten or accidentally deleted. Set up retention 
      policies and configure Bucket Lock on all storage buckets that are used as log sinks.
    remediation: |-
      For each project, run the following command to list log sinks that exporting to storage buckets:

      ```
      gcloud logging sinks list --project=[PROJECT_ID]
      ```

      For each bucket, set a `Retention Policy` and enable `Bucket Lock`

      ```
      gsutil retention set [TIME_DURATION] gs://[BUCKET_NAME]
      gsutil retention lock gs://[BUCKET_NAME]
      ```

      __Note__ that locking a bucket is an irreversible action. Once you lock a bucket, 
      you cannot remove the retention policy from the bucket or decrease the retention 
      period for the policy.
    validation: |-
      For each project, run the following command to list log sinks that are exporting to storage buckets:

      ```
      gcloud logging sinks list --project=[PROJECT_ID]
      ```

      For each bucket, verify `Retention Policies` and `Bucket Lock` are enabled.

      ```
      gsutil retention get gs://[BUCKET_NAME]
      ```
    impact: 8
    effort: 7
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GCS
    nodes:
      - GCP_STORAGE_BUCKET
      - GCP_LOGGING_LOGSINK
    refs:
      - text: Bucket Lock
        url: https://cloud.google.com/storage/docs/bucket-lock
      - text: Using Bucket Lock
        url: https://cloud.google.com/storage/docs/using-bucket-lock
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.3
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-74
    title: Log Metric Filter And Alert Should Notify On Project Ownership Changes
    description: |
      In order to prevent unnecessary project ownership assignments to users/service-accounts 
      and further misuses of projects and resources, all `roles/Owner` assignments should be 
      monitored. Project ownership has the highest level of privileges on a project. To avoid 
      misuse of project resources, the project ownership assignment/change actions mentioned 
      above should be monitored and alerted to concerned recipients.
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following advanced 
      filter.

      ```
      (protoPayload.serviceName="cloudresourcemanager.googleapis.com")
        AND (ProjectOwnership OR projectOwnerInvitee)
       OR (protoPayload.serviceData.policyDelta.bindingDeltas.action="REMOVE"
        AND protoPayload.serviceData.policyDelta.bindingDeltas.role="roles/owner") 
       OR (protoPayload.serviceData.policyDelta.bindingDeltas.action="ADD"
        AND protoPayload.serviceData.policyDelta.bindingDeltas.role="roles/owner")
      ```

      In the `Metric Editor` menu, fill out the `name` field. Set `Units` 
      to `1` (default) and the `Type` to `Counter`. This ensures that the log metric 
      counts the number of log entries matching the advanced logs query.
    validation: |-
      Run the following command to list log metrics.

      ```
      gcloud logging metrics list --format=json
      ```

      Verify at least one metric has the filter set to:

      ```
      (protoPayload.serviceName="cloudresourcemanager.googleapis.com")
        AND (ProjectOwnership OR projectOwnerInvitee)
       OR (protoPayload.serviceData.policyDelta.bindingDeltas.action="REMOVE"
        AND protoPayload.serviceData.policyDelta.bindingDeltas.role="roles/owner") 
       OR (protoPayload.serviceData.policyDelta.bindingDeltas.action="ADD"
        AND protoPayload.serviceData.policyDelta.bindingDeltas.role="roles/owner")
      ```

      Run the following command to list alerting policies.

      ```
      gcloud alpha monitoring policies list --format=json
      ```

      Ensure `conditions.conditionThreshold.filter` is set to 
      `metric.type="logging.googleapis.com/user/[Log Metric Name]"` and `enabled` is set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Google Cloud Logging
        url: https://cloud.google.com/logging/docs/reference/tools/gcloud-logging
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.4
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-75
    title: Log Metric Filter And Alert Should Notify On Audit Configuration Changes
    description: |
      Admin activity and data access logs produced by cloud audit logging enable security 
      analysis, resource change tracking, and compliance auditing. Configuring the metric 
      filter and alerts for audit configuration changes ensures the recommended state of 
      audit configuration is maintained so that all activities in the project are 
      auditable at any point in time.
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following 
      advanced filter:

      ```
      protoPayload.methodName="SetIamPolicy" AND 
      protoPayload.serviceData.policyDelta.auditConfigDeltas:*
      ```

      In the Metric Editor menu, fill out the `name` field. Set `Units` to `1` (default) 
      and `Type` to `Counter`. This will ensure that the log metric counts the number 
      of log entries matching the user's advanced logs query.

      Next, [Create an alert from the Metric](https://console.cloud.google.com/logs/metrics) with 
      an alert threshold of `0` to trigger a notification any time the project owner is changed.
    validation: |-
      List log metrics:

      ```
      gcloud beta logging metrics list --format json
      ```

      Ensure that at least one metric has the filter set to:

      ```
      protoPayload.methodName="SetIamPolicy" AND 
      protoPayload.serviceData.policyDelta.auditConfigDeltas:*
      ```

      List alerting policies:

      ```
      gcloud alpha monitoring policies list --format json
      ```

      Ensure that the output contains at least one alert policy where 
      `conditions.conditionThreshold.filter` is set to 
      `metric.type="logging.googleapis.com/user/[Log Metric Name]"` and 
      `enabled` is set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Monitoring Alerts
        url: https://cloud.google.com/monitoring/alerts/
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.5
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-76
    title: Log Metric Filter And Alert Should Notify On Firewall Rule Changes
    description: |
      A metric filter and alert should exist for VPC network firewall rule changes.
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following
      advanced filter:

      ```
      resource.type="gce_firewall_rule"
        AND jsonPayload.event_subtype="compute.firewalls.patch"
        OR jsonPayload.event_subtype="compute.firewalls.insert"
      ```

      In the Metric Editor menu, fill out the `name` field. Set `Units` to `1` (default)
      and `Type` to `Counter`. This will ensure that the log metric counts the number
      of log entries matching the user's advanced logs query.

      Next, [Create an alert from the Metric](https://console.cloud.google.com/logs/metrics) with
      an alert threshold of `0` to trigger a notification any time firewall rules are changed.
    validation: |-
      List the log metrics.

      ```
      gcloud beta logging metrics list --format json
      ```

      Ensure that the output contains at least one metrics with the filter:

      ```
      resource.type="gce_firewall_rule"
        AND jsonPayload.event_subtype="compute.firewalls.patch"
        OR jsonPayload.event_subtype="compute.firewalls.insert"
      ```

      List the alerting policies.

      ```
      gcloud alpha monitoring policies list --format json
      ```

      Ensure that at least one alert policy has `conditions.conditionThreshold.filter`
      set to `metric.type="logging.googleapis.com/user/[Log Metric Name]"` and `enabled` set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Monitoring Alerts
        url: https://cloud.google.com/monitoring/alerts/
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.7
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-77
    title: Log Metric Filter And Alert Should Notify On Route Changes
    description: |
      A metric filter and alert should exist for VPC network route changes. Monitoring changes 
      to route tables will help ensure that all VPC traffic flows through an expected path.
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following
      advanced filter:

      ```
      resource.type="gce_route"
        AND jsonPayload.event_subtype="compute.routes.delete" 
        OR jsonPayload.event_subtype="compute.routes.insert"
      ```

      In the Metric Editor menu, fill out the `name` field. Set `Units` to `1` (default)
      and `Type` to `Counter`. This will ensure that the log metric counts the number
      of log entries matching the user's advanced logs query.

      Next, [Create an alert from the Metric](https://console.cloud.google.com/logs/metrics) with
      an alert threshold of `0` to trigger a notification any time route tables are changed.
    validation: |-
      List the log metrics.

      ```
      gcloud beta logging metrics list --format json
      ```

      Ensure that the output contains at least one metrics with the filter:

      ```
      resource.type="gce_route"
        AND jsonPayload.event_subtype="compute.routes.delete"
        OR jsonPayload.event_subtype="compute.routes.insert"
      ```

      List the alerting policies.

      ```
      gcloud alpha monitoring policies list --format json
      ```

      Ensure that at least one alert policy has `conditions.conditionThreshold.filter`
      set to `metric.type="logging.googleapis.com/user/[Log Metric Name]"` and `enabled` set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Monitoring Alerts
        url: https://cloud.google.com/monitoring/alerts/
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.8
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-78
    title: Log Metric Filter And Alert Should Notify On Network Changes
    description: |
      A metric filter and alarm should exist for VPC network changes. Monitoring changes 
      to a VPC will help ensure VPC traffic flow is not being impacted.
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following
      advanced filter:

      ```
      resource.type=gce_network
        AND jsonPayload.event_subtype="compute.networks.insert"
        OR jsonPayload.event_subtype="compute.networks.patch"
        OR jsonPayload.event_subtype="compute.networks.delete"
        OR jsonPayload.event_subtype="compute.networks.removePeering" 
        OR jsonPayload.event_subtype="compute.networks.addPeering"
      ```

      In the Metric Editor menu, fill out the `name` field. Set `Units` to `1` (default)
      and `Type` to `Counter`. This will ensure that the log metric counts the number
      of log entries matching the user's advanced logs query.

      Next, [Create an alert from the Metric](https://console.cloud.google.com/logs/metrics) with
      an alert threshold of `0` to trigger a notification any time VPC networks are changed.
    validation: |-
      List the log metrics.

      ```
      gcloud beta logging metrics list --format json
      ```

      Ensure that the output contains at least one metrics with the filter:

      ```
      resource.type=gce_network
        AND jsonPayload.event_subtype="compute.networks.insert"
        OR jsonPayload.event_subtype="compute.networks.patch"
        OR jsonPayload.event_subtype="compute.networks.delete"
        OR jsonPayload.event_subtype="compute.networks.removePeering" 
        OR jsonPayload.event_subtype="compute.networks.addPeering"
      ```

      List the alerting policies.

      ```
      gcloud alpha monitoring policies list --format json
      ```

      Ensure that at least one alert policy has `conditions.conditionThreshold.filter`
      set to `metric.type="logging.googleapis.com/user/[Log Metric Name]"` and `enabled` set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Monitoring Alerts
        url: https://cloud.google.com/monitoring/alerts/
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.9
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-79
    title: Log Metric Filter And Alert Should Notify On Cloud Storage IAM Changes
    description: |
      A metric filter and alarm should exist for Cloud Storage Bucket IAM changes. Monitoring changes to 
      cloud storage bucket permissions may reduce the time needed to detect and correct permissions on 
      sensitive cloud storage buckets and objects inside the bucket.
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following
      advanced filter:

      ```
      resource.type=gcs_bucket
        AND protoPayload.methodName="storage.setIamPermissions"
      ```

      In the Metric Editor menu, fill out the `name` field. Set `Units` to `1` (default)
      and `Type` to `Counter`. This will ensure that the log metric counts the number
      of log entries matching the user's advanced logs query.

      Next, [Create an alert from the Metric](https://console.cloud.google.com/logs/metrics) with
      an alert threshold of `0` to trigger a notification any time permissions on sensitive cloud storage 
      buckets are changed.
    validation: |-
      List the log metrics.

      ```
      gcloud beta logging metrics list --format json
      ```

      Ensure that the output contains at least one metrics with the filter:

      ```
      resource.type=gcs_bucket
        AND protoPayload.methodName="storage.setIamPermissions"
      ```

      List the alerting policies.

      ```
      gcloud alpha monitoring policies list --format json
      ```

      Ensure that at least one alert policy has `conditions.conditionThreshold.filter`
      set to `metric.type="logging.googleapis.com/user/[Log Metric Name]"` and `enabled` set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Monitoring Alerts
        url: https://cloud.google.com/monitoring/alerts/
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.10
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-80
    title: Log Metric Filter And Alert Should Notify On SQL Instance Changes
    description: |
      A metric filter and alarm should exist for SQL instance configuration changes. Monitoring changes to 
      SQL instance configuration changes may reduce the time needed to detect and correct misconfigurations 
      done on the SQL server.
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following
      advanced filter:

      ```
      protoPayload.methodName="cloudsql.instances.update"
      ```

      In the Metric Editor menu, fill out the `name` field. Set `Units` to `1` (default)
      and `Type` to `Counter`. This will ensure that the log metric counts the number
      of log entries matching the user's advanced logs query.

      Next, [Create an alert from the Metric](https://console.cloud.google.com/logs/metrics) with
      an alert threshold of `0` to trigger a notification any time SQL server configurations are changed.
    validation: |-
      List the log metrics.

      ```
      gcloud beta logging metrics list --format json
      ```

      Ensure that the output contains at least one metrics with the filter:

      ```
      protoPayload.methodName="cloudsql.instances.update"
      ```

      List the alerting policies.

      ```
      gcloud alpha monitoring policies list --format json
      ```

      Ensure that at least one alert policy has `conditions.conditionThreshold.filter`
      set to `metric.type="logging.googleapis.com/user/[Log Metric Name]"` and `enabled` set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Monitoring Alerts
        url: https://cloud.google.com/monitoring/alerts/
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.11
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-81
    title: Legacy VPC Networks Should Not Be Present
    description: |
      There are several disadvantages to using legacy networks. Legacy networks have a single network 
      IPv4 prefix range and a single gateway IP address for the whole network. The network is global 
      in scope and spans all cloud regions. Subnetworks cannot be created in a legacy network and are 
      unable to switch from legacy to auto or custom subnet networks. Legacy networks can have an 
      impact for high network traffic projects and are subject to a single point of contention or failure.
    remediation: |-
      For each project with `legacy` mode networks, create a new non-legacy network and delete the legacy network.
    validation: |-
      List the networks configured for each project and confirm that none list networks in `legacy` mode.

      ```
      gcloud compute networks list
      ```
    impact: 5
    effort: 9
    platform: GCP
    category: Networking and Content Delivery
    resource: VPC
    nodes:
      - GCP_COMPUTE_NETWORK
    refs:
      - text: Deleting a legacy network
        url: https://cloud.google.com/vpc/docs/using-legacy#deleting_a_legacy_network
      - text: Creating a legacy network
        url: https://cloud.google.com/vpc/docs/using-legacy#creating_a_legacy_network
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-82
    title: Cloud DNS Domains Should Have DNSSEC Enabled
    description: |
      Cloud DNS domains should have Domain Name System Security Extensions (DNSSEC) enabled. DNSSEC in 
      Cloud DNS enables domain owners to take easy steps to protect their domains against DNS hijacking 
      and man-in-the-middle and other attacks.
    remediation: |-
      Enable DNSSEC for any Managed Zones without DNSSEC enabled.

      ```
      gcloud dns managed-zones update [ZONE_NAME] --dnssec-state on
      ```
    validation: |-
      List all Managed Zones in a project.

      ```
      gcloud dns managed-zones list
      ```

      For each zone with `public` visibility, get its metadata and esure that the `dnssecConfig.state` property is `on`.

      ```
      gcloud dns managed-zones describe [ZONE_NAME]
      ```
    impact: 5
    effort: 5
    platform: GCP
    category: Networking and Content Delivery
    resource: DNS
    nodes:
      - GCP_DNS_MANAGEDZONE
    refs:
      - text: Enabling Cloud DNS DNSSEC
        url: https://cloud.google.com/dns/dnssec-config#enabling
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.3
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-83
    title: Cloud DNS Domains Should Not Use RSASHA1 For The Key Signing Key
    description: |
      Legacy insecure key signing algorithms should not be used for Cloud DNS Domains.
    remediation: |-
      If it is necessary to change the settings for a Managed Zone where it has been enabled, DNSSEC must be 
      turned off and re-enabled with different settings. To turn off DNSSEC, run the following command:

      ```
      gcloud dns managed-zones update [ZONE_NAME] --dnssec-state off
      ```

      Update the key-signing algorithm for a Managed Zone.

      ```
      gcloud dns managed-zones update [ZONE_NAME] \
        --dnssec-state on \
        --ksk-algorithm ECDSAP256SHA256 --ksk-key-length 256 \
        --zsk-algorithm ECDSAP256SHA256 --zsk-key-length 256 \
        --denial-of-existence NSEC
      ```
    validation: |-
      List Managed Zones and ensure SHA1 is not configured.

      ```
      gcloud dns managed-zones describe [ZONE_NAME] \
        --format="json(dnsName,dnssecConfig.state,dnssecConfig.defaultKeySpecs)"
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Networking and Content Delivery
    resource: DNS
    nodes:
      - GCP_DNS_MANAGEDZONE
    refs:
      - text: Using Advanced DNSSEC
        url: https://cloud.google.com/dns/dnssec-advanced#advanced_signing_options
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-not-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.4
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-84
    title: Cloud DNS Domains Should Not Use RSASHA1 For The Zone Signing Key
    description: |
      Legacy insecure key signing algorithms should not be used for Cloud DNS Domains.
    remediation: |-
      If it is necessary to change the settings for a Managed Zone where it has been enabled, DNSSEC must be 
      turned off and re-enabled with different settings. To turn off DNSSEC, run the following command:

      ```
      gcloud dns managed-zones update [ZONE_NAME] --dnssec-state off
      ```

      Update the zone-signing algorithm for a Managed Zone.

      ```
      gcloud dns managed-zones update [ZONE_NAME] \
        --dnssec-state on \
        --ksk-algorithm ECDSAP256SHA256 --ksk-key-length 256 \
        --zsk-algorithm ECDSAP256SHA256 --zsk-key-length 256 \
        --denial-of-existence NSEC
      ```
    validation: |-
      List Managed Zones and ensure SHA1 is not configured.

      ```
      gcloud dns managed-zones describe [ZONE_NAME] \
        --format="json(dnsName,dnssecConfig.state,dnssecConfig.defaultKeySpecs)"
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Networking and Content Delivery
    resource: DNS
    nodes:
      - GCP_DNS_MANAGEDZONE
    refs:
      - text: Using Advanced DNSSEC
        url: https://cloud.google.com/dns/dnssec-advanced#advanced_signing_options
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-not-scored
          - gcp-cis1.1-3
          - gcp-cis1.1-3.5
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-85
    title: GCE Instances Should Not Have The Default Compute Service Account Attached
    description: |
      GCE instances should not use the default Compute Engine service account because it has the Editor role on the Project.
    remediation: |-
      For any instances with the Default Compute Service Account attached, stop the instance.

      ```
      gcloud compute instances stop [INSTANCE_NAME]
      ```

      Update the instance to use a non-default Service Account.

      ```
      gcloud compute instances set-service-account [INSTANCE_NAME] \
        --service-account=[SERVICE_ACCOUNT]
      ```

      Restart the instance.

      ```
      gcloud compute instances start [INSTANCE_NAME]
      ```

    validation: |-
      List instances in the project.

      ```
      gcloud compute instances list
      ```

      Get the details of each instance and ensure that none of the Service Accounts end with `-compute@developer.gserviceaccount.com`.

      ```
      gcloud compute instances describe [INSTANCE_NAME] --zone [ZONE]
      ```
    impact: 5
    effort: 8
    platform: GCP
    category: Identity and Access Management
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
      - GCP_IDENTITY
    refs:
      - text: Creating and enabling service accounts for instances
        url: https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-86
    title: GCE Instances Should Not Have The "cloud-platform" OAuth2 Scope Assigned
    description: |
      The principle of least privilege should be implemented to help prevent potential privilege escalation. Instances 
      should not have the `Compute Engine default service account` assigned with Scope `Allow full access to all Cloud APIs`.
    remediation: |-
      For any instances that have full access scope, stop the instance:

      ```
      gcloud compute instances stop [INSTANCE_NAME]
      ```

      Update the instance scopes.

      ```
      gcloud compute instances set-service-account [INSTANCE_NAME] \
        --service-account=[SERVICE_ACCOUNT] \
        --scopes [<SCOPE1>, <SCOPE2>...]
      ```

      Restart the instance.

      ```
      gcloud compute instances start [INSTANCE_NAME]
      ```
    validation: |-
      List GCE instances in a project.

      ```
      gcloud compute instances list
      ```

      Get the details of each instance and ensure `serviceAccount`.`scopes` does not include `https://www.googleapis.com/auth/cloud-platform`.

      ```
      gcloud compute instances describe [INSTANCE_NAME] --zone [ZONE] | grep -v "gke-"
      ```
    impact: 5
    effort: 8
    platform: GCP
    category: Identity and Access Management
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
      - GCP_IAM_OAUTHSCOPE
    refs:
      - text: Creating and enabling service accounts for instances
        url: https://cloud.google.com/compute/docs/access/create-enable-service-accounts-for-instances
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-87
    title: GCE Instances Should Block Project-Wide SSH Keys
    description: |
      Use instance-specific SSH keys instead of using common/shared project-wide SSH keys to access instances.
    remediation: |-
      Block project-wide SSH keys by setting `block-project-ssh-keys` to `true`.

      ```
      gcloud compute instances add-metadata [INSTANCE_NAME] \
        --metadata block-project-ssh-keys=TRUE
      ```
    validation: |-
      List all instances in a project.

      ```
      gcloud compute instances list
      ```

      For every instance, get the instance metadata and ensure `block-project-ssh-keys` is set to `true`.

      ```
      gcloud compute instances describe [INSTANCE_NAME]
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Identity and Access Management
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
      - GCP_COMPUTEMETADATAITEM
    refs:
      - text: Managing SSH keys in metadata
        url: https://cloud.google.com/compute/docs/instances/adding-removing-ssh-keys
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.3
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-88
    title: OSLogin Should Be Enabled In Compute Projects And GCE Instances Should Not Override Via Metadata
    description: |
      Enabling `osLogin` ensures that SSH keys used to connect to instances are mapped with IAM users. Revoking 
      access to IAM user will revoke all the SSH keys associated with that particular user. This facilitates 
      centralized and automated SSH key pair management.
    remediation: |-
      Configure `oslogin` on the project.

      ```
      gcloud compute project-info add-metadata \
        --metadata enable-oslogin=TRUE
      ```

      And remove instance metadata that overrides the project settings.

      ```
      gcloud compute instances remove-metadata [INSTANCE_NAME] \
        --keys=enable-oslogin
      ```
    validation: |-
      Ensure that `oslogin` is enabled on the project.

      ```
      gcloud compute project-info describe
      ```

      Verify that the section `commonInstanceMetadata` has a key `enable-oslogin` set to value `TRUE`. Also ensure that 
      no instance in the project overrides the project setting.

      ```
      gcloud compute instances describe [INSTANCE_NAME]
      ```
    impact: 5
    effort: 8
    platform: GCP
    category: Identity and Access Management
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
      - GCP_COMPUTEMETADATAITEM
    refs:
      - text: Setting up OS Login
        url: https://cloud.google.com/compute/docs/instances/managing-instance-access
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.4
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-89
    title: GCE Instances Should Not Be Available Via Serial Console
    description: |
      Tnteractive serial console support should be disabled. The interactive serial console does not support 
      IP-based access restrictions such as IP allowlists. If you enable the interactive serial console on an 
      instance, clients can attempt to connect to that instance from any IP address. This allows anybody to 
      connect to that instance if they know the correct SSH key, username, project ID, zone, and instance name.
    remediation: |-
      Disable the virtual serial console.

      ```
      gcloud compute instances add-metadata [INSTANCE_NAME] \
        --zone=[ZONE] \
        --metadata=serial-port-enable=false
      ```
    validation: |-
      Check instance metadata. The `serial-port-enable` should either be missing or set to `0`,

      ```
      gcloud compute instances describe [VM_NAME] \
        --zone=[ZONE] \
        --format="json(metadata.items[].key,metadata.items[].value)"
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Identity and Access Management
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
      - GCP_COMPUTEMETADATAITEM
    refs:
      - text: Interacting with the serial console
        url: https://cloud.google.com/compute/docs/instances/interacting-with-serial-console
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.5
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-90
    title: GCE Instances Should Not Have IP Forwarding Enabled
    description: |
      Instances not involved in routing network traffic should have IP forwarding disabled.
    remediation: |-
      Confirm that instances with IP forwarding enabled can safely be shut down without impact other 
      applications or services. Then shut those instances down and replace them with instances that have 
      `IP forwarding` set to `off`.
    validation: |-
      List instances and verify that none have `CAN_IP_FORWARD` set to `true` unless they need to route 
      network traffic.

      ```
      gcloud compute instances list \
        --format='table(name,canIpForward)'
      ```
    impact: 5
    effort: 8
    platform: GCP
    category: Networking and Content Delivery
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
    refs:
      - text: Enabling IP forwarding for instances
        url: https://cloud.google.com/vpc/docs/using-routes#canipforward
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.6
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-91
    title: Business Critical GCE Instances Should Have CSEK Encrypted Disks
    description: |
      If you provide your Customer-Supplied Encryption Key (CSEK), Compute Engine uses your key to protect the 
      Google-generated keys used to encrypt and decrypt your data. Only users who can provide 
      the correct key can use resources protected by a customer-supplied encryption key.
    remediation: |-
      Create compute instances with a CSEK.

      ```
      gcloud compute instances create [INSTANCE_NAME] \
        --csek-key-file <key-file.json>
      ```

      Encrypt standalone persistent disks.

      ```
      gcloud compute disks create [DISK_NAME] \
        --csek-key-file <key-file.json>
      ```
    validation: |-
      Verify Compute Engine disks are encrypted with CSEKs.

      ```
      gcloud compute disks describe [DISK_NAME] \
        --zone=ZONE \
        --format="json(diskEncryptionKey,name)"
      ```
    impact: 5
    effort: 9
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
      - GCP_COMPUTE_DISK
    refs:
      - text: Encrypt persistent disks with your own keys
        url: https://cloud.google.com/compute/docs/disks/customer-supplied-encryption#encrypt_a_new_persistent_disk_with_your_own_keys
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.7
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-92
    title: GCE Instances Should Be Shielded Nodes
    description: |
      Launch compute instances Shielded VM enabled to defend against against advanced threats 
      and ensure that the boot loader and firmware on VMs are signed and untampered.
    remediation: |-
      You can only enable Shielded VM options on instances that have Shielded VM support. Consider implementing 
      an Organization policy constraint to require Shielded VMs. For a list of Shielded VM public images, run 
      the gcloud compute images list command with the following flags:

      ```
      gcloud compute images list --project gce-uefi-images --no-standard-images
      ```

      To update an instance to enabled Shielded VM, first stop the VM:

      ```
      gcloud compute instances stop [INSTANCE_NAME]
      ```

      Then update the instance.

      ```
      gcloud compute instances update [INSTANCE_NAME] \
        --shielded-vtpm \
        --shielded-vm-integrity-monitoring
      ```

      Optionally, also enable secure boot.

      ```
      gcloud compute instances update [INSTANCE_NAME] \
        --shielded-vm-secure-boot
      ```

      Then restart the instance.

      ```
      gcloud compute instances start [INSTANCE_NAME]
      ```
    validation: |-
      List instances in a project and ensure `shieldedInstanceConfig` configuration is present and 
      that configuration has the `enableIntegrityMonitoring` and `enableVtpm` set to `true`. 

      ```
      gcloud compute instances describe [INSTANCE_NAME]
      ```
    impact: 5
    effort: 9
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GCE
    nodes:
      - GCP_COMPUTE_INSTANCE
    refs:
      - text: Organization policy constraints for Shielded VM
        url: https://cloud.google.com/security/shielded-cloud/shielded-vm#organization-policy-constraint
      - text: Modifying Shielded VM options
        url: https://cloud.google.com/compute/docs/instances/modifying-shielded-vm
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-4
          - gcp-cis1.1-4.8
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-94
    title: GCS Buckets Should Not Be Public
    description: |
      Cloud Storage bucket should not allow anonymous or public access.
    remediation: |-
      Remove anonymous and public access from buckets.

      ```
      gsutil iam ch -d allUsers gs://[BUCKET_NAME]
      gsutil iam ch -d allAuthenticatedUsers gs://[BUCKET_NAME]
      ```
    validation: |-
      List all buckets in a project.

      ```
      gsutil ls
      ```

      Check the IAM Policy for each bucket and ensure no role contains `allUsers` or `allAuthenticatedUsers`.

      ```
      gsutil iam get gs://[BUCKET_NAME]
      ```
    impact: 8
    effort: 3
    platform: GCP
    category: Identity and Access Management
    resource: GCS
    nodes:
      - GCP_STORAGE_BUCKET
    refs:
      - text: Making data public
        url: https://cloud.google.com/storage/docs/access-control/making-data-public
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-5
          - gcp-cis1.1-5.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-95
    title: GCS Buckets Should Have Uniform IAM Access Configured
    description: |
      Cloud Storage has uniform bucket-level access. Using this feature disables ACLs for all Cloud 
      Storage resources. Access to Cloud Storage resources is then granted exclusively through Cloud 
      IAM. Enabling uniform bucket-level access guarantees that if a Storage bucket is not publicly 
      accessible, no object in the bucket is publicly accessible either.
    remediation: |-
      Enable `uniformbucketlevelaccess` for buckets.

      ```
      gsutil uniformbucketlevelaccess set on gs://[BUCKET_NAME]
      ```
    validation: |-
      List all buckets in a project.

      ```
      gsutil ls
      ```

      For each bucket, verify that uniform bucket-level access is enabled.

      ```
      gsutil uniformbucketlevelaccess get gs://[BUCKET_NAME]
      ```
    impact: 3
    effort: 5
    platform: GCP
    category: Identity and Access Management
    resource: GCS
    nodes:
      - GCP_STORAGE_BUCKET
    refs:
      - text: Uniform bucket-level access
        url: https://cloud.google.com/storage/docs/uniform-bucket-level-access
      - text: Organization policy constraints for Cloud Storage
        url: https://cloud.google.com/storage/docs/org-policy-constraints#uniform-bucket
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level2
          - gcp-cis1.1-scored
          - gcp-cis1.1-5
          - gcp-cis1.1-5.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-97
    title: Cloud SQL Instances Running MySQL Should Have "local_infile" Disabled
    description: |
      Diabling `local_infile` controls the server-side `LOCAL` capability for `LOAD DATA`
      statements which may be used to gain privileged access to the underlying virtual machine
      if privileged access to the MySQL database server is obtained.
    remediation: |-
      Diable `local_infile` for MySQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags local_infile=off
      ```
    validation: |-
      List MySQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:MYSQL*'
      ```

      Ensure `local_infile` is `off` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="local_infile") | .value'
      ```
    impact: 8
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Creating and managing MySQL users
        url: https://cloud.google.com/sql/docs/mysql/create-manage-users
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.1
          - gcp-cis1.1-6.1.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-98
    title: Cloud SQL Instances Running PostgreSQL Should Have "log_checkpoints" Enabled
    description: |
      Enabling `log_checkpoints` causes checkpoints and restart points to be logged in the server log. 
      Useful statistics are included in the log messages, including the number of buffers written and the 
      time spent writing them.
    remediation: |-
      Enable `log_checkpoints` for PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags log_checkpoints=on
      ```
    validation: |-
      List PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:POSTGRES*'
      ```

      Ensure `log_checkpoints` is `on` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="log_checkpoints") | .value'
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.2
          - gcp-cis1.1-6.2.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-99
    title: Cloud SQL Instances Running PostgreSQL Should Have "log_connections" Enabled
    description: |
      The `log_connections` setting should be enabled. PostgreSQL does not log attempted connections by 
      default. Enabling the log_connections setting will create log entries for each attempted 
      connection as well as successful completion of client authentication which can be useful in 
      troubleshooting issues and to determine any unusual connection attempts to the server. The 
      `log_disconnections` and `log_connections` work hand in hand and generally, the pair should be 
      enabled/disabled together.
    remediation: |-
      Enable `log_connections` for PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags log_connections=on
      ```
    validation: |-
      List PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:POSTGRES*'
      ```

      Ensure `log_connections` is `on` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="log_connections") | .value'
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.2
          - gcp-cis1.1-6.2.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-100
    title: Cloud SQL Instances Running PostgreSQL Should Have "log_disconnections" Enabled
    description: |
      The `log_connections` setting should be enabled. PostgreSQL does not log session details such as 
      duration and session end by default. Enabling the log_disconnections setting will create log entries 
      at the end of each session which can be useful in troubleshooting issues and determine any unusual 
      activity across a time period. The `log_disconnections` and `log_connections` work hand in hand and 
      generally, the pair should be enabled/disabled together.
    remediation: |-
      Enable `log_disconnections` for PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags log_disconnections=on
      ```
    validation: |-
      List PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:POSTGRES*'
      ```

      Ensure `log_disconnections` is `on` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="log_disconnections") | .value'
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.2
          - gcp-cis1.1-6.2.3
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-101
    title: Cloud SQL Instances Running PostgreSQL Should Have "log_lock_waits" Enabled
    description: |
      The `log_lock_waits` setting should be enabled. The deadlock timeout defines the time to wait 
      on a lock before checking for any conditions. Frequent run overs on deadlock timeout can be an 
      indication of an underlying issue. Logging such waits on locks by enabling the `log_lock_waits` 
      flag can be used to identify poor performance due to locking delays or if a specially-crafted 
      SQL is attempting to starve resources through holding locks for excessive amounts of time.
    remediation: |-
      Enable `log_lock_waits` for PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags log_lock_waits=on
      ```
    validation: |-
      List PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:POSTGRES*'
      ```

      Ensure `log_lock_waits` is `on` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="log_lock_waits") | .value'
      ```
    impact: 3
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.2
          - gcp-cis1.1-6.2.4
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-102
    title: Cloud SQL Instances Running PostgreSQL Should Have "log_min_error_statement" Set To "ERROR"
    description: |
      The `log_min_error_statement` setting should be enabled to aid troubleshooting and forensic analysis. The
      `log_min_error_statement` flag defines the minimum message severity level that is considered as an error 
      statement. Messages for error statements are logged with the SQL statement.
    remediation: |-
      Enable `log_min_error_statement` for PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags log_min_error_statement=on
      ```
    validation: |-
      List PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:POSTGRES*'
      ```

      Ensure `log_min_error_statement` is `on` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="log_min_error_statement") | .value'
      ```
    impact: 4
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-not-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.2
          - gcp-cis1.1-6.2.5
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-103
    title: Cloud SQL Instances Running PostgreSQL Should Have "log_temp_files" Enabled
    description: |
      PostgreSQL instances should have `log_temp_files` enabled to aid in troubleshooting. PostgreSQL will create a 
      temporary file for actions such as sorting, hashing and temporary query results when these operations exceed 
      `work_mem`. The `log_temp_files` flag controls logging names and the file size when it is deleted. Configuring 
      `log_temp_files` to `0` causes all temporary file information to be logged, while positive values log only 
      files whose size is greater than or equal to the specified number of kilobytes. A value of `-1` disables 
      temporary file information logging.
    remediation: |-
      Enable `log_temp_files` for PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags log_temp_files=0
      ```
    validation: |-
      List PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:POSTGRES*'
      ```

      Ensure `log_temp_files` is `0` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="log_temp_files") | .value'
      ```
    impact: 3
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.2
          - gcp-cis1.1-6.2.6
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-104
    title: Cloud SQL Instances Running PostgreSQL Should Have "log_min_duration_statement" Disabled
    description: |
      SQL statements may include sensitive information that should not be recorded in logs. The `log_min_duration_statement` 
      flag defines the minimum amount of execution time of a statement in milliseconds where the total duration of 
      the statement is logged. Ensure that `log_min_duration_statement` is disabled by setting the value to `-1`.
    remediation: |-
      Enable `log_min_duration_statement` for PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --database-flags log_min_duration_statement=on
      ```
    validation: |-
      List PostgreSQL Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:POSTGRES*'
      ```

      Ensure `log_min_duration_statement` is `-1` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="log_min_duration_statement") | .value'
      ```
    impact: 2
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.2
          - gcp-cis1.1-6.2.7
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-105
    title: Cloud SQL Instances Running SQL Server Should Have "cross db ownership chaining" Disabled
    description: |
      The database flag `set cross db ownership chaining` should be disabled for SQL Server instances. This server 
      option allows you to control cross-database ownership chaining at the database level or to allow 
      cross-database ownership chaining for all databases. Enabling `cross db ownership` is not recommended 
      unless all of the databases hosted by the instance of SQL Server must participate in cross-database
      ownership chaining and you are aware of the security implications of this setting.
    remediation: |-
      Disable `cross db ownership chaining` for SQL Server Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] --database-flags "cross db ownership chaining=off"
      ```
    validation: |-
      List SQL Server Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:SQLSERVER*'
      ```

      Ensure `cross db ownership chaining` is `off` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="cross db ownership chaining") | .value'
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.3
          - gcp-cis1.1-6.3.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-106
    title: Cloud SQL Instances Running SQL Server Should Have "contained database authentication" Disabled
    description: |
      The database flag `contained database authentication` should be disabled for SQL Server instances. A **contained** 
      database includes all database settings and metadata required to define the database and has no configuration 
      dependencies on the instance of the Database Engine where the database is installed. Users can connect to 
      the database without authenticating a login at the Database Engine level. Isolating the database 
      from the Database Engine makes it possible to easily move the database to another instance of SQL Server. 
      Contained databases have some unique threats that should be understood and mitigated by SQL Server Database 
      Engine administrators. Most of the threats are related to the `USER WITH PASSWORD` authentication process, which 
      moves the authentication boundary from the Database Engine level to the database level, hence it is 
      recommended to disable this flag.
    remediation: |-
      Disable `contained database authentication` for SQL Server Cloud SQL instances.

      ```
      gcloud sql instances patch [INSTANCE_NAME] --database-flags "contained database authentication=off"
      ```
    validation: |-
      List SQL Server Cloud SQL instances.

      ```
      gcloud sql instances list --filter='DATABASE_VERSION:SQLSERVER*'
      ```

      Ensure `contained database authentication` is `0` for each instance.

      ```
      gcloud sql instances describe [INSTANCE_NAME] \
        --format=json | jq '.settings.databaseFlags[] | 
          select(.name=="contained database authentication") | .value'
      ```
    impact: 5
    effort: 3
    platform: GCP
    category: Database
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Setting a database flag
        url: https://cloud.google.com/sql/docs/postgres/flags#setting_a_database_flag
      - text: Security Best Practices with Contained Databases
        url: https://docs.microsoft.com/en-us/sql/relational-databases/databases/security-best-practices-with-contained-databases
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.3
          - gcp-cis1.1-6.3.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-107
    title: Cloud SQL Instances Should Have Restrictive Authorized Networks Configured
    description: |
      Cloud SQL instances should only accept connections from trusted networks or IP addresses and restrict 
      access otherwise.
    remediation: |
      Ensure Cloud SQL instances have `authorized-networks` defined.

      ```
      gcloud sql instances patch [INSTANCE_NAME] \
        --authorized-networks=[CIDR_1],[CIDR_2],[CIDR_N]
      ```
    validation: |-
      List all Cloud SQL database instances.

      ```
      gcloud sql instances list
      ```

      Get detailed configuration for every Cloud SQL database instance and ensure that the section 
      `settings: ipConfiguration : authorizedNetworks` does not have any parameter value containing `0.0.0.0/0`.

      ```
      gcloud sql instances describe [INSTANCE_NAME]
      ```
    impact: 8
    effort: 3
    platform: GCP
    category: Network Access Control
    resource: Cloud SQL
    nodes:
      - GCP_SQLADMIN_INSTANCE
    refs:
      - text: Configuring public IP connectivity for MySQL
        url: https://cloud.google.com/sql/docs/mysql/configure-ip
      - text: Configuring public IP connectivity for PostgreSQL
        url: https://cloud.google.com/sql/docs/postgres/configure-ip
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-6
          - gcp-cis1.1-6.5
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-108
    title: BigQuery Datasets Should Not Be Public
    description: |
      BigQuery datasets should not allow anonymous or public access.
    remediation: |-
      Get the dataset information.

      ```
      bq show --format=prettyjson [PROJECT_ID]:[DATASET_NAME] > PATH_TO_FILE
      ```

      Edit the file to remove any occurrences of `allUsers` or `allAuthenticatedUsers`. Then update the dataset.

      ```
      bq update --source PATH_TO_FILE [PROJECT_ID]:[DATASET_NAME]
      ```
    validation: |-
      List datasets and ensure that `allUsers` and `allAuthenticatedUsers` have not been granted access.

      ```
      bq show [PROJECT_ID]:[DATASET_NAME]
      ```
    impact: 8
    effort: 3
    platform: GCP
    category: Identity and Access Management
    resource: BigQuery
    nodes:
      - GCP_BIGQUERY_DATASET
    refs:
      - text: Controlling access to datasets
        url: https://cloud.google.com/bigquery/docs/dataset-access-controls
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-7
          - gcp-cis1.1-7.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-109
    title: GKE Clusters Should Not Have Client Certificate Authentication Enabled
    description: |
      Disable client certificates, which require certificate rotation, for authentication. 
      Instead, use another authentication method like OpenID Connect.
    remediation: |-
      There is no way to remove a client certificate from an existing cluster. Create a new cluster 
      with no client certificate.

      ```
      gcloud container clusters create [CLUSTER_NAME] \
        --no-issue-client-certificate
      ```
    validation: |-
      Verify that a client certificate has not been issued.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --zone [COMPUTE_ZONE] \
        --format json | jq '.masterAuth.clientKey'
      ```
    impact: 5
    effort: 9
    platform: GCP
    category: Identity and Access Management
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Leave legacy client authentication methods disabled
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-2
          - gke-cis1.1-2.1
          - gke-cis1.1-2.1.1
          - gke-cis1.1-5
          - gke-cis1.1-5.8
          - gke-cis1.1-5.8.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-110
    title: GCR Image Vulnerability Scanning Should Be Enabled
    description: |
      Vulnerabilities in software packages can be exploited by hackers or malicious users to 
      obtain unauthorized access to local cloud resources. GCR Container Analysis and other 
      third party products allow images stored in GCR to be scanned for known vulnerabilities.
    remediation: |-
      Enable the container scanning API.

      ```
      gcloud services enable containerscanning.googleapis.com
      ```
    validation: |-
      List enabled APIs and confirm that `Container Scanning API` is listed in the output.

      ```
      gcloud services list --enabled --filter=containerregistry
      ```
    impact: 6
    effort: 5
    platform: GCP
    category: Containers
    resource: GCR
    nodes:
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Container analysis and vulnerability scanning
        url: https://cloud.google.com/container-registry/docs/container-analysis
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.1
          - gke-cis1.1-5.1.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-111
    title: GCR Push Access Should Be Minimized
    description: |
      Google Container Registries are commonly the central storage point for all internal container images in an organization, and these container images are run in multiple situations with direct access to sensitive data and credentials.  Therefore, the ability to "push" or upload images is a privileged operation that should be restricted to a small number of identities to maintain the integrity of the software supply chain.  Permissions for accessing GCR registries are handled directly by the permissions assigned to the Google Storage Bucket.
    remediation: |
      Review the IAM permissions assigned to the GCP Project where the GCR registry is held and the IAM permissions directly assigned to the GCS Storage Bucket named `artifacts.[PROJECT_ID].appspot.com` in that project.  Modify the IAM permissions to ensure the following `roles/*` are bound to the minimal set of identities: `storage.admin`, `storage.objectAdmin`, `storage.objectCreator`, `storage.legacyBucketOwner`, `storage.legacyBucketWriter`, and `storage.legacyObjectOwner`.  For the ability to "pull" or "read" images, assign just the role named `roles/storage.objectViewer`.
    validation: |-
      In each project where GCR registries are held, run
      ```
      gsutil iam get gs://artifacts.[PROJECT_ID].appspot.com
      ```

      and

      ```
      gcloud projects get-iam-policy [PROJECT_ID] \
         --flatten="bindings[].members" \
         --format='table(bindings.members,bindings.role)' \
         --filter="bindings.role:roles/storage.admin \
           OR bindings.role:roles/storage.objectAdmin \
           OR bindings.role:roles/storage.objectCreator \
           OR bindings.role:roles/storage.legacyBucketOwner \
           OR bindings.role:roles/storage.legacyBucketWriter \
           OR bindings.role:roles/storage.legacyObjectOwner"
      ```
      to ensure the desired bindings are in effect.
    impact: 5
    effort: 7
    platform: GCP
    category: Identity and Access Management
    resource: GCR
    refs:
      - text: GCR IAM Access Control
        url: https://cloud.google.com/container-registry/docs/access-control
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.1
          - gke-cis1.1-5.1.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-112
    title: GKE Clusters Should Have Read-Only Permissions to GCR Registries
    description: |
      The Cluster Service Account should have read-only access to GCR. The Cluster Service Account does 
      not require administrative access to GCR, only requiring pull access for containers to deploy onto 
      GKE. Restricting permissions follows the principle of least privilege and prevents credentials 
      from being abused beyond the required role.
    remediation: |-
      First, add read access to the Kubernetes Service Account.

      ```
      gsutil iam ch [TYPE]:[EMAIL_ADDRESS]:objectViewer \
        gs://artifacts.[PROJECT_ID].appspot.com
      ```

      Then remove over-privileged roles (`Storage Admin`, `Storage Object Admin`, and `Storage Object Creator`)

      ```
      gsutil iam ch -d [TYPE]:[EMAIL_ADDRESS]:[ROLE] \
        gs://artifacts.[PROJECT_ID].appspot.com
      ```
    validation: |-
      List GCR bucket permissions and ensure that if the GKE Service Account is listed, the role is 
      set to `roles/storage.objectViewer`.

      ```
      gsutil iam get gs://artifacts.[PROJECT_ID].appspot.com
      ```

      If the GKE Service Account has project level permissions that are inherited within the bucket, 
      ensure that these are not privileged. Your GKE Service Account should not be listed.

      ```
      gcloud projects get-iam-policy [PROJECT_ID] \
        --flatten="bindings[].members" \
        --format='table(bindings.members,bindings.role)' \
        --filter="bindings.role:roles/storage.admin \
        OR bindings.role:roles/storage.objectAdmin \
        OR bindings.role:roles/storage.objectCreator \
        OR bindings.role:roles/storage.legacyBucketOwner \
        OR bindings.role:roles/storage.legacyBucketWriter \
        OR bindings.role:roles/storage.legacyObjectOwner"
      ```
    impact: 8
    effort: 7
    platform: GCP
    category: Identity and Access Management
    resource: GCR
    nodes:
      - GCP_STORAGE_BUCKET
      - GCP_IAM_ROLE
    refs:
      - text: Configuring access control
        url: https://cloud.google.com/container-registry/docs/access-control
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.1
          - gke-cis1.1-5.1.3
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-113
    title: GKE Clusters Nodes Should Disable Legacy Metadata API Access
    description: |
      Legacy GCE instance metadata APIs for should be disabled for GKE nodes to prevent a pod workload from 
      being able to extract the node's credentials.
    remediation: |-
      The legacy metadata API can only be disabled during node pool creation.

      ```
      gcloud container node-pools create [POOL_NAME] \ 
        --cluster [CLUSTER_NAME] \
        --zone [COMPUTE_ZONE] \
        --metadata disable-legacy-endpoints=true
      ```
    validation: |-
      Verify a cluster node pool is not using the legacy metadata API.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \ 
        --zone [COMPUTE_ZONE] \
        --format json | jq .nodePools[].config.metadata
      ```

      Confirm that `disable-legacy-endpoints` is `true` for all node pools.
    impact: 8
    effort: 7
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_NODEPOOL
    refs:
      - text: Disabling and transitioning from legacy metadata APIs
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/protecting-cluster-metadata#disable-legacy-apis
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.4
          - gke-cis1.1-5.4.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-114
    title: GKE Clusters Should Use Stable Or Regular Release Channels
    description: |
      Clusters should be subscribed to the Regular or Stable Release Channel to 
      automate version upgrades to the GKE cluster and to reduce version management 
      complexity to the number of features and level of stability required. Release 
      Channels signal a graduating level of stability and production-readiness. 
      These are based on observed performance of GKE clusters running that version 
      and represent experience and confidence in the cluster version.
    remediation: |-
      Cluster Release Channels are only configurable during creation of the cluster.

      ```
      gcloud beta container clusters create [CLUSTER_NAME] \ 
        --zone [ZONE] \
        --release-channel [RELEASE_CHANNEL]
      ```
    validation: |-
      Describe GKE clusters and confirm that `regular` or `stable` release channels 
      are used.

      ```
      gcloud beta container clusters describe [CLUSTER_NAME] \ 
        --zone [ZONE] \
        --format json | jq .releaseChannel.channel
      ```
    impact: 8
    effort: 8
    platform: GCP
    category: Management and Governance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: GKE Release Channels
        url: https://cloud.google.com/kubernetes-engine/docs/concepts/release-channels
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.5
          - gke-cis1.1-5.5.4
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-115
    title: GKE Cluster Nodes Should Not Have Public IP Addresses
    description: |
      Disable public IP addresses for cluster nodes, so that they only have private 
      IP addresses. Disabling public IP addresses on cluster nodes restricts access 
      to only internal networks, forcing attackers to obtain local network access 
      before attempting to compromise the underlying Kubernetes hosts.
    remediation: |-
      Private Nodes can only be configured during cluster creation. To create a 
      cluster with Private Nodes enabled, include the `--enable-private-nodes` flag 
      within the cluster create command.

      ```
      gcloud container clusters create [CLUSTER_NAME] --enable-private-nodes
      ```
    validation: |-
      Describe GKE clusters and confirm `enablePrivateNodes` is `true`.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format json | jq '.privateClusterConfig.enablePrivateNodes'
      ```
    impact: 8
    effort: 7
    platform: GCP
    category: Networking and Content Delivery
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Creating a private GKE cluster
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.6
          - gke-cis1.1-5.6.5
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-116
    title: GKE Worker Nodes Should Have Restrictive Ingress and Egress Firewall Rules
    description: |
      GKE Worker Nodes should not provisioned with public IP addresses in projects 
      with permissive inbound SSH access and any egress traffic allowed.  If a 
      container running on a node were compromised or an administrative GCP 
      credential stolen, an attacker would experience minimal friction in terms of 
      network access controls.
    remediation: |
      Ensure that GKE Worker Nodes are not provisioned with public IPs, remove the 
      default ingress SSH firewall rule allowing traffic from `0.0.0.0/0`, and 
      ensure that at least one egress deny firewall rule is in the project.
    validation: |-
      In each project where GKE clusters are present, describe GKE clusters and 
      confirm `enablePrivateNodes` is `true`.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --format json | jq '.privateClusterConfig.enablePrivateNodes'
      ```

      Ensure the following command returns no entries.

      ```
      gcloud compute firewall-rules list \
        --filter="name=(default-allow-ssh)"

      ```

      Finally, ensure at firewall rule returns via the following command.

      ```
      gcloud compute firewall-rules list \
        --format=json | jq -r '.[] | . as $rule | 
          select(.direction=="EGRESS") | 
          select(.disabled==false) | 
          select(.denied) | 
          select(.destinationRanges) | 
          select(.destinationRanges[] | contains("0.0.0.0/0")) | .denied[] | 
          select(.IPProtocol=="all") | $rule.name'
      ```
    impact: 5
    effort: 5
    platform: GCP
    category: Network Access Control
    resource: Firewall
    refs:
      - text: GCP Firewall Rules
        url: https://cloud.google.com/vpc/docs/firewalls#firewall_rule_components
      - text: Creating a private GKE cluster
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/private-clusters
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level2
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.6
          - gke-cis1.1-5.6.6
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-117
    title: HTTPS Load Balancers Should Use Google-Managed SSL Certificates
    description: |
      Traffic to HTTPS load balancers should be encrypted using Google-managed SSL 
      certificates. Encrypting traffic between users and your Kubernetes workload 
      is fundamental to protecting data sent over the web.
    remediation: |-
      Identify any workloads exposed publicly using Services of `type:LoadBalancer`.

      ```
      kubectl get svc -A -o json | jq '.items[] | select(.spec.type=="LoadBalancer")'
      ```

      Replace each Service with an Ingress using a Google-managed SSL Certificate.
    validation: |-
      Confirm there are no workloads exposed publicly using Services of `type:LoadBalancer`.

      ```
      kubectl get svc -A -o json | jq '.items[] | select(.spec.type=="LoadBalancer")'
      ```

      Inspect ingresses and confirm the annotation `networking.gke.io/managed-certificates` 
      is present.

      ```
      kubectl get ingress -A -o json | jq .items[] | jq \
        '{name: .metadata.name, 
          annotations: .metadata.annotations, 
          namespace: .metadata.namespace, 
          status: .status}'
      ```
    impact: 6
    effort: 6
    platform: GCP
    category: Networking and Content Delivery
    resource: Load Balancer
    nodes:
      - K8S_INGRESS
    refs:
      - text: Using Google-managed SSL certificates
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/managed-certs
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level2
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.6
          - gke-cis1.1-5.6.8
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-121
    title: GKE Clusters Should Support Google Groups In RBAC Bindings
    description: |
      Leverage Google Groups and Cloud IAM to assign Kubernetes user roles to a 
      collection of users, instead of to individual emails using only Cloud IAM.
    remediation: |-
      The security group can only be specified during cluster creation. Follow 
      the setup [instructions](https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control#google-groups-for-gke) 
      for Google Groups for GKE. Then create a cluster specifying the security group.

      ```
      gcloud beta container clusters create my-cluster \ 
        --security-group="gke-security-groups@[yourdomain.com]"
      ```

      Then create `Roles`, `ClusterRoles`, `RoleBindings`, and `ClusterRoleBindings` that reference your Google Groups.
    validation: |-
      Examine GKE [clusters](https://console.cloud.google.com/kubernetes/list) 
      and confirm `Google Groups for RBAC` is set to `Enabled`.
    impact: 5
    effort: 8
    platform: GCP
    category: Identity and Access Management
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Google Groups for GKE
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/role-based-access-control#google-groups-for-gke
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level2
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.8
          - gke-cis1.1-5.8.3
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-122
    title: GKE Cluster Node Boot And Persistent Disks Should Be Encrypted With Customer Managed Encryption Keys (CMEK)
    description: |
      GCE persistent disks are encrypted at rest by default using envelope 
      encryption with keys managed by Google. Customer-Managed Encryption Keys 
      (CMEK) should be used to encrypt node boot and dynamically provisioned 
      attached Google Compute Engine Persistent Disks (PDs) using keys managed 
      within Cloud Key Management Service (Cloud KMS).
    remediation: |-
      Disk encryption cannot be changed after cluster creations. Create a new 
      cluster using a CMEK for the node boot disk.

      ```
      gcloud beta container clusters create [CLUSTER_NAME] \
        --disk-type [DISK_TYPE] \
        --boot-disk-kms-key \
          projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]
      ```

      Create node pools using a CMEK for the node boot disk.

      ```
      gcloud beta container node-pools create [CLUSTER_NAME] \
        --disk-type [DISK_TYPE] \
        --boot-disk-kms-key \
          projects/[PROJECT_ID]/locations/[LOCATION]/keyRings/[RING_NAME]/cryptoKeys/[KEY_NAME]
      ```
    validation: |-
      Verify node boot disks are encrypted with a CMEK by examining node pool 
      details. Confirm `diskType` is either`pd-standard` or `pd-ssd` and 
      `bootDiskKmsKey` specifies a CMEK.

      ```
      gcloud beta container node-pools describe [NODE_POOL_NAME] \ 
        --cluster [CLUSTER_NAME]
      ```

      Verify attached disks are encrypted with a CMEK by examining GCE disks. 
      First, list Persistent Volumes.

      ``
      kubectl get pv -o json | jq '.items[].metadata.name'
      ```

      For each volume, confirm that `kmsKeyName` contains a value.

      ```
      gcloud compute disks describe [PV_NAME] \
        --zone [COMPUTE_ZONE] \
        --format json | jq '.diskEncryptionKey.kmsKeyName'
      ```
    impact: 5
    effort: 9
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
      - GCP_COMPUTE_INSTANCE
      - GCP_COMPUTE_DISK
    refs:
      - text: Using customer-managed encryption keys (CMEK)
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/using-cmek
      - text: Protecting GCE Resources with Cloud KMS Keys
        url: https://cloud.google.com/compute/docs/disks/customer-managed-encryption
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.9
          - gke-cis1.1-5.9.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-123
    title: Alpha GKE Clusters Should Not Be Used In Production
    description: |
      Alpha clusters should not be used in production as they are not covered 
      by an SLA and are not production-ready. They have all Kubernetes API features 
      enabled, but are not covered by the GKE SLA, do not receive security updates, 
      have node auto-upgrade and node auto-repair disabled, cannot be upgraded, 
      and are also deleted after 30 days.
    remediation: |-
      Alpha clusters cannot be disabled. A new cluster must be created without 
      the `--enable-kubernetes-alpha` option.
    validation: |-
      Describe GKE clusters and ensure none are `true` for Alpha clusters.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \ 
        --zone [COMPUTE_ZONE]> \
        --format json | jq '.enableKubernetesAlpha'
      ```
    impact: 5
    effort: 8
    platform: GCP
    category: Management and Governance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Alpha clusters
        url: https://cloud.google.com/kubernetes-engine/docs/concepts/alpha-clusters
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.10
          - gke-cis1.1-5.10.2
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-124
    title: GKE Sandbox (gVisor) Should Be Enabled For Untrusted Workloads
    description: |
      GKE Sandbox should be used to restrict untrusted workloads as an additional 
      layer of protection when running in a multi-tenant environment.
    remediation: |-
      GKE Sanbox can only be enabled during cluster creation.

      ```
      gcloud container node-pools create [NODE_POOL_NAME] \ 
        --zone=[COMPUTE_ZONE] \
        --cluster=[CLUSTER_NAME] \ 
        --image-type=cos_containerd \
        --sandbox type=gvisor
      ```
    validation: |-
      Describe cluster node pools and confirm that `sandboxType` is `gvisor`.

      ```
      gcloud container node-pools describe [NODE_POOL_NAME] \ 
        --zone [COMPUTE_ZONE] \
        --cluster [CLUSTER_NAME] \
        --format json | jq '.config.sandboxConfig'
      ```
    impact: 5
    effort: 7
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_NODEPOOL
    refs:
      - text: Harden workload isolation with GKE Sandbox
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/sandbox-pods
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level2
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.10
          - gke-cis1.1-5.10.4
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-125
    title: GKE Clusters Should Enable Binary Authorization
    description: |
      Binary Authorization should be used to help enforce supply-chain security by 
      only allowing images with verifiable cryptographically signed metadata into 
      the cluster.
    remediation: |-
      Update the cluster to enable Binary Authorization.

      ```
      gcloud container cluster update [CLUSTER_NAME] \ 
        --zone [COMPUTE_ZONE] \
        --enable-binauthz
      ```

      Then create a [Binary Authorization Policy](https://cloud.google.com/binary-authorization/docs/policy-yaml-reference) and import it.

      ```
      gcloud container binauthz policy import [policy_file.yaml]
      ```
    validation: |-
      Describe GKE clusters and confirm `enabled` is `true` for 
      `binaryAuthorization`.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \ 
        --zone [COMPUTE_ZONE] \
        --format json | jq .binaryAuthorization
      ```
    impact: 5
    effort: 6
    platform: GCP
    category: Security, Identity, and Compliance
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Binary Authorization
        url: https://cloud.google.com/binary-authorization/
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level2
          - gke-cis1.1-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.10
          - gke-cis1.1-5.10.5
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-127
    title: GKE Clusters Should Not Have Basic Authentication Enabled
    description: |
      Disable Basic Authentication (basic auth) for API server authentication as 
      it uses static passwords which need to be rotated.
    remediation: |-
      Disable Basic Authentication by removing the status password.

      ```
      gcloud container clusters update [CLUSTER_NAME] \ 
        --no-enable-basic-auth
      ```
    validation: |-
      Describe GKE clusters and ensure `masterAuth` `username` and `password` 
      are `false`.

      ```
      gcloud container clusters describe [CLUSTER_NAME] \
        --zone [COMPUTE_ZONE] \
        --format json | jq '.masterAuth.password and .masterAuth.username'
      ```
    impact: 9
    effort: 8
    platform: GCP
    category: Identity and Access Management
    resource: GKE
    nodes:
      - GCP_CONTAINER_CLUSTER
    refs:
      - text: Legacy client authentication methods
        url: https://cloud.google.com/kubernetes-engine/docs/how-to/hardening-your-cluster#restrict_authn_methods
    public: true
    tags:
      - cis:
          - gke-cis1.1
          - gke-cis1.1-level1
          - gke-cis1.1-not-scored
          - gke-cis1.1-5
          - gke-cis1.1-5.8
          - gke-cis1.1-5.8.1
      - nist-csf:
          - nist-csf-todo
      - google cloud
  - id: darkbit-gcp-128
    title: Log Metric Filter And Alert Should Notify On Custom Role Changes
    description: |
      A metric filter and alert should exist for changes to Identity and Access 
      Management (IAM) role creation, deletion and updating activities. Monitoring 
      role creation, deletion and updating activities will help in identifying 
      any over-privileged role at early stages.
    remediation: |-
      Create a [logging metric](https://console.cloud.google.com/logs/metrics) with the following
      advanced filter:

      ```
      resource.type="iam_role" 
        AND protoPayload.methodName="google.iam.admin.v1.CreateRole" 
        OR protoPayload.methodName="google.iam.admin.v1.DeleteRole" 
        OR protoPayload.methodName="google.iam.admin.v1.UpdateRole"
      ```

      In the Metric Editor menu, fill out the `name` field. Set `Units` to `1` 
      (default) and `Type` to `Counter`. This will ensure that the log metric 
      counts the number of log entries matching the user's advanced logs query.

      Next, [Create an alert from the Metric](https://console.cloud.google.com/logs/metrics) with
      an alert threshold of `0` to trigger a notification any time IAM admin 
      roles are changed.
    validation: |-
      List the log metrics.

      ```
      gcloud beta logging metrics list --format json
      ```

      Ensure that the output contains at least one metrics with the filter:

      ```
      resource.type="iam_role" 
        AND protoPayload.methodName="google.iam.admin.v1.CreateRole" 
        OR protoPayload.methodName="google.iam.admin.v1.DeleteRole" 
        OR protoPayload.methodName="google.iam.admin.v1.UpdateRole"
      ```

      List the alerting policies.

      ```
      gcloud alpha monitoring policies list --format json
      ```

      Ensure that at least one alert policy has `conditions.conditionThreshold.filter`
      set to `metric.type="logging.googleapis.com/user/[Log Metric Name]"` 
      and `enabled` set to `true`.
    impact: 5
    effort: 5
    platform: GCP
    category: Security, Identity, and Compliance
    resource: Logging
    nodes:
      - GCP_LOGGING_LOGSINK
      - GCP_CLOUDRESOURCEMANAGER_PROJECT
    refs:
      - text: Log Based Metrics
        url: https://cloud.google.com/logging/docs/logs-based-metrics/
      - text: Monitoring Alerts
        url: https://cloud.google.com/monitoring/alerts/
    public: true
    tags:
      - cis:
          - gcp-cis1.1
          - gcp-cis1.1-level1
          - gcp-cis1.1-scored
          - gcp-cis1.1-2
          - gcp-cis1.1-2.6
      - nist-csf:
          - nist-csf-todo
      - google cloud
